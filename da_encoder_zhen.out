nohup: ignoring input
2021-05-05 16:37:01 | INFO | fairseq.distributed.utils | distributed init (rank 0): env://
2021-05-05 16:37:01 | INFO | root | Added key: store_based_barrier_key:1 to store for rank: 0
2021-05-05 16:37:01 | INFO | fairseq.distributed.utils | initialized host ip-172-31-29-120 as rank 0
2021-05-05 16:37:05 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': '/home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/logs', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': '/home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 20, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_da', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_da', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='/home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=True, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='/home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', graph_embedding='/home/ubuntu/fgda_nmt/data/graph/zh/embeds.npy', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=20, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen', save_interval=1, save_interval_updates=1000, scoring='bleu', seed=1, sentence_avg=False, sentencepiece_model='???', shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='zh', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation_da', tensorboard_logdir='/home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/logs', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation_da', 'data': '/home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin', 'source_lang': 'zh', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.997)', 'adam_eps': 1e-09, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': '???'}, 'tokenizer': None, 'simul_type': None}
2021-05-05 16:37:05 | INFO | fairseq.tasks.translation_da | [zh] dictionary: 32001 types
2021-05-05 16:37:05 | INFO | fairseq.tasks.translation_da | [en] dictionary: 32001 types
2021-05-05 16:37:06 | INFO | fairseq_cli.train | TransformerDAModel(
  (encoder): TransformerDAEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(32001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (embed_domains): Embedding(7265, 512, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (domain_projection): Linear(in_features=512, out_features=512, bias=False)
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(32001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=32001, bias=False)
  )
)
2021-05-05 16:37:06 | INFO | fairseq_cli.train | task: TranslationDATask
2021-05-05 16:37:06 | INFO | fairseq_cli.train | model: TransformerDAModel
2021-05-05 16:37:06 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2021-05-05 16:37:06 | INFO | fairseq_cli.train | num. shared model params: 97,273,856 (num. trained: 93,554,176)
2021-05-05 16:37:06 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2021-05-05 16:37:06 | INFO | fairseq.data.data_utils | loaded 7,607 examples from: /home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin/valid.zh-en.zh
2021-05-05 16:37:06 | INFO | fairseq.data.data_utils | loaded 7,607 examples from: /home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin/valid.zh-en.en
2021-05-05 16:37:06 | INFO | fairseq.data.data_utils | loaded 7,607 examples from: /home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin/valid.da.zh-en.zh
2021-05-05 16:37:06 | INFO | fairseq.tasks.translation_da | /home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin valid zh-en 7607 examples
2021-05-05 16:37:06 | INFO | fairseq.trainer | detected shared parameter: encoder.domain_projection.bias <- decoder.output_projection.bias
2021-05-05 16:37:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-05-05 16:37:06 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                
2021-05-05 16:37:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-05-05 16:37:06 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-05-05 16:37:06 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2021-05-05 16:37:06 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-05-05 16:37:06 | INFO | fairseq.trainer | Preparing to load checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt
2021-05-05 16:37:07 | INFO | fairseq.trainer | Loaded checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt (epoch 5 @ 0 updates)
2021-05-05 16:37:07 | INFO | fairseq.trainer | loading train data for epoch 1
2021-05-05 16:37:07 | INFO | fairseq.data.data_utils | loaded 8,000,000 examples from: /home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin/train.zh-en.zh
2021-05-05 16:37:07 | INFO | fairseq.data.data_utils | loaded 8,000,000 examples from: /home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin/train.zh-en.en
2021-05-05 16:37:07 | INFO | fairseq.data.data_utils | loaded 8,000,000 examples from: /home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin/train.da.zh-en.zh
2021-05-05 16:37:07 | INFO | fairseq.tasks.translation_da | /home/ubuntu/fgda_nmt/data-bin/translation_da/mixed_zhen/data-bin train zh-en 8000000 examples
2021-05-05 16:37:10 | INFO | fairseq.trainer | begin training epoch 1
2021-05-05 16:37:10 | INFO | fairseq_cli.train | Start iterating over samples
2021-05-05 16:37:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-05-05 16:37:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-05-05 16:37:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-05-05 16:37:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-05-05 16:37:34 | INFO | train_inner | epoch 001:    104 / 60421 loss=2.661, ppl=6.33, wps=16517.6, ups=4.32, wpb=3826, bsz=125.3, num_updates=100, lr=1.25e-05, gnorm=2.302, loss_scale=8, train_wall=23, gb_free=10.8, wall=27
2021-05-05 16:37:56 | INFO | train_inner | epoch 001:    204 / 60421 loss=2.625, ppl=6.17, wps=16573.4, ups=4.43, wpb=3738.9, bsz=128.8, num_updates=200, lr=2.5e-05, gnorm=2.123, loss_scale=8, train_wall=22, gb_free=10.6, wall=50
2021-05-05 16:38:19 | INFO | train_inner | epoch 001:    304 / 60421 loss=2.637, ppl=6.22, wps=16682.8, ups=4.44, wpb=3760, bsz=109, num_updates=300, lr=3.75e-05, gnorm=2.249, loss_scale=8, train_wall=22, gb_free=10.8, wall=73
2021-05-05 16:38:42 | INFO | train_inner | epoch 001:    404 / 60421 loss=2.67, ppl=6.36, wps=16407, ups=4.39, wpb=3740.1, bsz=131.7, num_updates=400, lr=5e-05, gnorm=2.553, loss_scale=8, train_wall=23, gb_free=11, wall=95
2021-05-05 16:39:04 | INFO | train_inner | epoch 001:    504 / 60421 loss=2.537, ppl=5.8, wps=16438.1, ups=4.36, wpb=3769, bsz=137.5, num_updates=500, lr=6.25e-05, gnorm=2.101, loss_scale=8, train_wall=23, gb_free=10.8, wall=118
2021-05-05 16:39:27 | INFO | train_inner | epoch 001:    604 / 60421 loss=2.47, ppl=5.54, wps=16354.9, ups=4.37, wpb=3739.3, bsz=129.5, num_updates=600, lr=7.5e-05, gnorm=2.379, loss_scale=8, train_wall=23, gb_free=10.8, wall=141
2021-05-05 16:39:50 | INFO | train_inner | epoch 001:    704 / 60421 loss=2.588, ppl=6.01, wps=16223.7, ups=4.35, wpb=3732, bsz=130.1, num_updates=700, lr=8.75e-05, gnorm=2.346, loss_scale=8, train_wall=23, gb_free=10.7, wall=164
2021-05-05 16:40:14 | INFO | train_inner | epoch 001:    804 / 60421 loss=2.51, ppl=5.7, wps=16169.5, ups=4.23, wpb=3819.2, bsz=136.7, num_updates=800, lr=0.0001, gnorm=1.995, loss_scale=8, train_wall=23, gb_free=10.9, wall=188
2021-05-05 16:40:38 | INFO | train_inner | epoch 001:    904 / 60421 loss=2.565, ppl=5.92, wps=15870.2, ups=4.22, wpb=3757.7, bsz=141, num_updates=900, lr=0.0001125, gnorm=2.262, loss_scale=8, train_wall=23, gb_free=10.7, wall=211
2021-05-05 16:40:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-05 16:41:01 | INFO | train_inner | epoch 001:   1005 / 60421 loss=2.472, ppl=5.55, wps=15629, ups=4.25, wpb=3680.2, bsz=126.9, num_updates=1000, lr=0.000125, gnorm=2.322, loss_scale=4, train_wall=23, gb_free=10.8, wall=235
2021-05-05 16:41:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 16:41:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:41:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:41:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:41:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:41:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:41:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:41:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:41:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:41:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:41:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:41:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:41:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:41:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:41:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:41:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:42:06 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.445 | ppl 21.79 | bleu 16.43 | wps 2448.1 | wpb 2024.1 | bsz 97.5 | num_updates 1000
2021-05-05 16:42:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1000 updates
2021-05-05 16:42:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_1000.pt
2021-05-05 16:42:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_1000.pt
2021-05-05 16:42:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 16.43) (writing took 7.269672673006426 seconds)
2021-05-05 16:42:36 | INFO | train_inner | epoch 001:   1105 / 60421 loss=2.449, ppl=5.46, wps=4001.8, ups=1.05, wpb=3799.8, bsz=157.8, num_updates=1100, lr=0.0001375, gnorm=1.982, loss_scale=4, train_wall=23, gb_free=10.9, wall=330
2021-05-05 16:42:59 | INFO | train_inner | epoch 001:   1205 / 60421 loss=2.444, ppl=5.44, wps=16486.5, ups=4.36, wpb=3777.8, bsz=133.8, num_updates=1200, lr=0.00015, gnorm=2.076, loss_scale=4, train_wall=23, gb_free=11.1, wall=353
2021-05-05 16:43:22 | INFO | train_inner | epoch 001:   1305 / 60421 loss=2.43, ppl=5.39, wps=16193.7, ups=4.42, wpb=3659.7, bsz=126.7, num_updates=1300, lr=0.0001625, gnorm=2.245, loss_scale=4, train_wall=22, gb_free=10.9, wall=375
2021-05-05 16:43:44 | INFO | train_inner | epoch 001:   1405 / 60421 loss=2.47, ppl=5.54, wps=16228.1, ups=4.37, wpb=3710.2, bsz=131, num_updates=1400, lr=0.000175, gnorm=2.215, loss_scale=4, train_wall=23, gb_free=11, wall=398
2021-05-05 16:44:08 | INFO | train_inner | epoch 001:   1505 / 60421 loss=2.335, ppl=5.04, wps=16204.6, ups=4.31, wpb=3759.5, bsz=142.6, num_updates=1500, lr=0.0001875, gnorm=2.349, loss_scale=4, train_wall=23, gb_free=12.1, wall=422
2021-05-05 16:44:31 | INFO | train_inner | epoch 001:   1605 / 60421 loss=2.589, ppl=6.02, wps=16182, ups=4.36, wpb=3708.7, bsz=131, num_updates=1600, lr=0.0002, gnorm=2.743, loss_scale=4, train_wall=23, gb_free=11.9, wall=444
2021-05-05 16:44:54 | INFO | train_inner | epoch 001:   1705 / 60421 loss=2.423, ppl=5.36, wps=16043.2, ups=4.27, wpb=3758.2, bsz=130.9, num_updates=1700, lr=0.0002125, gnorm=2.169, loss_scale=4, train_wall=23, gb_free=11.1, wall=468
2021-05-05 16:45:17 | INFO | train_inner | epoch 001:   1805 / 60421 loss=2.388, ppl=5.24, wps=15860.9, ups=4.27, wpb=3716.9, bsz=156.9, num_updates=1800, lr=0.000225, gnorm=2.39, loss_scale=4, train_wall=23, gb_free=10.7, wall=491
2021-05-05 16:45:41 | INFO | train_inner | epoch 001:   1905 / 60421 loss=2.469, ppl=5.54, wps=15737.2, ups=4.23, wpb=3722.6, bsz=119.1, num_updates=1900, lr=0.0002375, gnorm=2.235, loss_scale=4, train_wall=23, gb_free=10.9, wall=515
2021-05-05 16:46:05 | INFO | train_inner | epoch 001:   2005 / 60421 loss=2.357, ppl=5.12, wps=15549, ups=4.19, wpb=3714.2, bsz=126.9, num_updates=2000, lr=0.00025, gnorm=2.371, loss_scale=4, train_wall=24, gb_free=10.8, wall=539
2021-05-05 16:46:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 16:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:46:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:46:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:46:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:46:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:46:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:46:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:46:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:46:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:46:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:47:12 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.284 | ppl 19.49 | bleu 18.01 | wps 2373.7 | wpb 2024.1 | bsz 97.5 | num_updates 2000 | best_bleu 18.01
2021-05-05 16:47:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2000 updates
2021-05-05 16:47:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_2000.pt
2021-05-05 16:47:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_2000.pt
2021-05-05 16:47:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 18.01) (writing took 14.645973567006877 seconds)
2021-05-05 16:47:49 | INFO | train_inner | epoch 001:   2105 / 60421 loss=2.605, ppl=6.08, wps=3541.7, ups=0.96, wpb=3685.1, bsz=136.7, num_updates=2100, lr=0.0002625, gnorm=2.96, loss_scale=4, train_wall=22, gb_free=10.9, wall=643
2021-05-05 16:48:12 | INFO | train_inner | epoch 001:   2205 / 60421 loss=2.234, ppl=4.71, wps=16459.5, ups=4.38, wpb=3754.4, bsz=113.7, num_updates=2200, lr=0.000275, gnorm=2.41, loss_scale=4, train_wall=23, gb_free=10.8, wall=666
2021-05-05 16:48:35 | INFO | train_inner | epoch 001:   2305 / 60421 loss=2.389, ppl=5.24, wps=16531.2, ups=4.39, wpb=3768.3, bsz=141.2, num_updates=2300, lr=0.0002875, gnorm=1.99, loss_scale=4, train_wall=23, gb_free=10.7, wall=689
2021-05-05 16:48:58 | INFO | train_inner | epoch 001:   2405 / 60421 loss=2.477, ppl=5.57, wps=16243.4, ups=4.34, wpb=3744.1, bsz=135.1, num_updates=2400, lr=0.0003, gnorm=2.533, loss_scale=4, train_wall=23, gb_free=10.8, wall=712
2021-05-05 16:49:21 | INFO | train_inner | epoch 001:   2505 / 60421 loss=2.514, ppl=5.71, wps=16358.1, ups=4.34, wpb=3767.5, bsz=122, num_updates=2500, lr=0.0003125, gnorm=2.39, loss_scale=4, train_wall=23, gb_free=10.7, wall=735
2021-05-05 16:49:44 | INFO | train_inner | epoch 001:   2605 / 60421 loss=2.444, ppl=5.44, wps=16138.7, ups=4.31, wpb=3742.8, bsz=138.9, num_updates=2600, lr=0.000325, gnorm=2.208, loss_scale=4, train_wall=23, gb_free=10.7, wall=758
2021-05-05 16:50:07 | INFO | train_inner | epoch 001:   2705 / 60421 loss=2.501, ppl=5.66, wps=16034.8, ups=4.29, wpb=3734, bsz=134.5, num_updates=2700, lr=0.0003375, gnorm=2.708, loss_scale=4, train_wall=23, gb_free=10.8, wall=781
2021-05-05 16:50:31 | INFO | train_inner | epoch 001:   2805 / 60421 loss=2.494, ppl=5.63, wps=15995.5, ups=4.27, wpb=3748.6, bsz=139.1, num_updates=2800, lr=0.00035, gnorm=2.355, loss_scale=4, train_wall=23, gb_free=10.7, wall=805
2021-05-05 16:50:54 | INFO | train_inner | epoch 001:   2905 / 60421 loss=2.419, ppl=5.35, wps=15712.7, ups=4.21, wpb=3734.8, bsz=147, num_updates=2900, lr=0.0003625, gnorm=2.061, loss_scale=4, train_wall=24, gb_free=10.8, wall=828
2021-05-05 16:51:18 | INFO | train_inner | epoch 001:   3005 / 60421 loss=2.406, ppl=5.3, wps=16058.1, ups=4.31, wpb=3728.2, bsz=110.3, num_updates=3000, lr=0.000375, gnorm=2.238, loss_scale=4, train_wall=23, gb_free=10.8, wall=852
2021-05-05 16:51:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 16:51:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:51:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:51:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:51:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:51:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:51:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:51:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:51:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:51:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:51:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:51:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:51:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:51:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:51:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:51:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:51:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:51:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:51:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:52:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.266 | ppl 19.24 | bleu 17.45 | wps 2387.1 | wpb 2024.1 | bsz 97.5 | num_updates 3000 | best_bleu 18.01
2021-05-05 16:52:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3000 updates
2021-05-05 16:52:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_3000.pt
2021-05-05 16:52:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_3000.pt
2021-05-05 16:52:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 17.45) (writing took 8.080584227995132 seconds)
2021-05-05 16:52:55 | INFO | train_inner | epoch 001:   3105 / 60421 loss=2.453, ppl=5.47, wps=3854.1, ups=1.03, wpb=3747.8, bsz=136, num_updates=3100, lr=0.0003875, gnorm=2.141, loss_scale=4, train_wall=23, gb_free=10.9, wall=949
2021-05-05 16:53:18 | INFO | train_inner | epoch 001:   3205 / 60421 loss=2.439, ppl=5.42, wps=16550.1, ups=4.4, wpb=3763.9, bsz=132.2, num_updates=3200, lr=0.0004, gnorm=2.297, loss_scale=4, train_wall=23, gb_free=10.8, wall=972
2021-05-05 16:53:41 | INFO | train_inner | epoch 001:   3305 / 60421 loss=2.474, ppl=5.56, wps=16459.6, ups=4.34, wpb=3790.7, bsz=149.5, num_updates=3300, lr=0.0004125, gnorm=2.207, loss_scale=4, train_wall=23, gb_free=10.8, wall=995
2021-05-05 16:54:03 | INFO | train_inner | epoch 001:   3405 / 60421 loss=2.748, ppl=6.72, wps=16324.2, ups=4.39, wpb=3722.3, bsz=161.1, num_updates=3400, lr=0.000425, gnorm=2.866, loss_scale=4, train_wall=23, gb_free=10.7, wall=1017
2021-05-05 16:54:27 | INFO | train_inner | epoch 001:   3505 / 60421 loss=2.623, ppl=6.16, wps=16257.2, ups=4.34, wpb=3747.6, bsz=139.8, num_updates=3500, lr=0.0004375, gnorm=2.473, loss_scale=4, train_wall=23, gb_free=11.1, wall=1040
2021-05-05 16:54:50 | INFO | train_inner | epoch 001:   3605 / 60421 loss=2.713, ppl=6.55, wps=16177.1, ups=4.34, wpb=3729.4, bsz=134.9, num_updates=3600, lr=0.00045, gnorm=2.388, loss_scale=4, train_wall=23, gb_free=10.8, wall=1063
2021-05-05 16:55:13 | INFO | train_inner | epoch 001:   3705 / 60421 loss=2.583, ppl=5.99, wps=16069, ups=4.26, wpb=3767.9, bsz=136.6, num_updates=3700, lr=0.0004625, gnorm=2.415, loss_scale=4, train_wall=23, gb_free=10.6, wall=1087
2021-05-05 16:55:37 | INFO | train_inner | epoch 001:   3805 / 60421 loss=2.636, ppl=6.22, wps=15875.1, ups=4.21, wpb=3768.5, bsz=132.4, num_updates=3800, lr=0.000475, gnorm=2.422, loss_scale=4, train_wall=24, gb_free=10.7, wall=1111
2021-05-05 16:56:00 | INFO | train_inner | epoch 001:   3905 / 60421 loss=2.804, ppl=6.98, wps=15753.1, ups=4.28, wpb=3682.5, bsz=129.7, num_updates=3900, lr=0.0004875, gnorm=3.155, loss_scale=4, train_wall=23, gb_free=10.7, wall=1134
2021-05-05 16:56:23 | INFO | train_inner | epoch 001:   4005 / 60421 loss=2.773, ppl=6.84, wps=16113.4, ups=4.32, wpb=3733.8, bsz=127.2, num_updates=4000, lr=0.0005, gnorm=3.085, loss_scale=4, train_wall=23, gb_free=10.9, wall=1157
2021-05-05 16:56:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 16:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:56:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:56:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:56:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:56:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:56:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:56:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:56:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:56:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:56:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:56:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:56:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:56:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:56:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 16:56:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 16:56:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 16:57:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.494 | ppl 22.53 | bleu 15.93 | wps 2315.1 | wpb 2024.1 | bsz 97.5 | num_updates 4000 | best_bleu 18.01
2021-05-05 16:57:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 4000 updates
2021-05-05 16:57:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_4000.pt
2021-05-05 16:57:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_4000.pt
2021-05-05 16:57:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 15.93) (writing took 8.162820408993866 seconds)
2021-05-05 16:58:03 | INFO | train_inner | epoch 001:   4105 / 60421 loss=2.796, ppl=6.94, wps=3734.6, ups=1.01, wpb=3709.2, bsz=105.3, num_updates=4100, lr=0.000493865, gnorm=3.2, loss_scale=4, train_wall=23, gb_free=10.8, wall=1257
2021-05-05 16:58:25 | INFO | train_inner | epoch 001:   4205 / 60421 loss=2.793, ppl=6.93, wps=16501.4, ups=4.42, wpb=3732.2, bsz=128.2, num_updates=4200, lr=0.00048795, gnorm=2.757, loss_scale=4, train_wall=22, gb_free=10.8, wall=1279
2021-05-05 16:58:48 | INFO | train_inner | epoch 001:   4305 / 60421 loss=2.635, ppl=6.21, wps=16470.7, ups=4.36, wpb=3780.4, bsz=145.8, num_updates=4300, lr=0.000482243, gnorm=2.322, loss_scale=4, train_wall=23, gb_free=11, wall=1302
2021-05-05 16:59:11 | INFO | train_inner | epoch 001:   4405 / 60421 loss=2.468, ppl=5.53, wps=16234.4, ups=4.36, wpb=3720.8, bsz=118.1, num_updates=4400, lr=0.000476731, gnorm=2.181, loss_scale=4, train_wall=23, gb_free=10.8, wall=1325
2021-05-05 16:59:34 | INFO | train_inner | epoch 001:   4505 / 60421 loss=2.63, ppl=6.19, wps=16301.3, ups=4.3, wpb=3788.5, bsz=127.4, num_updates=4500, lr=0.000471405, gnorm=2.568, loss_scale=4, train_wall=23, gb_free=10.8, wall=1348
2021-05-05 16:59:58 | INFO | train_inner | epoch 001:   4605 / 60421 loss=2.603, ppl=6.07, wps=16209.2, ups=4.32, wpb=3755.9, bsz=147.3, num_updates=4600, lr=0.000466252, gnorm=2.312, loss_scale=4, train_wall=23, gb_free=10.8, wall=1371
2021-05-05 17:00:21 | INFO | train_inner | epoch 001:   4705 / 60421 loss=2.676, ppl=6.39, wps=15965, ups=4.29, wpb=3720, bsz=123.1, num_updates=4700, lr=0.000461266, gnorm=2.61, loss_scale=4, train_wall=23, gb_free=10.8, wall=1395
2021-05-05 17:00:45 | INFO | train_inner | epoch 001:   4805 / 60421 loss=2.417, ppl=5.34, wps=16023.3, ups=4.23, wpb=3791.7, bsz=134.6, num_updates=4800, lr=0.000456435, gnorm=2.08, loss_scale=4, train_wall=23, gb_free=10.8, wall=1418
2021-05-05 17:01:08 | INFO | train_inner | epoch 001:   4905 / 60421 loss=2.628, ppl=6.18, wps=15880.3, ups=4.2, wpb=3777.8, bsz=140.5, num_updates=4900, lr=0.000451754, gnorm=2.378, loss_scale=4, train_wall=24, gb_free=11, wall=1442
2021-05-05 17:01:31 | INFO | train_inner | epoch 001:   5005 / 60421 loss=2.549, ppl=5.85, wps=16281.3, ups=4.33, wpb=3764.3, bsz=136.2, num_updates=5000, lr=0.000447214, gnorm=2.331, loss_scale=4, train_wall=23, gb_free=11, wall=1465
2021-05-05 17:01:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:01:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:01:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:01:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:01:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:01:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:01:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:01:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:01:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:01:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:01:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:01:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:01:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:01:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:01:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:01:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:02:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.219 | ppl 18.62 | bleu 18.41 | wps 2444.3 | wpb 2024.1 | bsz 97.5 | num_updates 5000 | best_bleu 18.41
2021-05-05 17:02:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 5000 updates
2021-05-05 17:02:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_5000.pt
2021-05-05 17:02:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_5000.pt
2021-05-05 17:02:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 18.41) (writing took 14.658773460992961 seconds)
2021-05-05 17:03:14 | INFO | train_inner | epoch 001:   5105 / 60421 loss=2.53, ppl=5.78, wps=3681, ups=0.98, wpb=3762.9, bsz=137.4, num_updates=5100, lr=0.000442807, gnorm=2.289, loss_scale=4, train_wall=23, gb_free=10.9, wall=1568
2021-05-05 17:03:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-05-05 17:03:36 | INFO | train_inner | epoch 001:   5206 / 60421 loss=2.611, ppl=6.11, wps=16215.4, ups=4.38, wpb=3698.4, bsz=144.6, num_updates=5200, lr=0.000438529, gnorm=2.558, loss_scale=2, train_wall=23, gb_free=10.7, wall=1590
2021-05-05 17:03:59 | INFO | train_inner | epoch 001:   5306 / 60421 loss=2.753, ppl=6.74, wps=16269, ups=4.44, wpb=3664.2, bsz=124.6, num_updates=5300, lr=0.000434372, gnorm=2.735, loss_scale=2, train_wall=22, gb_free=10.7, wall=1613
2021-05-05 17:04:22 | INFO | train_inner | epoch 001:   5406 / 60421 loss=2.588, ppl=6.01, wps=16494.1, ups=4.36, wpb=3779.8, bsz=143, num_updates=5400, lr=0.000430331, gnorm=2.309, loss_scale=2, train_wall=23, gb_free=10.8, wall=1636
2021-05-05 17:04:45 | INFO | train_inner | epoch 001:   5506 / 60421 loss=2.443, ppl=5.44, wps=16405.1, ups=4.35, wpb=3770, bsz=117.8, num_updates=5500, lr=0.000426401, gnorm=2.037, loss_scale=2, train_wall=23, gb_free=10.8, wall=1659
2021-05-05 17:05:08 | INFO | train_inner | epoch 001:   5606 / 60421 loss=2.58, ppl=5.98, wps=16090.3, ups=4.34, wpb=3708.5, bsz=139.8, num_updates=5600, lr=0.000422577, gnorm=2.466, loss_scale=2, train_wall=23, gb_free=10.7, wall=1682
2021-05-05 17:05:31 | INFO | train_inner | epoch 001:   5706 / 60421 loss=2.605, ppl=6.08, wps=15948.3, ups=4.26, wpb=3744.6, bsz=152.2, num_updates=5700, lr=0.000418854, gnorm=2.382, loss_scale=2, train_wall=23, gb_free=10.7, wall=1705
2021-05-05 17:05:55 | INFO | train_inner | epoch 001:   5806 / 60421 loss=2.403, ppl=5.29, wps=15831.7, ups=4.23, wpb=3739.2, bsz=130.4, num_updates=5800, lr=0.000415227, gnorm=2.116, loss_scale=2, train_wall=23, gb_free=10.8, wall=1729
2021-05-05 17:06:18 | INFO | train_inner | epoch 001:   5906 / 60421 loss=2.346, ppl=5.08, wps=16184.7, ups=4.29, wpb=3771.7, bsz=137.8, num_updates=5900, lr=0.000411693, gnorm=1.99, loss_scale=2, train_wall=23, gb_free=10.8, wall=1752
2021-05-05 17:06:41 | INFO | train_inner | epoch 001:   6006 / 60421 loss=2.587, ppl=6.01, wps=16363.3, ups=4.44, wpb=3684.5, bsz=123.5, num_updates=6000, lr=0.000408248, gnorm=2.46, loss_scale=2, train_wall=22, gb_free=10.9, wall=1775
2021-05-05 17:06:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:06:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:06:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:06:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:06:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:06:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:06:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:07:45 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.083 | ppl 16.94 | bleu 20.05 | wps 2477.3 | wpb 2024.1 | bsz 97.5 | num_updates 6000 | best_bleu 20.05
2021-05-05 17:07:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 6000 updates
2021-05-05 17:07:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_6000.pt
2021-05-05 17:07:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_6000.pt
2021-05-05 17:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 20.05) (writing took 14.629089788999408 seconds)
2021-05-05 17:08:22 | INFO | train_inner | epoch 001:   6106 / 60421 loss=2.606, ppl=6.09, wps=3666, ups=0.99, wpb=3713.6, bsz=126, num_updates=6100, lr=0.000404888, gnorm=2.786, loss_scale=2, train_wall=22, gb_free=10.8, wall=1876
2021-05-05 17:08:45 | INFO | train_inner | epoch 001:   6206 / 60421 loss=2.568, ppl=5.93, wps=16428.8, ups=4.44, wpb=3703.6, bsz=130.2, num_updates=6200, lr=0.00040161, gnorm=2.398, loss_scale=2, train_wall=22, gb_free=11.1, wall=1899
2021-05-05 17:09:08 | INFO | train_inner | epoch 001:   6306 / 60421 loss=2.371, ppl=5.17, wps=16338.8, ups=4.39, wpb=3725.8, bsz=136.7, num_updates=6300, lr=0.00039841, gnorm=2.197, loss_scale=2, train_wall=23, gb_free=10.8, wall=1921
2021-05-05 17:09:30 | INFO | train_inner | epoch 001:   6406 / 60421 loss=2.638, ppl=6.23, wps=16371.1, ups=4.39, wpb=3733.4, bsz=137.2, num_updates=6400, lr=0.000395285, gnorm=2.83, loss_scale=2, train_wall=23, gb_free=10.9, wall=1944
2021-05-05 17:09:53 | INFO | train_inner | epoch 001:   6506 / 60421 loss=2.536, ppl=5.8, wps=16201.6, ups=4.32, wpb=3752.2, bsz=160.6, num_updates=6500, lr=0.000392232, gnorm=2.44, loss_scale=2, train_wall=23, gb_free=10.8, wall=1967
2021-05-05 17:10:17 | INFO | train_inner | epoch 001:   6606 / 60421 loss=2.515, ppl=5.71, wps=16163.5, ups=4.34, wpb=3727.8, bsz=125, num_updates=6600, lr=0.000389249, gnorm=2.125, loss_scale=2, train_wall=23, gb_free=10.7, wall=1990
2021-05-05 17:10:40 | INFO | train_inner | epoch 001:   6706 / 60421 loss=2.522, ppl=5.74, wps=16045, ups=4.27, wpb=3753.9, bsz=114.7, num_updates=6700, lr=0.000386334, gnorm=2.599, loss_scale=2, train_wall=23, gb_free=11, wall=2014
2021-05-05 17:11:04 | INFO | train_inner | epoch 001:   6806 / 60421 loss=2.397, ppl=5.27, wps=15890.7, ups=4.24, wpb=3749.2, bsz=154.6, num_updates=6800, lr=0.000383482, gnorm=2.088, loss_scale=2, train_wall=23, gb_free=10.9, wall=2037
2021-05-05 17:11:26 | INFO | train_inner | epoch 001:   6906 / 60421 loss=2.469, ppl=5.54, wps=16140.3, ups=4.4, wpb=3665.9, bsz=124.2, num_updates=6900, lr=0.000380693, gnorm=2.444, loss_scale=2, train_wall=23, gb_free=10.7, wall=2060
2021-05-05 17:11:49 | INFO | train_inner | epoch 001:   7006 / 60421 loss=2.36, ppl=5.13, wps=16368.3, ups=4.34, wpb=3767.7, bsz=139.8, num_updates=7000, lr=0.000377964, gnorm=1.977, loss_scale=2, train_wall=23, gb_free=10.8, wall=2083
2021-05-05 17:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:12:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:12:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:12:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:12:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:12:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:12:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:12:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:12:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:12:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:12:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:12:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:12:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:12:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.015 | ppl 16.17 | bleu 20.49 | wps 2354.5 | wpb 2024.1 | bsz 97.5 | num_updates 7000 | best_bleu 20.49
2021-05-05 17:12:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 7000 updates
2021-05-05 17:12:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_7000.pt
2021-05-05 17:12:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_7000.pt
2021-05-05 17:13:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 20.49) (writing took 14.665958297991892 seconds)
2021-05-05 17:13:34 | INFO | train_inner | epoch 001:   7106 / 60421 loss=2.297, ppl=4.91, wps=3588.7, ups=0.96, wpb=3757.4, bsz=119.8, num_updates=7100, lr=0.000375293, gnorm=2.022, loss_scale=2, train_wall=22, gb_free=11, wall=2188
2021-05-05 17:13:57 | INFO | train_inner | epoch 001:   7206 / 60421 loss=2.357, ppl=5.12, wps=16358.2, ups=4.43, wpb=3689.1, bsz=120.4, num_updates=7200, lr=0.000372678, gnorm=2.192, loss_scale=2, train_wall=22, gb_free=11, wall=2210
2021-05-05 17:14:19 | INFO | train_inner | epoch 001:   7306 / 60421 loss=2.221, ppl=4.66, wps=16513.9, ups=4.36, wpb=3786.3, bsz=136.8, num_updates=7300, lr=0.000370117, gnorm=1.845, loss_scale=2, train_wall=23, gb_free=10.7, wall=2233
2021-05-05 17:14:42 | INFO | train_inner | epoch 001:   7406 / 60421 loss=2.326, ppl=5.01, wps=16291.2, ups=4.36, wpb=3734.1, bsz=130.6, num_updates=7400, lr=0.000367607, gnorm=2.082, loss_scale=2, train_wall=23, gb_free=10.8, wall=2256
2021-05-05 17:15:05 | INFO | train_inner | epoch 001:   7506 / 60421 loss=2.397, ppl=5.27, wps=16115.1, ups=4.35, wpb=3703.1, bsz=107.1, num_updates=7500, lr=0.000365148, gnorm=2.573, loss_scale=2, train_wall=23, gb_free=10.8, wall=2279
2021-05-05 17:15:29 | INFO | train_inner | epoch 001:   7606 / 60421 loss=2.234, ppl=4.7, wps=16138.6, ups=4.24, wpb=3806.9, bsz=138.2, num_updates=7600, lr=0.000362738, gnorm=1.859, loss_scale=2, train_wall=23, gb_free=10.8, wall=2303
2021-05-05 17:15:52 | INFO | train_inner | epoch 001:   7706 / 60421 loss=2.414, ppl=5.33, wps=15872.7, ups=4.28, wpb=3710.5, bsz=121.4, num_updates=7700, lr=0.000360375, gnorm=2.231, loss_scale=2, train_wall=23, gb_free=10.8, wall=2326
2021-05-05 17:16:15 | INFO | train_inner | epoch 001:   7806 / 60421 loss=2.482, ppl=5.59, wps=16013.7, ups=4.35, wpb=3683.6, bsz=151.4, num_updates=7800, lr=0.000358057, gnorm=2.463, loss_scale=2, train_wall=23, gb_free=10.8, wall=2349
2021-05-05 17:16:38 | INFO | train_inner | epoch 001:   7906 / 60421 loss=2.434, ppl=5.4, wps=16522.2, ups=4.38, wpb=3769.1, bsz=129, num_updates=7900, lr=0.000355784, gnorm=2.394, loss_scale=2, train_wall=23, gb_free=10.7, wall=2372
2021-05-05 17:17:01 | INFO | train_inner | epoch 001:   8006 / 60421 loss=2.327, ppl=5.02, wps=16295.6, ups=4.44, wpb=3673.2, bsz=132.7, num_updates=8000, lr=0.000353553, gnorm=2.138, loss_scale=2, train_wall=22, gb_free=10.8, wall=2395
2021-05-05 17:17:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:17:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:17:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:17:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:17:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:17:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:17:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:17:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:17:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:17:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:18:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.02 | ppl 16.22 | bleu 20.3 | wps 2449.6 | wpb 2024.1 | bsz 97.5 | num_updates 8000 | best_bleu 20.49
2021-05-05 17:18:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 8000 updates
2021-05-05 17:18:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_8000.pt
2021-05-05 17:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_8000.pt
2021-05-05 17:18:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 20.3) (writing took 8.479327562992694 seconds)
2021-05-05 17:18:37 | INFO | train_inner | epoch 001:   8106 / 60421 loss=2.274, ppl=4.84, wps=3931.6, ups=1.04, wpb=3771.8, bsz=148.6, num_updates=8100, lr=0.000351364, gnorm=1.974, loss_scale=2, train_wall=23, gb_free=10.9, wall=2490
2021-05-05 17:18:59 | INFO | train_inner | epoch 001:   8206 / 60421 loss=2.175, ppl=4.52, wps=16696.1, ups=4.42, wpb=3776.4, bsz=144.2, num_updates=8200, lr=0.000349215, gnorm=1.901, loss_scale=2, train_wall=22, gb_free=10.8, wall=2513
2021-05-05 17:19:22 | INFO | train_inner | epoch 001:   8306 / 60421 loss=2.267, ppl=4.81, wps=16603.4, ups=4.44, wpb=3738.4, bsz=145, num_updates=8300, lr=0.000347105, gnorm=2.037, loss_scale=2, train_wall=22, gb_free=10.8, wall=2536
2021-05-05 17:19:45 | INFO | train_inner | epoch 001:   8406 / 60421 loss=2.362, ppl=5.14, wps=16527.2, ups=4.38, wpb=3769.1, bsz=125.8, num_updates=8400, lr=0.000345033, gnorm=2.151, loss_scale=2, train_wall=23, gb_free=10.7, wall=2558
2021-05-05 17:20:07 | INFO | train_inner | epoch 001:   8506 / 60421 loss=2.087, ppl=4.25, wps=16766.2, ups=4.4, wpb=3809.1, bsz=121.8, num_updates=8500, lr=0.000342997, gnorm=1.57, loss_scale=2, train_wall=23, gb_free=10.7, wall=2581
2021-05-05 17:20:30 | INFO | train_inner | epoch 001:   8606 / 60421 loss=2.253, ppl=4.77, wps=16716.7, ups=4.47, wpb=3739.2, bsz=121.9, num_updates=8600, lr=0.000340997, gnorm=2.049, loss_scale=2, train_wall=22, gb_free=10.8, wall=2604
2021-05-05 17:20:52 | INFO | train_inner | epoch 001:   8706 / 60421 loss=2.339, ppl=5.06, wps=16437.5, ups=4.44, wpb=3705, bsz=150.4, num_updates=8700, lr=0.000339032, gnorm=2.403, loss_scale=2, train_wall=22, gb_free=10.8, wall=2626
2021-05-05 17:21:15 | INFO | train_inner | epoch 001:   8806 / 60421 loss=2.262, ppl=4.8, wps=16407.6, ups=4.44, wpb=3698, bsz=143.6, num_updates=8800, lr=0.0003371, gnorm=2.068, loss_scale=2, train_wall=22, gb_free=11, wall=2649
2021-05-05 17:21:37 | INFO | train_inner | epoch 001:   8906 / 60421 loss=2.221, ppl=4.66, wps=16589.7, ups=4.46, wpb=3717.9, bsz=126.5, num_updates=8900, lr=0.000335201, gnorm=2.235, loss_scale=2, train_wall=22, gb_free=10.7, wall=2671
2021-05-05 17:22:00 | INFO | train_inner | epoch 001:   9006 / 60421 loss=2.348, ppl=5.09, wps=16447.3, ups=4.44, wpb=3701.9, bsz=145.3, num_updates=9000, lr=0.000333333, gnorm=2.258, loss_scale=2, train_wall=22, gb_free=10.8, wall=2694
2021-05-05 17:22:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:22:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:22:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:22:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:22:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:22:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:22:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:22:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:22:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:22:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:22:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:22:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:22:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:22:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:22:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:22:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:23:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.933 | ppl 15.28 | bleu 20.59 | wps 2479.7 | wpb 2024.1 | bsz 97.5 | num_updates 9000 | best_bleu 20.59
2021-05-05 17:23:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 9000 updates
2021-05-05 17:23:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_9000.pt
2021-05-05 17:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_9000.pt
2021-05-05 17:23:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_9000.pt (epoch 1 @ 9000 updates, score 20.59) (writing took 14.615419801004464 seconds)
2021-05-05 17:23:41 | INFO | train_inner | epoch 001:   9106 / 60421 loss=2.368, ppl=5.16, wps=3688.4, ups=0.99, wpb=3735.2, bsz=139.9, num_updates=9100, lr=0.000331497, gnorm=2.087, loss_scale=2, train_wall=22, gb_free=10.7, wall=2795
2021-05-05 17:24:04 | INFO | train_inner | epoch 001:   9206 / 60421 loss=2.213, ppl=4.64, wps=16737.2, ups=4.4, wpb=3807.3, bsz=138, num_updates=9200, lr=0.00032969, gnorm=1.724, loss_scale=2, train_wall=23, gb_free=10.9, wall=2818
2021-05-05 17:24:27 | INFO | train_inner | epoch 001:   9306 / 60421 loss=2.08, ppl=4.23, wps=16363.6, ups=4.34, wpb=3768.8, bsz=140.2, num_updates=9300, lr=0.000327913, gnorm=1.726, loss_scale=2, train_wall=23, gb_free=10.9, wall=2841
2021-05-05 17:24:50 | INFO | train_inner | epoch 001:   9406 / 60421 loss=2.122, ppl=4.35, wps=16443.7, ups=4.34, wpb=3784.5, bsz=147.8, num_updates=9400, lr=0.000326164, gnorm=1.907, loss_scale=2, train_wall=23, gb_free=10.8, wall=2864
2021-05-05 17:25:13 | INFO | train_inner | epoch 001:   9506 / 60421 loss=2.298, ppl=4.92, wps=16069.8, ups=4.31, wpb=3732.4, bsz=130.1, num_updates=9500, lr=0.000324443, gnorm=2.536, loss_scale=2, train_wall=23, gb_free=10.8, wall=2887
2021-05-05 17:25:36 | INFO | train_inner | epoch 001:   9606 / 60421 loss=2.278, ppl=4.85, wps=15993.4, ups=4.27, wpb=3749.1, bsz=132.6, num_updates=9600, lr=0.000322749, gnorm=2.067, loss_scale=2, train_wall=23, gb_free=10.9, wall=2910
2021-05-05 17:26:00 | INFO | train_inner | epoch 001:   9706 / 60421 loss=2.027, ppl=4.08, wps=16074.8, ups=4.18, wpb=3848.3, bsz=117.5, num_updates=9700, lr=0.000321081, gnorm=1.654, loss_scale=2, train_wall=24, gb_free=10.7, wall=2934
2021-05-05 17:26:23 | INFO | train_inner | epoch 001:   9806 / 60421 loss=2.158, ppl=4.46, wps=16330.8, ups=4.32, wpb=3776.6, bsz=122.2, num_updates=9800, lr=0.000319438, gnorm=2.06, loss_scale=2, train_wall=23, gb_free=10.6, wall=2957
2021-05-05 17:26:46 | INFO | train_inner | epoch 001:   9906 / 60421 loss=2.072, ppl=4.21, wps=16388.7, ups=4.39, wpb=3733.3, bsz=114.2, num_updates=9900, lr=0.000317821, gnorm=1.784, loss_scale=2, train_wall=23, gb_free=10.9, wall=2980
2021-05-05 17:27:09 | INFO | train_inner | epoch 001:  10006 / 60421 loss=2.167, ppl=4.49, wps=16443.8, ups=4.38, wpb=3750.6, bsz=137.4, num_updates=10000, lr=0.000316228, gnorm=1.905, loss_scale=2, train_wall=23, gb_free=10.9, wall=3003
2021-05-05 17:27:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:27:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:27:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:27:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:27:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:27:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:27:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:27:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:27:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:27:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:27:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:27:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:27:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:28:15 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.845 | ppl 14.37 | bleu 22.47 | wps 2385.6 | wpb 2024.1 | bsz 97.5 | num_updates 10000 | best_bleu 22.47
2021-05-05 17:28:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 10000 updates
2021-05-05 17:28:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_10000.pt
2021-05-05 17:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_10000.pt
2021-05-05 17:28:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 22.47) (writing took 14.636495970000396 seconds)
2021-05-05 17:28:53 | INFO | train_inner | epoch 001:  10106 / 60421 loss=2.265, ppl=4.81, wps=3585.7, ups=0.96, wpb=3725.5, bsz=141.4, num_updates=10100, lr=0.000314658, gnorm=2.286, loss_scale=2, train_wall=23, gb_free=11.2, wall=3107
2021-05-05 17:29:16 | INFO | train_inner | epoch 001:  10206 / 60421 loss=2.162, ppl=4.47, wps=16245.1, ups=4.37, wpb=3715.6, bsz=143.3, num_updates=10200, lr=0.000313112, gnorm=1.941, loss_scale=2, train_wall=23, gb_free=10.7, wall=3130
2021-05-05 17:29:39 | INFO | train_inner | epoch 001:  10306 / 60421 loss=2.161, ppl=4.47, wps=16345.4, ups=4.38, wpb=3733.1, bsz=131.8, num_updates=10300, lr=0.000311588, gnorm=2.305, loss_scale=2, train_wall=23, gb_free=10.9, wall=3153
2021-05-05 17:30:02 | INFO | train_inner | epoch 001:  10406 / 60421 loss=2.19, ppl=4.56, wps=16330.2, ups=4.34, wpb=3760.1, bsz=143.8, num_updates=10400, lr=0.000310087, gnorm=2.188, loss_scale=2, train_wall=23, gb_free=10.8, wall=3176
2021-05-05 17:30:25 | INFO | train_inner | epoch 001:  10506 / 60421 loss=2.227, ppl=4.68, wps=15988.3, ups=4.26, wpb=3755.7, bsz=135, num_updates=10500, lr=0.000308607, gnorm=2.182, loss_scale=2, train_wall=23, gb_free=10.7, wall=3199
2021-05-05 17:30:49 | INFO | train_inner | epoch 001:  10606 / 60421 loss=2.126, ppl=4.37, wps=15822.7, ups=4.24, wpb=3732.2, bsz=144.9, num_updates=10600, lr=0.000307148, gnorm=2.101, loss_scale=2, train_wall=23, gb_free=10.8, wall=3223
2021-05-05 17:31:12 | INFO | train_inner | epoch 001:  10706 / 60421 loss=2.185, ppl=4.55, wps=16038.5, ups=4.26, wpb=3766.3, bsz=152.9, num_updates=10700, lr=0.000305709, gnorm=2.135, loss_scale=2, train_wall=23, gb_free=10.7, wall=3246
2021-05-05 17:31:35 | INFO | train_inner | epoch 001:  10806 / 60421 loss=2.193, ppl=4.57, wps=16231.9, ups=4.36, wpb=3721.1, bsz=123.4, num_updates=10800, lr=0.00030429, gnorm=2.128, loss_scale=2, train_wall=23, gb_free=11, wall=3269
2021-05-05 17:31:58 | INFO | train_inner | epoch 001:  10906 / 60421 loss=2.201, ppl=4.6, wps=16485.3, ups=4.39, wpb=3752.6, bsz=133.5, num_updates=10900, lr=0.000302891, gnorm=2.037, loss_scale=2, train_wall=23, gb_free=11.2, wall=3292
2021-05-05 17:32:21 | INFO | train_inner | epoch 001:  11006 / 60421 loss=2.087, ppl=4.25, wps=16459.1, ups=4.39, wpb=3747.2, bsz=154.3, num_updates=11000, lr=0.000301511, gnorm=2.047, loss_scale=2, train_wall=23, gb_free=10.5, wall=3315
2021-05-05 17:32:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:32:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:32:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:32:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:32:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:32:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:32:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:32:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:32:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:32:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:33:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.881 | ppl 14.74 | bleu 22.33 | wps 2495.7 | wpb 2024.1 | bsz 97.5 | num_updates 11000 | best_bleu 22.47
2021-05-05 17:33:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 11000 updates
2021-05-05 17:33:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_11000.pt
2021-05-05 17:33:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_11000.pt
2021-05-05 17:33:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_11000.pt (epoch 1 @ 11000 updates, score 22.33) (writing took 7.933018749987241 seconds)
2021-05-05 17:33:55 | INFO | train_inner | epoch 001:  11106 / 60421 loss=2.14, ppl=4.41, wps=3959.5, ups=1.06, wpb=3729.3, bsz=133.6, num_updates=11100, lr=0.00030015, gnorm=2.132, loss_scale=2, train_wall=22, gb_free=10.6, wall=3409
2021-05-05 17:34:18 | INFO | train_inner | epoch 001:  11206 / 60421 loss=2.164, ppl=4.48, wps=16558.7, ups=4.41, wpb=3756.3, bsz=113.5, num_updates=11200, lr=0.000298807, gnorm=2.086, loss_scale=2, train_wall=22, gb_free=10.9, wall=3431
2021-05-05 17:34:41 | INFO | train_inner | epoch 001:  11306 / 60421 loss=2.144, ppl=4.42, wps=16422.3, ups=4.31, wpb=3808.7, bsz=139.8, num_updates=11300, lr=0.000297482, gnorm=2.014, loss_scale=2, train_wall=23, gb_free=10.8, wall=3455
2021-05-05 17:35:04 | INFO | train_inner | epoch 001:  11406 / 60421 loss=2.103, ppl=4.3, wps=16133, ups=4.34, wpb=3717.6, bsz=126.6, num_updates=11400, lr=0.000296174, gnorm=2.039, loss_scale=2, train_wall=23, gb_free=10.8, wall=3478
2021-05-05 17:35:27 | INFO | train_inner | epoch 001:  11506 / 60421 loss=2.136, ppl=4.4, wps=16144.7, ups=4.29, wpb=3760.7, bsz=120.6, num_updates=11500, lr=0.000294884, gnorm=1.909, loss_scale=2, train_wall=23, gb_free=10.9, wall=3501
2021-05-05 17:35:51 | INFO | train_inner | epoch 001:  11606 / 60421 loss=2.074, ppl=4.21, wps=15683.1, ups=4.22, wpb=3720.2, bsz=143.3, num_updates=11600, lr=0.00029361, gnorm=2.103, loss_scale=2, train_wall=24, gb_free=10.8, wall=3525
2021-05-05 17:36:14 | INFO | train_inner | epoch 001:  11706 / 60421 loss=2.011, ppl=4.03, wps=15969.6, ups=4.3, wpb=3713.3, bsz=131.5, num_updates=11700, lr=0.000292353, gnorm=2.055, loss_scale=2, train_wall=23, gb_free=10.7, wall=3548
2021-05-05 17:36:37 | INFO | train_inner | epoch 001:  11806 / 60421 loss=2.154, ppl=4.45, wps=16435.8, ups=4.37, wpb=3764.7, bsz=143.1, num_updates=11800, lr=0.000291111, gnorm=1.956, loss_scale=2, train_wall=23, gb_free=10.8, wall=3571
2021-05-05 17:37:00 | INFO | train_inner | epoch 001:  11906 / 60421 loss=2.135, ppl=4.39, wps=16479.2, ups=4.39, wpb=3757.3, bsz=131.5, num_updates=11900, lr=0.000289886, gnorm=2.26, loss_scale=2, train_wall=23, gb_free=11, wall=3594
2021-05-05 17:37:23 | INFO | train_inner | epoch 001:  12006 / 60421 loss=2.109, ppl=4.31, wps=16480.1, ups=4.4, wpb=3746.8, bsz=135.4, num_updates=12000, lr=0.000288675, gnorm=2.013, loss_scale=2, train_wall=23, gb_free=10.8, wall=3616
2021-05-05 17:37:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:37:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:37:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:37:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:37:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:37:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:37:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:37:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:37:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:37:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:37:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:37:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:37:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:38:28 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.826 | ppl 14.18 | bleu 22.78 | wps 2412.9 | wpb 2024.1 | bsz 97.5 | num_updates 12000 | best_bleu 22.78
2021-05-05 17:38:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 12000 updates
2021-05-05 17:38:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_12000.pt
2021-05-05 17:38:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_12000.pt
2021-05-05 17:38:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 22.78) (writing took 14.602083129007951 seconds)
2021-05-05 17:39:06 | INFO | train_inner | epoch 001:  12106 / 60421 loss=2.043, ppl=4.12, wps=3648.9, ups=0.97, wpb=3768.3, bsz=152.4, num_updates=12100, lr=0.00028748, gnorm=1.918, loss_scale=2, train_wall=23, gb_free=11, wall=3720
2021-05-05 17:39:28 | INFO | train_inner | epoch 001:  12206 / 60421 loss=2.138, ppl=4.4, wps=16266, ups=4.41, wpb=3684.8, bsz=115.3, num_updates=12200, lr=0.000286299, gnorm=2.009, loss_scale=2, train_wall=22, gb_free=11.6, wall=3742
2021-05-05 17:39:52 | INFO | train_inner | epoch 001:  12306 / 60421 loss=2.111, ppl=4.32, wps=16323.5, ups=4.34, wpb=3758.9, bsz=125.4, num_updates=12300, lr=0.000285133, gnorm=2.049, loss_scale=2, train_wall=23, gb_free=10.9, wall=3765
2021-05-05 17:40:14 | INFO | train_inner | epoch 001:  12406 / 60421 loss=2.247, ppl=4.75, wps=16174.9, ups=4.37, wpb=3702.6, bsz=129.8, num_updates=12400, lr=0.000283981, gnorm=2.224, loss_scale=2, train_wall=23, gb_free=10.8, wall=3788
2021-05-05 17:40:38 | INFO | train_inner | epoch 001:  12506 / 60421 loss=2.093, ppl=4.27, wps=15904.2, ups=4.3, wpb=3696.1, bsz=120.1, num_updates=12500, lr=0.000282843, gnorm=2.139, loss_scale=2, train_wall=23, gb_free=10.6, wall=3811
2021-05-05 17:41:01 | INFO | train_inner | epoch 001:  12606 / 60421 loss=2.069, ppl=4.19, wps=15819.7, ups=4.27, wpb=3707.2, bsz=114.7, num_updates=12600, lr=0.000281718, gnorm=1.947, loss_scale=2, train_wall=23, gb_free=11, wall=3835
2021-05-05 17:41:24 | INFO | train_inner | epoch 001:  12706 / 60421 loss=2.084, ppl=4.24, wps=16198.4, ups=4.37, wpb=3710.6, bsz=125, num_updates=12700, lr=0.000280607, gnorm=2.226, loss_scale=2, train_wall=23, gb_free=10.8, wall=3858
2021-05-05 17:41:47 | INFO | train_inner | epoch 001:  12806 / 60421 loss=2.098, ppl=4.28, wps=16303, ups=4.38, wpb=3718.6, bsz=133.8, num_updates=12800, lr=0.000279508, gnorm=1.918, loss_scale=2, train_wall=23, gb_free=10.9, wall=3881
2021-05-05 17:42:09 | INFO | train_inner | epoch 001:  12906 / 60421 loss=2.026, ppl=4.07, wps=16447.9, ups=4.41, wpb=3733.8, bsz=142.7, num_updates=12900, lr=0.000278423, gnorm=1.981, loss_scale=2, train_wall=23, gb_free=10.7, wall=3903
2021-05-05 17:42:32 | INFO | train_inner | epoch 001:  13006 / 60421 loss=2.153, ppl=4.45, wps=16245.1, ups=4.48, wpb=3629.7, bsz=135.2, num_updates=13000, lr=0.00027735, gnorm=2.345, loss_scale=2, train_wall=22, gb_free=11.2, wall=3926
2021-05-05 17:42:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:42:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:42:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:42:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:42:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:42:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:42:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:42:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:42:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:42:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:42:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:42:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:42:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:42:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:42:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:42:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:43:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.825 | ppl 14.18 | bleu 22.57 | wps 2484.3 | wpb 2024.1 | bsz 97.5 | num_updates 13000 | best_bleu 22.78
2021-05-05 17:43:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 13000 updates
2021-05-05 17:43:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_13000.pt
2021-05-05 17:43:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_13000.pt
2021-05-05 17:43:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_13000.pt (epoch 1 @ 13000 updates, score 22.57) (writing took 8.522203143991646 seconds)
2021-05-05 17:44:07 | INFO | train_inner | epoch 001:  13106 / 60421 loss=2.095, ppl=4.27, wps=3830.4, ups=1.06, wpb=3629.3, bsz=131.2, num_updates=13100, lr=0.000276289, gnorm=2.096, loss_scale=2, train_wall=22, gb_free=10.8, wall=4020
2021-05-05 17:44:29 | INFO | train_inner | epoch 001:  13206 / 60421 loss=2.064, ppl=4.18, wps=16416.7, ups=4.38, wpb=3750.1, bsz=133.2, num_updates=13200, lr=0.000275241, gnorm=1.961, loss_scale=2, train_wall=23, gb_free=10.9, wall=4043
2021-05-05 17:44:53 | INFO | train_inner | epoch 001:  13306 / 60421 loss=1.971, ppl=3.92, wps=16246.1, ups=4.3, wpb=3777.8, bsz=129.8, num_updates=13300, lr=0.000274204, gnorm=1.681, loss_scale=2, train_wall=23, gb_free=11, wall=4067
2021-05-05 17:45:16 | INFO | train_inner | epoch 001:  13406 / 60421 loss=2.061, ppl=4.17, wps=16260.2, ups=4.3, wpb=3785.8, bsz=127.7, num_updates=13400, lr=0.000273179, gnorm=1.894, loss_scale=2, train_wall=23, gb_free=11.2, wall=4090
2021-05-05 17:45:39 | INFO | train_inner | epoch 001:  13506 / 60421 loss=2.076, ppl=4.22, wps=15858.9, ups=4.25, wpb=3730, bsz=125.6, num_updates=13500, lr=0.000272166, gnorm=1.931, loss_scale=2, train_wall=23, gb_free=10.8, wall=4113
2021-05-05 17:46:03 | INFO | train_inner | epoch 001:  13606 / 60421 loss=2.159, ppl=4.47, wps=15813.1, ups=4.24, wpb=3732.9, bsz=125.7, num_updates=13600, lr=0.000271163, gnorm=2.118, loss_scale=2, train_wall=23, gb_free=11, wall=4137
2021-05-05 17:46:26 | INFO | train_inner | epoch 001:  13706 / 60421 loss=2.069, ppl=4.2, wps=16413.3, ups=4.31, wpb=3808.5, bsz=134.3, num_updates=13700, lr=0.000270172, gnorm=1.836, loss_scale=2, train_wall=23, gb_free=11, wall=4160
2021-05-05 17:46:49 | INFO | train_inner | epoch 001:  13806 / 60421 loss=1.95, ppl=3.86, wps=16526, ups=4.33, wpb=3813.2, bsz=132.7, num_updates=13800, lr=0.000269191, gnorm=1.708, loss_scale=2, train_wall=23, gb_free=10.9, wall=4183
2021-05-05 17:47:12 | INFO | train_inner | epoch 001:  13906 / 60421 loss=1.971, ppl=3.92, wps=16404.6, ups=4.39, wpb=3736.7, bsz=153, num_updates=13900, lr=0.000268221, gnorm=1.898, loss_scale=2, train_wall=23, gb_free=11, wall=4206
2021-05-05 17:47:35 | INFO | train_inner | epoch 001:  14006 / 60421 loss=1.905, ppl=3.75, wps=16614.6, ups=4.35, wpb=3821.6, bsz=135.7, num_updates=14000, lr=0.000267261, gnorm=1.733, loss_scale=2, train_wall=23, gb_free=11, wall=4229
2021-05-05 17:47:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:47:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:47:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:47:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:47:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:47:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:47:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:47:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:47:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:47:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:47:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:47:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:47:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:48:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.81 | ppl 14.02 | bleu 23.46 | wps 2419.5 | wpb 2024.1 | bsz 97.5 | num_updates 14000 | best_bleu 23.46
2021-05-05 17:48:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 14000 updates
2021-05-05 17:48:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_14000.pt
2021-05-05 17:48:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_14000.pt
2021-05-05 17:48:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_14000.pt (epoch 1 @ 14000 updates, score 23.46) (writing took 14.640709714993136 seconds)
2021-05-05 17:49:18 | INFO | train_inner | epoch 001:  14106 / 60421 loss=1.97, ppl=3.92, wps=3613.4, ups=0.97, wpb=3726.2, bsz=143.5, num_updates=14100, lr=0.000266312, gnorm=1.926, loss_scale=2, train_wall=23, gb_free=10.8, wall=4332
2021-05-05 17:49:41 | INFO | train_inner | epoch 001:  14206 / 60421 loss=2.041, ppl=4.11, wps=16282, ups=4.4, wpb=3704.2, bsz=126.8, num_updates=14200, lr=0.000265372, gnorm=2.28, loss_scale=2, train_wall=23, gb_free=10.7, wall=4355
2021-05-05 17:50:04 | INFO | train_inner | epoch 001:  14306 / 60421 loss=2.02, ppl=4.05, wps=16071.7, ups=4.33, wpb=3708.2, bsz=125.2, num_updates=14300, lr=0.000264443, gnorm=2.034, loss_scale=2, train_wall=23, gb_free=11.1, wall=4378
2021-05-05 17:50:28 | INFO | train_inner | epoch 001:  14406 / 60421 loss=2.059, ppl=4.17, wps=15803.1, ups=4.25, wpb=3718.8, bsz=127.1, num_updates=14400, lr=0.000263523, gnorm=1.921, loss_scale=2, train_wall=23, gb_free=10.8, wall=4402
2021-05-05 17:50:52 | INFO | train_inner | epoch 001:  14506 / 60421 loss=2.064, ppl=4.18, wps=15685.1, ups=4.18, wpb=3751.5, bsz=137.1, num_updates=14500, lr=0.000262613, gnorm=2.113, loss_scale=2, train_wall=24, gb_free=10.7, wall=4425
2021-05-05 17:51:15 | INFO | train_inner | epoch 001:  14606 / 60421 loss=1.999, ppl=4, wps=16058.3, ups=4.25, wpb=3776.6, bsz=136.9, num_updates=14600, lr=0.000261712, gnorm=1.751, loss_scale=2, train_wall=23, gb_free=10.9, wall=4449
2021-05-05 17:51:38 | INFO | train_inner | epoch 001:  14706 / 60421 loss=1.998, ppl=4, wps=16255.8, ups=4.34, wpb=3745.7, bsz=121.8, num_updates=14700, lr=0.00026082, gnorm=1.948, loss_scale=2, train_wall=23, gb_free=11.1, wall=4472
2021-05-05 17:52:01 | INFO | train_inner | epoch 001:  14806 / 60421 loss=2.095, ppl=4.27, wps=16345.9, ups=4.38, wpb=3733.6, bsz=127.8, num_updates=14800, lr=0.000259938, gnorm=2.19, loss_scale=2, train_wall=23, gb_free=10.8, wall=4495
2021-05-05 17:52:24 | INFO | train_inner | epoch 001:  14906 / 60421 loss=2.024, ppl=4.07, wps=16316.2, ups=4.37, wpb=3733.9, bsz=160.7, num_updates=14900, lr=0.000259064, gnorm=2.096, loss_scale=2, train_wall=23, gb_free=10.8, wall=4518
2021-05-05 17:52:46 | INFO | train_inner | epoch 001:  15006 / 60421 loss=1.97, ppl=3.92, wps=16340.2, ups=4.43, wpb=3688.6, bsz=122.7, num_updates=15000, lr=0.000258199, gnorm=2.17, loss_scale=2, train_wall=22, gb_free=11.2, wall=4540
2021-05-05 17:52:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:53:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:53:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:53:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:53:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:53:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:53:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:53:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:53:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:53:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:53:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:53:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:53:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:53:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:53:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:53:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:53:53 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.883 | ppl 14.75 | bleu 22.75 | wps 2393.4 | wpb 2024.1 | bsz 97.5 | num_updates 15000 | best_bleu 23.46
2021-05-05 17:53:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 15000 updates
2021-05-05 17:53:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_15000.pt
2021-05-05 17:53:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_15000.pt
2021-05-05 17:54:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_15000.pt (epoch 1 @ 15000 updates, score 22.75) (writing took 7.874996019992977 seconds)
2021-05-05 17:54:24 | INFO | train_inner | epoch 001:  15106 / 60421 loss=1.991, ppl=3.98, wps=3860.9, ups=1.03, wpb=3751.2, bsz=145.7, num_updates=15100, lr=0.000257343, gnorm=1.826, loss_scale=2, train_wall=23, gb_free=10.8, wall=4637
2021-05-05 17:54:47 | INFO | train_inner | epoch 001:  15206 / 60421 loss=2.08, ppl=4.23, wps=16301.7, ups=4.33, wpb=3760.7, bsz=145.9, num_updates=15200, lr=0.000256495, gnorm=1.87, loss_scale=2, train_wall=23, gb_free=10.7, wall=4661
2021-05-05 17:55:10 | INFO | train_inner | epoch 001:  15306 / 60421 loss=1.944, ppl=3.85, wps=16233.8, ups=4.26, wpb=3807.8, bsz=145.6, num_updates=15300, lr=0.000255655, gnorm=1.774, loss_scale=2, train_wall=23, gb_free=10.8, wall=4684
2021-05-05 17:55:34 | INFO | train_inner | epoch 001:  15406 / 60421 loss=1.897, ppl=3.72, wps=15817.1, ups=4.22, wpb=3747.6, bsz=135, num_updates=15400, lr=0.000254824, gnorm=1.726, loss_scale=2, train_wall=24, gb_free=10.9, wall=4708
2021-05-05 17:55:58 | INFO | train_inner | epoch 001:  15506 / 60421 loss=1.864, ppl=3.64, wps=15578.2, ups=4.21, wpb=3703.3, bsz=147, num_updates=15500, lr=0.000254, gnorm=1.718, loss_scale=2, train_wall=24, gb_free=10.8, wall=4731
2021-05-05 17:56:21 | INFO | train_inner | epoch 001:  15606 / 60421 loss=1.849, ppl=3.6, wps=16107.5, ups=4.33, wpb=3723.7, bsz=117.2, num_updates=15600, lr=0.000253185, gnorm=1.66, loss_scale=2, train_wall=23, gb_free=10.9, wall=4755
2021-05-05 17:56:43 | INFO | train_inner | epoch 001:  15706 / 60421 loss=1.912, ppl=3.76, wps=16275.8, ups=4.39, wpb=3705.4, bsz=137.1, num_updates=15700, lr=0.000252377, gnorm=1.883, loss_scale=2, train_wall=23, gb_free=10.8, wall=4777
2021-05-05 17:57:06 | INFO | train_inner | epoch 001:  15806 / 60421 loss=1.892, ppl=3.71, wps=16265.9, ups=4.39, wpb=3701.6, bsz=137.5, num_updates=15800, lr=0.000251577, gnorm=1.711, loss_scale=2, train_wall=23, gb_free=10.7, wall=4800
2021-05-05 17:57:29 | INFO | train_inner | epoch 001:  15906 / 60421 loss=1.952, ppl=3.87, wps=16080.2, ups=4.37, wpb=3677.5, bsz=154.4, num_updates=15900, lr=0.000250785, gnorm=2.405, loss_scale=2, train_wall=23, gb_free=10.9, wall=4823
2021-05-05 17:57:52 | INFO | train_inner | epoch 001:  16006 / 60421 loss=1.932, ppl=3.82, wps=16463.7, ups=4.39, wpb=3751.5, bsz=121, num_updates=16000, lr=0.00025, gnorm=1.766, loss_scale=2, train_wall=23, gb_free=10.7, wall=4846
2021-05-05 17:57:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 17:58:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:58:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:58:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:58:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:58:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:58:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:58:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:58:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:58:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 17:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 17:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 17:58:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.774 | ppl 13.68 | bleu 24.08 | wps 2452.4 | wpb 2024.1 | bsz 97.5 | num_updates 16000 | best_bleu 24.08
2021-05-05 17:58:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 16000 updates
2021-05-05 17:58:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_16000.pt
2021-05-05 17:58:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_16000.pt
2021-05-05 17:59:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_16000.pt (epoch 1 @ 16000 updates, score 24.08) (writing took 14.624778469995363 seconds)
2021-05-05 17:59:34 | INFO | train_inner | epoch 001:  16106 / 60421 loss=1.899, ppl=3.73, wps=3663.1, ups=0.98, wpb=3751.1, bsz=148.4, num_updates=16100, lr=0.000249222, gnorm=1.957, loss_scale=2, train_wall=23, gb_free=10.7, wall=4948
2021-05-05 17:59:57 | INFO | train_inner | epoch 001:  16206 / 60421 loss=1.804, ppl=3.49, wps=16258.5, ups=4.34, wpb=3746.7, bsz=127.4, num_updates=16200, lr=0.000248452, gnorm=1.688, loss_scale=2, train_wall=23, gb_free=10.8, wall=4971
2021-05-05 18:00:21 | INFO | train_inner | epoch 001:  16306 / 60421 loss=1.965, ppl=3.9, wps=15912.8, ups=4.28, wpb=3720.5, bsz=160.3, num_updates=16300, lr=0.000247689, gnorm=2.193, loss_scale=2, train_wall=23, gb_free=10.7, wall=4995
2021-05-05 18:00:45 | INFO | train_inner | epoch 001:  16406 / 60421 loss=1.919, ppl=3.78, wps=15955.3, ups=4.18, wpb=3814.2, bsz=124.6, num_updates=16400, lr=0.000246932, gnorm=1.815, loss_scale=2, train_wall=24, gb_free=10.9, wall=5018
2021-05-05 18:01:08 | INFO | train_inner | epoch 001:  16506 / 60421 loss=2.053, ppl=4.15, wps=15851.8, ups=4.24, wpb=3736.3, bsz=133, num_updates=16500, lr=0.000246183, gnorm=2.104, loss_scale=2, train_wall=23, gb_free=11.2, wall=5042
2021-05-05 18:01:31 | INFO | train_inner | epoch 001:  16606 / 60421 loss=1.946, ppl=3.85, wps=15983.4, ups=4.39, wpb=3638.9, bsz=138, num_updates=16600, lr=0.00024544, gnorm=2.007, loss_scale=2, train_wall=23, gb_free=10.9, wall=5065
2021-05-05 18:01:54 | INFO | train_inner | epoch 001:  16706 / 60421 loss=1.95, ppl=3.86, wps=16414.1, ups=4.36, wpb=3761.2, bsz=124.2, num_updates=16700, lr=0.000244704, gnorm=1.716, loss_scale=2, train_wall=23, gb_free=10.9, wall=5088
2021-05-05 18:02:16 | INFO | train_inner | epoch 001:  16806 / 60421 loss=2.01, ppl=4.03, wps=16306.9, ups=4.52, wpb=3611.4, bsz=121, num_updates=16800, lr=0.000243975, gnorm=2.483, loss_scale=2, train_wall=22, gb_free=10.8, wall=5110
2021-05-05 18:02:39 | INFO | train_inner | epoch 001:  16906 / 60421 loss=1.979, ppl=3.94, wps=16383.6, ups=4.44, wpb=3690.4, bsz=124, num_updates=16900, lr=0.000243252, gnorm=1.981, loss_scale=2, train_wall=22, gb_free=11, wall=5132
2021-05-05 18:03:01 | INFO | train_inner | epoch 001:  17006 / 60421 loss=2.057, ppl=4.16, wps=16122.1, ups=4.45, wpb=3620.9, bsz=120.8, num_updates=17000, lr=0.000242536, gnorm=2.422, loss_scale=2, train_wall=22, gb_free=10.7, wall=5155
2021-05-05 18:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:03:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:03:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:03:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:03:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:03:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:03:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:04:08 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.873 | ppl 14.66 | bleu 23.26 | wps 2382 | wpb 2024.1 | bsz 97.5 | num_updates 17000 | best_bleu 24.08
2021-05-05 18:04:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 17000 updates
2021-05-05 18:04:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_17000.pt
2021-05-05 18:04:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_17000.pt
2021-05-05 18:04:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_17000.pt (epoch 1 @ 17000 updates, score 23.26) (writing took 8.447323037005845 seconds)
2021-05-05 18:04:39 | INFO | train_inner | epoch 001:  17106 / 60421 loss=1.889, ppl=3.7, wps=3852.4, ups=1.02, wpb=3780.7, bsz=132.7, num_updates=17100, lr=0.000241825, gnorm=1.678, loss_scale=2, train_wall=23, gb_free=10.8, wall=5253
2021-05-05 18:05:02 | INFO | train_inner | epoch 001:  17206 / 60421 loss=2.017, ppl=4.05, wps=16061.3, ups=4.36, wpb=3686.5, bsz=120.9, num_updates=17200, lr=0.000241121, gnorm=2.173, loss_scale=2, train_wall=23, gb_free=10.8, wall=5276
2021-05-05 18:05:25 | INFO | train_inner | epoch 001:  17306 / 60421 loss=1.859, ppl=3.63, wps=15935.7, ups=4.28, wpb=3721.7, bsz=130.1, num_updates=17300, lr=0.000240424, gnorm=2.191, loss_scale=2, train_wall=23, gb_free=12.6, wall=5299
2021-05-05 18:05:49 | INFO | train_inner | epoch 001:  17406 / 60421 loss=2.029, ppl=4.08, wps=15625, ups=4.21, wpb=3709.9, bsz=128.6, num_updates=17400, lr=0.000239732, gnorm=2.202, loss_scale=2, train_wall=24, gb_free=11, wall=5323
2021-05-05 18:06:13 | INFO | train_inner | epoch 001:  17506 / 60421 loss=1.839, ppl=3.58, wps=15885, ups=4.25, wpb=3741, bsz=126.7, num_updates=17500, lr=0.000239046, gnorm=1.839, loss_scale=2, train_wall=23, gb_free=10.8, wall=5347
2021-05-05 18:06:36 | INFO | train_inner | epoch 001:  17606 / 60421 loss=1.876, ppl=3.67, wps=16378.1, ups=4.37, wpb=3748.5, bsz=133.7, num_updates=17600, lr=0.000238366, gnorm=1.685, loss_scale=2, train_wall=23, gb_free=10.7, wall=5370
2021-05-05 18:06:58 | INFO | train_inner | epoch 001:  17706 / 60421 loss=1.949, ppl=3.86, wps=16220.4, ups=4.4, wpb=3690.4, bsz=150.6, num_updates=17700, lr=0.000237691, gnorm=2.028, loss_scale=2, train_wall=23, gb_free=10.8, wall=5392
2021-05-05 18:07:21 | INFO | train_inner | epoch 001:  17806 / 60421 loss=1.817, ppl=3.52, wps=16397.8, ups=4.4, wpb=3724.8, bsz=117.8, num_updates=17800, lr=0.000237023, gnorm=1.712, loss_scale=2, train_wall=23, gb_free=10.8, wall=5415
2021-05-05 18:07:44 | INFO | train_inner | epoch 001:  17906 / 60421 loss=1.789, ppl=3.46, wps=16504.3, ups=4.42, wpb=3732.1, bsz=140, num_updates=17900, lr=0.00023636, gnorm=1.78, loss_scale=2, train_wall=22, gb_free=10.8, wall=5438
2021-05-05 18:08:07 | INFO | train_inner | epoch 001:  18006 / 60421 loss=1.911, ppl=3.76, wps=16420, ups=4.37, wpb=3755.9, bsz=137.1, num_updates=18000, lr=0.000235702, gnorm=1.831, loss_scale=2, train_wall=23, gb_free=10.7, wall=5460
2021-05-05 18:08:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:08:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:08:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:08:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:08:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:08:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:08:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:08:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:08:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:08:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:08:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:08:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:08:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:08:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:08:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:08:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:09:11 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.771 | ppl 13.65 | bleu 24.45 | wps 2467 | wpb 2024.1 | bsz 97.5 | num_updates 18000 | best_bleu 24.45
2021-05-05 18:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 18000 updates
2021-05-05 18:09:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_18000.pt
2021-05-05 18:09:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_18000.pt
2021-05-05 18:09:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_18000.pt (epoch 1 @ 18000 updates, score 24.45) (writing took 14.615053906993126 seconds)
2021-05-05 18:09:48 | INFO | train_inner | epoch 001:  18106 / 60421 loss=1.844, ppl=3.59, wps=3644.8, ups=0.98, wpb=3713.4, bsz=145.8, num_updates=18100, lr=0.00023505, gnorm=1.994, loss_scale=2, train_wall=23, gb_free=10.8, wall=5562
2021-05-05 18:10:12 | INFO | train_inner | epoch 001:  18206 / 60421 loss=1.888, ppl=3.7, wps=16189.7, ups=4.32, wpb=3747.2, bsz=131.4, num_updates=18200, lr=0.000234404, gnorm=1.849, loss_scale=2, train_wall=23, gb_free=10.9, wall=5586
2021-05-05 18:10:35 | INFO | train_inner | epoch 001:  18306 / 60421 loss=1.896, ppl=3.72, wps=15973.3, ups=4.23, wpb=3777.6, bsz=141.8, num_updates=18300, lr=0.000233762, gnorm=1.749, loss_scale=2, train_wall=23, gb_free=10.8, wall=5609
2021-05-05 18:10:59 | INFO | train_inner | epoch 001:  18406 / 60421 loss=1.901, ppl=3.73, wps=15696.9, ups=4.22, wpb=3724, bsz=121.4, num_updates=18400, lr=0.000233126, gnorm=2.029, loss_scale=2, train_wall=24, gb_free=10.7, wall=5633
2021-05-05 18:11:22 | INFO | train_inner | epoch 001:  18506 / 60421 loss=1.735, ppl=3.33, wps=16267.1, ups=4.3, wpb=3784.9, bsz=137, num_updates=18500, lr=0.000232495, gnorm=1.569, loss_scale=2, train_wall=23, gb_free=10.7, wall=5656
2021-05-05 18:11:45 | INFO | train_inner | epoch 001:  18606 / 60421 loss=1.814, ppl=3.52, wps=16331.4, ups=4.38, wpb=3729.4, bsz=152, num_updates=18600, lr=0.000231869, gnorm=1.739, loss_scale=2, train_wall=23, gb_free=10.6, wall=5679
2021-05-05 18:12:08 | INFO | train_inner | epoch 001:  18706 / 60421 loss=1.802, ppl=3.49, wps=16500.5, ups=4.38, wpb=3765.8, bsz=121.1, num_updates=18700, lr=0.000231249, gnorm=1.615, loss_scale=2, train_wall=23, gb_free=10.6, wall=5702
2021-05-05 18:12:31 | INFO | train_inner | epoch 001:  18806 / 60421 loss=1.825, ppl=3.54, wps=16379.2, ups=4.41, wpb=3715.1, bsz=127.2, num_updates=18800, lr=0.000230633, gnorm=1.681, loss_scale=2, train_wall=22, gb_free=10.8, wall=5724
2021-05-05 18:12:53 | INFO | train_inner | epoch 001:  18906 / 60421 loss=1.809, ppl=3.5, wps=16415, ups=4.41, wpb=3720.8, bsz=125, num_updates=18900, lr=0.000230022, gnorm=1.93, loss_scale=2, train_wall=22, gb_free=10.8, wall=5747
2021-05-05 18:13:16 | INFO | train_inner | epoch 001:  19006 / 60421 loss=1.756, ppl=3.38, wps=16562.1, ups=4.39, wpb=3776.2, bsz=135.9, num_updates=19000, lr=0.000229416, gnorm=1.759, loss_scale=2, train_wall=23, gb_free=10.6, wall=5770
2021-05-05 18:13:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:13:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:13:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:13:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:13:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:13:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:13:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:13:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:13:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:13:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:13:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:13:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:13:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:14:23 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.832 | ppl 14.24 | bleu 23.66 | wps 2366.3 | wpb 2024.1 | bsz 97.5 | num_updates 19000 | best_bleu 24.45
2021-05-05 18:14:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 19000 updates
2021-05-05 18:14:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_19000.pt
2021-05-05 18:14:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_19000.pt
2021-05-05 18:14:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_19000.pt (epoch 1 @ 19000 updates, score 23.66) (writing took 8.490793556993594 seconds)
2021-05-05 18:14:55 | INFO | train_inner | epoch 001:  19106 / 60421 loss=1.915, ppl=3.77, wps=3763.8, ups=1.01, wpb=3713, bsz=126.3, num_updates=19100, lr=0.000228814, gnorm=1.982, loss_scale=2, train_wall=23, gb_free=10.8, wall=5869
2021-05-05 18:15:18 | INFO | train_inner | epoch 001:  19206 / 60421 loss=1.949, ppl=3.86, wps=16009.4, ups=4.32, wpb=3708.4, bsz=138.2, num_updates=19200, lr=0.000228218, gnorm=2.099, loss_scale=2, train_wall=23, gb_free=10.7, wall=5892
2021-05-05 18:15:41 | INFO | train_inner | epoch 001:  19306 / 60421 loss=1.846, ppl=3.6, wps=15908.4, ups=4.26, wpb=3737.2, bsz=114.6, num_updates=19300, lr=0.000227626, gnorm=1.651, loss_scale=2, train_wall=23, gb_free=10.8, wall=5915
2021-05-05 18:16:05 | INFO | train_inner | epoch 001:  19406 / 60421 loss=1.847, ppl=3.6, wps=15802, ups=4.27, wpb=3704.5, bsz=150.8, num_updates=19400, lr=0.000227038, gnorm=1.829, loss_scale=2, train_wall=23, gb_free=10.8, wall=5939
2021-05-05 18:16:28 | INFO | train_inner | epoch 001:  19506 / 60421 loss=1.774, ppl=3.42, wps=16367.6, ups=4.27, wpb=3837.3, bsz=138.6, num_updates=19500, lr=0.000226455, gnorm=1.579, loss_scale=2, train_wall=23, gb_free=10.7, wall=5962
2021-05-05 18:16:51 | INFO | train_inner | epoch 001:  19606 / 60421 loss=1.865, ppl=3.64, wps=16364.8, ups=4.38, wpb=3737.8, bsz=134.6, num_updates=19600, lr=0.000225877, gnorm=1.767, loss_scale=2, train_wall=23, gb_free=10.8, wall=5985
2021-05-05 18:17:14 | INFO | train_inner | epoch 001:  19706 / 60421 loss=1.77, ppl=3.41, wps=16448.9, ups=4.4, wpb=3742.3, bsz=110.8, num_updates=19700, lr=0.000225303, gnorm=1.791, loss_scale=2, train_wall=23, gb_free=11.2, wall=6008
2021-05-05 18:17:37 | INFO | train_inner | epoch 001:  19806 / 60421 loss=1.813, ppl=3.51, wps=16531.1, ups=4.37, wpb=3782.4, bsz=122.3, num_updates=19800, lr=0.000224733, gnorm=1.885, loss_scale=2, train_wall=23, gb_free=10.8, wall=6031
2021-05-05 18:18:00 | INFO | train_inner | epoch 001:  19906 / 60421 loss=1.763, ppl=3.39, wps=16395.9, ups=4.36, wpb=3762.3, bsz=137.3, num_updates=19900, lr=0.000224168, gnorm=1.648, loss_scale=2, train_wall=23, gb_free=10.8, wall=6054
2021-05-05 18:18:23 | INFO | train_inner | epoch 001:  20006 / 60421 loss=1.795, ppl=3.47, wps=16502, ups=4.39, wpb=3763.2, bsz=133.8, num_updates=20000, lr=0.000223607, gnorm=1.701, loss_scale=2, train_wall=23, gb_free=10.8, wall=6076
2021-05-05 18:18:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:18:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:18:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:18:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:18:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:18:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:18:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:18:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:18:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:18:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:18:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:18:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:18:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:19:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.78 | ppl 13.73 | bleu 25.01 | wps 2462.4 | wpb 2024.1 | bsz 97.5 | num_updates 20000 | best_bleu 25.01
2021-05-05 18:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 20000 updates
2021-05-05 18:19:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_20000.pt
2021-05-05 18:19:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_20000.pt
2021-05-05 18:19:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_20000.pt (epoch 1 @ 20000 updates, score 25.01) (writing took 14.62340137900901 seconds)
2021-05-05 18:20:05 | INFO | train_inner | epoch 001:  20106 / 60421 loss=1.759, ppl=3.38, wps=3716.4, ups=0.98, wpb=3798.2, bsz=125.7, num_updates=20100, lr=0.00022305, gnorm=1.63, loss_scale=2, train_wall=23, gb_free=10.7, wall=6179
2021-05-05 18:20:28 | INFO | train_inner | epoch 001:  20206 / 60421 loss=1.905, ppl=3.75, wps=16161.7, ups=4.33, wpb=3730.9, bsz=127.7, num_updates=20200, lr=0.000222497, gnorm=1.862, loss_scale=2, train_wall=23, gb_free=10.9, wall=6202
2021-05-05 18:20:51 | INFO | train_inner | epoch 001:  20306 / 60421 loss=1.891, ppl=3.71, wps=15792, ups=4.26, wpb=3708.6, bsz=114.9, num_updates=20300, lr=0.000221948, gnorm=1.851, loss_scale=2, train_wall=23, gb_free=10.9, wall=6225
2021-05-05 18:21:15 | INFO | train_inner | epoch 001:  20406 / 60421 loss=1.709, ppl=3.27, wps=15972.2, ups=4.29, wpb=3723.2, bsz=127.5, num_updates=20400, lr=0.000221404, gnorm=1.737, loss_scale=2, train_wall=23, gb_free=10.8, wall=6248
2021-05-05 18:21:38 | INFO | train_inner | epoch 001:  20506 / 60421 loss=1.811, ppl=3.51, wps=16328.9, ups=4.36, wpb=3745.8, bsz=128.8, num_updates=20500, lr=0.000220863, gnorm=1.78, loss_scale=2, train_wall=23, gb_free=10.8, wall=6271
2021-05-05 18:22:00 | INFO | train_inner | epoch 001:  20606 / 60421 loss=1.959, ppl=3.89, wps=16522.9, ups=4.41, wpb=3748.6, bsz=129.4, num_updates=20600, lr=0.000220326, gnorm=1.909, loss_scale=2, train_wall=22, gb_free=11, wall=6294
2021-05-05 18:22:23 | INFO | train_inner | epoch 001:  20706 / 60421 loss=1.74, ppl=3.34, wps=16622.8, ups=4.35, wpb=3818.2, bsz=126.4, num_updates=20700, lr=0.000219793, gnorm=1.487, loss_scale=2, train_wall=23, gb_free=11, wall=6317
2021-05-05 18:22:46 | INFO | train_inner | epoch 001:  20806 / 60421 loss=1.747, ppl=3.36, wps=16489, ups=4.39, wpb=3754, bsz=126.6, num_updates=20800, lr=0.000219265, gnorm=1.768, loss_scale=2, train_wall=23, gb_free=10.8, wall=6340
2021-05-05 18:23:09 | INFO | train_inner | epoch 001:  20906 / 60421 loss=1.753, ppl=3.37, wps=16328.4, ups=4.38, wpb=3728.5, bsz=134.9, num_updates=20900, lr=0.000218739, gnorm=1.79, loss_scale=2, train_wall=23, gb_free=11.5, wall=6363
2021-05-05 18:23:32 | INFO | train_inner | epoch 001:  21006 / 60421 loss=1.707, ppl=3.27, wps=16426.9, ups=4.34, wpb=3781.5, bsz=136.8, num_updates=21000, lr=0.000218218, gnorm=1.647, loss_scale=2, train_wall=23, gb_free=10.6, wall=6386
2021-05-05 18:23:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:23:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:23:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:23:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:23:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:23:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:23:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:23:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:23:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:23:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:24:37 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.793 | ppl 13.86 | bleu 24.95 | wps 2415.8 | wpb 2024.1 | bsz 97.5 | num_updates 21000 | best_bleu 25.01
2021-05-05 18:24:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 21000 updates
2021-05-05 18:24:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_21000.pt
2021-05-05 18:24:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_21000.pt
2021-05-05 18:24:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_21000.pt (epoch 1 @ 21000 updates, score 24.95) (writing took 7.905029911999009 seconds)
2021-05-05 18:25:08 | INFO | train_inner | epoch 001:  21106 / 60421 loss=1.741, ppl=3.34, wps=3863.5, ups=1.03, wpb=3733.1, bsz=135.6, num_updates=21100, lr=0.0002177, gnorm=1.997, loss_scale=2, train_wall=23, gb_free=11.2, wall=6482
2021-05-05 18:25:32 | INFO | train_inner | epoch 001:  21206 / 60421 loss=1.805, ppl=3.49, wps=16008.8, ups=4.3, wpb=3727.2, bsz=130.8, num_updates=21200, lr=0.000217186, gnorm=1.921, loss_scale=2, train_wall=23, gb_free=10.7, wall=6506
2021-05-05 18:25:55 | INFO | train_inner | epoch 001:  21306 / 60421 loss=1.726, ppl=3.31, wps=15870.8, ups=4.22, wpb=3764, bsz=135.2, num_updates=21300, lr=0.000216676, gnorm=1.623, loss_scale=2, train_wall=24, gb_free=10.9, wall=6529
2021-05-05 18:26:19 | INFO | train_inner | epoch 001:  21406 / 60421 loss=1.833, ppl=3.56, wps=16178.9, ups=4.3, wpb=3766.6, bsz=130.2, num_updates=21400, lr=0.000216169, gnorm=1.89, loss_scale=2, train_wall=23, gb_free=11.1, wall=6553
2021-05-05 18:26:42 | INFO | train_inner | epoch 001:  21506 / 60421 loss=1.763, ppl=3.39, wps=16205.3, ups=4.38, wpb=3700.5, bsz=148.6, num_updates=21500, lr=0.000215666, gnorm=1.777, loss_scale=2, train_wall=23, gb_free=10.8, wall=6575
2021-05-05 18:27:04 | INFO | train_inner | epoch 001:  21606 / 60421 loss=1.875, ppl=3.67, wps=16254, ups=4.49, wpb=3622, bsz=127.3, num_updates=21600, lr=0.000215166, gnorm=2.421, loss_scale=4, train_wall=22, gb_free=10.9, wall=6598
2021-05-05 18:27:27 | INFO | train_inner | epoch 001:  21706 / 60421 loss=1.802, ppl=3.49, wps=16300, ups=4.38, wpb=3721.4, bsz=142.2, num_updates=21700, lr=0.000214669, gnorm=1.902, loss_scale=4, train_wall=23, gb_free=10.8, wall=6621
2021-05-05 18:27:50 | INFO | train_inner | epoch 001:  21806 / 60421 loss=1.66, ppl=3.16, wps=16528.5, ups=4.38, wpb=3770.4, bsz=139.6, num_updates=21800, lr=0.000214176, gnorm=1.464, loss_scale=4, train_wall=23, gb_free=10.9, wall=6643
2021-05-05 18:28:12 | INFO | train_inner | epoch 001:  21906 / 60421 loss=1.74, ppl=3.34, wps=16497, ups=4.36, wpb=3780, bsz=119.9, num_updates=21900, lr=0.000213687, gnorm=1.703, loss_scale=4, train_wall=23, gb_free=10.9, wall=6666
2021-05-05 18:28:35 | INFO | train_inner | epoch 001:  22006 / 60421 loss=1.727, ppl=3.31, wps=16292.6, ups=4.37, wpb=3726.4, bsz=125, num_updates=22000, lr=0.000213201, gnorm=1.85, loss_scale=4, train_wall=23, gb_free=10.8, wall=6689
2021-05-05 18:28:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:28:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:28:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:28:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:28:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:28:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:28:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:28:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:28:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:28:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:28:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:28:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:28:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:29:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.787 | ppl 13.8 | bleu 25.15 | wps 2419.1 | wpb 2024.1 | bsz 97.5 | num_updates 22000 | best_bleu 25.15
2021-05-05 18:29:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 22000 updates
2021-05-05 18:29:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_22000.pt
2021-05-05 18:29:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_22000.pt
2021-05-05 18:29:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_22000.pt (epoch 1 @ 22000 updates, score 25.15) (writing took 14.619520436011953 seconds)
2021-05-05 18:30:19 | INFO | train_inner | epoch 001:  22106 / 60421 loss=1.87, ppl=3.66, wps=3548.6, ups=0.97, wpb=3662.7, bsz=137.1, num_updates=22100, lr=0.000212718, gnorm=2.172, loss_scale=4, train_wall=23, gb_free=10.8, wall=6792
2021-05-05 18:30:42 | INFO | train_inner | epoch 001:  22206 / 60421 loss=1.756, ppl=3.38, wps=15963, ups=4.25, wpb=3757.2, bsz=144.4, num_updates=22200, lr=0.000212238, gnorm=1.664, loss_scale=4, train_wall=23, gb_free=10.8, wall=6816
2021-05-05 18:31:06 | INFO | train_inner | epoch 001:  22306 / 60421 loss=1.777, ppl=3.43, wps=15862.9, ups=4.25, wpb=3734.5, bsz=130.2, num_updates=22300, lr=0.000211762, gnorm=1.812, loss_scale=4, train_wall=23, gb_free=11, wall=6839
2021-05-05 18:31:28 | INFO | train_inner | epoch 001:  22406 / 60421 loss=1.713, ppl=3.28, wps=16228.6, ups=4.4, wpb=3689, bsz=128.2, num_updates=22400, lr=0.000211289, gnorm=1.843, loss_scale=4, train_wall=23, gb_free=11.3, wall=6862
2021-05-05 18:31:51 | INFO | train_inner | epoch 001:  22506 / 60421 loss=1.786, ppl=3.45, wps=16514.9, ups=4.36, wpb=3787.8, bsz=119.8, num_updates=22500, lr=0.000210819, gnorm=1.78, loss_scale=4, train_wall=23, gb_free=10.8, wall=6885
2021-05-05 18:32:14 | INFO | train_inner | epoch 001:  22606 / 60421 loss=1.813, ppl=3.51, wps=16247, ups=4.42, wpb=3674.1, bsz=153.7, num_updates=22600, lr=0.000210352, gnorm=1.829, loss_scale=4, train_wall=22, gb_free=10.7, wall=6908
2021-05-05 18:32:36 | INFO | train_inner | epoch 001:  22706 / 60421 loss=1.727, ppl=3.31, wps=16199.4, ups=4.43, wpb=3660.5, bsz=126.3, num_updates=22700, lr=0.000209888, gnorm=1.866, loss_scale=4, train_wall=22, gb_free=10.9, wall=6930
2021-05-05 18:32:59 | INFO | train_inner | epoch 001:  22806 / 60421 loss=1.726, ppl=3.31, wps=16345.1, ups=4.38, wpb=3728, bsz=122.7, num_updates=22800, lr=0.000209427, gnorm=1.779, loss_scale=4, train_wall=23, gb_free=10.7, wall=6953
2021-05-05 18:33:22 | INFO | train_inner | epoch 001:  22906 / 60421 loss=1.595, ppl=3.02, wps=16419, ups=4.34, wpb=3782.8, bsz=147, num_updates=22900, lr=0.000208969, gnorm=1.578, loss_scale=4, train_wall=23, gb_free=11, wall=6976
2021-05-05 18:33:45 | INFO | train_inner | epoch 001:  23006 / 60421 loss=1.837, ppl=3.57, wps=16234.5, ups=4.38, wpb=3707.7, bsz=119.7, num_updates=23000, lr=0.000208514, gnorm=2.148, loss_scale=4, train_wall=23, gb_free=10.7, wall=6999
2021-05-05 18:33:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:34:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:34:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:34:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:34:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:34:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:34:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:34:51 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.836 | ppl 14.28 | bleu 24.69 | wps 2392.7 | wpb 2024.1 | bsz 97.5 | num_updates 23000 | best_bleu 25.15
2021-05-05 18:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 23000 updates
2021-05-05 18:34:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_23000.pt
2021-05-05 18:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_23000.pt
2021-05-05 18:35:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_23000.pt (epoch 1 @ 23000 updates, score 24.69) (writing took 8.479618038996705 seconds)
2021-05-05 18:35:23 | INFO | train_inner | epoch 001:  23106 / 60421 loss=1.953, ppl=3.87, wps=3778.2, ups=1.02, wpb=3696.1, bsz=125.8, num_updates=23100, lr=0.000208063, gnorm=2.425, loss_scale=4, train_wall=23, gb_free=10.7, wall=7097
2021-05-05 18:35:47 | INFO | train_inner | epoch 001:  23206 / 60421 loss=1.745, ppl=3.35, wps=15912.1, ups=4.24, wpb=3755.1, bsz=142.5, num_updates=23200, lr=0.000207614, gnorm=1.876, loss_scale=4, train_wall=23, gb_free=10.9, wall=7120
2021-05-05 18:36:10 | INFO | train_inner | epoch 001:  23306 / 60421 loss=1.58, ppl=2.99, wps=15981.4, ups=4.2, wpb=3803.2, bsz=144.9, num_updates=23300, lr=0.000207168, gnorm=1.386, loss_scale=4, train_wall=24, gb_free=10.7, wall=7144
2021-05-05 18:36:34 | INFO | train_inner | epoch 001:  23406 / 60421 loss=1.568, ppl=2.96, wps=16294.5, ups=4.3, wpb=3786.2, bsz=150.5, num_updates=23400, lr=0.000206725, gnorm=1.367, loss_scale=4, train_wall=23, gb_free=10.8, wall=7167
2021-05-05 18:36:56 | INFO | train_inner | epoch 001:  23506 / 60421 loss=1.617, ppl=3.07, wps=16545.7, ups=4.38, wpb=3781.2, bsz=132.1, num_updates=23500, lr=0.000206284, gnorm=1.406, loss_scale=4, train_wall=23, gb_free=11, wall=7190
2021-05-05 18:37:19 | INFO | train_inner | epoch 001:  23606 / 60421 loss=1.617, ppl=3.07, wps=16447.1, ups=4.35, wpb=3782.3, bsz=147.6, num_updates=23600, lr=0.000205847, gnorm=1.593, loss_scale=4, train_wall=23, gb_free=10.7, wall=7213
2021-05-05 18:37:42 | INFO | train_inner | epoch 001:  23706 / 60421 loss=1.848, ppl=3.6, wps=16357.1, ups=4.39, wpb=3725.8, bsz=128.2, num_updates=23700, lr=0.000205412, gnorm=2.233, loss_scale=4, train_wall=23, gb_free=10.7, wall=7236
2021-05-05 18:38:05 | INFO | train_inner | epoch 001:  23806 / 60421 loss=1.79, ppl=3.46, wps=16321, ups=4.42, wpb=3692.6, bsz=137.3, num_updates=23800, lr=0.00020498, gnorm=2.053, loss_scale=4, train_wall=22, gb_free=11.5, wall=7259
2021-05-05 18:38:28 | INFO | train_inner | epoch 001:  23906 / 60421 loss=1.8, ppl=3.48, wps=16193.7, ups=4.39, wpb=3690.3, bsz=141.6, num_updates=23900, lr=0.000204551, gnorm=2.483, loss_scale=4, train_wall=23, gb_free=10.8, wall=7282
2021-05-05 18:38:51 | INFO | train_inner | epoch 001:  24006 / 60421 loss=1.831, ppl=3.56, wps=16265.4, ups=4.37, wpb=3720.6, bsz=125.1, num_updates=24000, lr=0.000204124, gnorm=1.786, loss_scale=4, train_wall=23, gb_free=11.2, wall=7304
2021-05-05 18:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:39:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:39:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:39:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:39:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:39:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:39:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:39:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:39:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:39:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:39:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:39:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:39:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:39:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.733 | ppl 13.3 | bleu 25.25 | wps 2420.2 | wpb 2024.1 | bsz 97.5 | num_updates 24000 | best_bleu 25.25
2021-05-05 18:39:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 24000 updates
2021-05-05 18:39:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_24000.pt
2021-05-05 18:39:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_24000.pt
2021-05-05 18:40:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_24000.pt (epoch 1 @ 24000 updates, score 25.25) (writing took 14.595609783995315 seconds)
2021-05-05 18:40:34 | INFO | train_inner | epoch 001:  24106 / 60421 loss=1.654, ppl=3.15, wps=3630.4, ups=0.97, wpb=3755.9, bsz=132.6, num_updates=24100, lr=0.0002037, gnorm=1.678, loss_scale=4, train_wall=23, gb_free=10.7, wall=7408
2021-05-05 18:40:58 | INFO | train_inner | epoch 001:  24206 / 60421 loss=1.737, ppl=3.33, wps=15882.7, ups=4.24, wpb=3746.7, bsz=137.4, num_updates=24200, lr=0.000203279, gnorm=1.839, loss_scale=4, train_wall=23, gb_free=10.9, wall=7431
2021-05-05 18:41:21 | INFO | train_inner | epoch 001:  24306 / 60421 loss=1.707, ppl=3.26, wps=16251.9, ups=4.3, wpb=3775.1, bsz=150.1, num_updates=24300, lr=0.00020286, gnorm=1.925, loss_scale=4, train_wall=23, gb_free=10.8, wall=7455
2021-05-05 18:41:44 | INFO | train_inner | epoch 001:  24406 / 60421 loss=1.662, ppl=3.17, wps=16460, ups=4.32, wpb=3812.4, bsz=118.2, num_updates=24400, lr=0.000202444, gnorm=1.521, loss_scale=4, train_wall=23, gb_free=10.8, wall=7478
2021-05-05 18:42:07 | INFO | train_inner | epoch 001:  24506 / 60421 loss=1.638, ppl=3.11, wps=16390.8, ups=4.38, wpb=3744.3, bsz=136.5, num_updates=24500, lr=0.000202031, gnorm=1.746, loss_scale=4, train_wall=23, gb_free=10.8, wall=7501
2021-05-05 18:42:30 | INFO | train_inner | epoch 001:  24606 / 60421 loss=1.676, ppl=3.2, wps=16481.1, ups=4.38, wpb=3766.5, bsz=122.6, num_updates=24600, lr=0.000201619, gnorm=1.685, loss_scale=4, train_wall=23, gb_free=10.9, wall=7524
2021-05-05 18:42:53 | INFO | train_inner | epoch 001:  24706 / 60421 loss=1.594, ppl=3.02, wps=16320.4, ups=4.36, wpb=3741.6, bsz=125.7, num_updates=24700, lr=0.000201211, gnorm=1.575, loss_scale=4, train_wall=23, gb_free=10.8, wall=7546
2021-05-05 18:43:16 | INFO | train_inner | epoch 001:  24806 / 60421 loss=1.525, ppl=2.88, wps=16419.7, ups=4.32, wpb=3800.2, bsz=149.4, num_updates=24800, lr=0.000200805, gnorm=1.404, loss_scale=4, train_wall=23, gb_free=11.3, wall=7570
2021-05-05 18:43:39 | INFO | train_inner | epoch 001:  24906 / 60421 loss=1.759, ppl=3.39, wps=16309.3, ups=4.34, wpb=3759.7, bsz=133, num_updates=24900, lr=0.000200401, gnorm=1.996, loss_scale=4, train_wall=23, gb_free=10.8, wall=7593
2021-05-05 18:44:02 | INFO | train_inner | epoch 001:  25006 / 60421 loss=1.664, ppl=3.17, wps=16277.2, ups=4.37, wpb=3727.6, bsz=153.1, num_updates=25000, lr=0.0002, gnorm=1.692, loss_scale=4, train_wall=23, gb_free=10.6, wall=7616
2021-05-05 18:44:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:44:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:44:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:44:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:44:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:44:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:44:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:44:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:44:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:44:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:44:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:44:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:44:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:45:06 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.766 | ppl 13.61 | bleu 25.41 | wps 2480.8 | wpb 2024.1 | bsz 97.5 | num_updates 25000 | best_bleu 25.41
2021-05-05 18:45:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 25000 updates
2021-05-05 18:45:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_25000.pt
2021-05-05 18:45:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_25000.pt
2021-05-05 18:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_25000.pt (epoch 1 @ 25000 updates, score 25.41) (writing took 14.642402773999493 seconds)
2021-05-05 18:45:44 | INFO | train_inner | epoch 001:  25106 / 60421 loss=1.636, ppl=3.11, wps=3635, ups=0.98, wpb=3702.9, bsz=118.1, num_updates=25100, lr=0.000199601, gnorm=1.617, loss_scale=4, train_wall=23, gb_free=10.7, wall=7717
2021-05-05 18:46:07 | INFO | train_inner | epoch 001:  25206 / 60421 loss=1.758, ppl=3.38, wps=16079.1, ups=4.27, wpb=3764, bsz=115.9, num_updates=25200, lr=0.000199205, gnorm=1.822, loss_scale=4, train_wall=23, gb_free=11.2, wall=7741
2021-05-05 18:46:30 | INFO | train_inner | epoch 001:  25306 / 60421 loss=1.719, ppl=3.29, wps=16248, ups=4.35, wpb=3732.7, bsz=135, num_updates=25300, lr=0.000198811, gnorm=1.917, loss_scale=4, train_wall=23, gb_free=11.3, wall=7764
2021-05-05 18:46:53 | INFO | train_inner | epoch 001:  25406 / 60421 loss=1.681, ppl=3.21, wps=16492.7, ups=4.35, wpb=3789.2, bsz=121.6, num_updates=25400, lr=0.000198419, gnorm=1.659, loss_scale=4, train_wall=23, gb_free=10.9, wall=7787
2021-05-05 18:47:15 | INFO | train_inner | epoch 001:  25506 / 60421 loss=1.751, ppl=3.37, wps=16349.7, ups=4.45, wpb=3676, bsz=116, num_updates=25500, lr=0.00019803, gnorm=2.023, loss_scale=4, train_wall=22, gb_free=10.8, wall=7809
2021-05-05 18:47:38 | INFO | train_inner | epoch 001:  25606 / 60421 loss=1.715, ppl=3.28, wps=16188.4, ups=4.44, wpb=3642.3, bsz=138.9, num_updates=25600, lr=0.000197642, gnorm=1.878, loss_scale=4, train_wall=22, gb_free=10.8, wall=7832
2021-05-05 18:48:00 | INFO | train_inner | epoch 001:  25706 / 60421 loss=1.732, ppl=3.32, wps=16246.6, ups=4.44, wpb=3662.5, bsz=119.8, num_updates=25700, lr=0.000197257, gnorm=1.97, loss_scale=4, train_wall=22, gb_free=10.8, wall=7854
2021-05-05 18:48:23 | INFO | train_inner | epoch 001:  25806 / 60421 loss=1.678, ppl=3.2, wps=16292.7, ups=4.39, wpb=3714, bsz=129.4, num_updates=25800, lr=0.000196875, gnorm=1.791, loss_scale=4, train_wall=23, gb_free=10.8, wall=7877
2021-05-05 18:48:46 | INFO | train_inner | epoch 001:  25906 / 60421 loss=1.782, ppl=3.44, wps=16386.2, ups=4.4, wpb=3721.1, bsz=111.6, num_updates=25900, lr=0.000196494, gnorm=2.122, loss_scale=4, train_wall=23, gb_free=10.7, wall=7900
2021-05-05 18:49:09 | INFO | train_inner | epoch 001:  26006 / 60421 loss=1.646, ppl=3.13, wps=16338.5, ups=4.33, wpb=3771.1, bsz=156.4, num_updates=26000, lr=0.000196116, gnorm=1.615, loss_scale=4, train_wall=23, gb_free=10.8, wall=7923
2021-05-05 18:49:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:49:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:49:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:49:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:49:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:49:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:49:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:49:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:49:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:49:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:50:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.799 | ppl 13.92 | bleu 25.3 | wps 2490.7 | wpb 2024.1 | bsz 97.5 | num_updates 26000 | best_bleu 25.41
2021-05-05 18:50:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 26000 updates
2021-05-05 18:50:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_26000.pt
2021-05-05 18:50:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_26000.pt
2021-05-05 18:50:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_26000.pt (epoch 1 @ 26000 updates, score 25.3) (writing took 8.456304644001648 seconds)
2021-05-05 18:50:45 | INFO | train_inner | epoch 001:  26106 / 60421 loss=1.671, ppl=3.18, wps=3883.5, ups=1.05, wpb=3709.4, bsz=120.6, num_updates=26100, lr=0.00019574, gnorm=1.691, loss_scale=4, train_wall=23, gb_free=10.9, wall=8018
2021-05-05 18:51:08 | INFO | train_inner | epoch 001:  26206 / 60421 loss=1.599, ppl=3.03, wps=15910.7, ups=4.3, wpb=3704.3, bsz=131.8, num_updates=26200, lr=0.000195366, gnorm=1.931, loss_scale=4, train_wall=23, gb_free=10.9, wall=8042
2021-05-05 18:51:31 | INFO | train_inner | epoch 001:  26306 / 60421 loss=1.857, ppl=3.62, wps=16356.4, ups=4.4, wpb=3713.2, bsz=125, num_updates=26300, lr=0.000194994, gnorm=2.277, loss_scale=4, train_wall=23, gb_free=10.8, wall=8064
2021-05-05 18:51:53 | INFO | train_inner | epoch 001:  26406 / 60421 loss=1.764, ppl=3.4, wps=16492.9, ups=4.38, wpb=3768.1, bsz=131, num_updates=26400, lr=0.000194625, gnorm=1.961, loss_scale=4, train_wall=23, gb_free=11, wall=8087
2021-05-05 18:52:16 | INFO | train_inner | epoch 001:  26506 / 60421 loss=1.754, ppl=3.37, wps=16468.8, ups=4.39, wpb=3748.9, bsz=133.1, num_updates=26500, lr=0.000194257, gnorm=2.068, loss_scale=4, train_wall=23, gb_free=10.9, wall=8110
2021-05-05 18:52:39 | INFO | train_inner | epoch 001:  26606 / 60421 loss=1.587, ppl=3, wps=16546.8, ups=4.39, wpb=3767.7, bsz=125.1, num_updates=26600, lr=0.000193892, gnorm=1.558, loss_scale=4, train_wall=23, gb_free=10.9, wall=8133
2021-05-05 18:53:01 | INFO | train_inner | epoch 001:  26706 / 60421 loss=1.631, ppl=3.1, wps=16349.1, ups=4.44, wpb=3679.8, bsz=134.1, num_updates=26700, lr=0.000193528, gnorm=1.796, loss_scale=4, train_wall=22, gb_free=10.6, wall=8155
2021-05-05 18:53:24 | INFO | train_inner | epoch 001:  26806 / 60421 loss=1.647, ppl=3.13, wps=16438.2, ups=4.37, wpb=3757.9, bsz=124.6, num_updates=26800, lr=0.000193167, gnorm=1.615, loss_scale=4, train_wall=23, gb_free=11.1, wall=8178
2021-05-05 18:53:47 | INFO | train_inner | epoch 001:  26906 / 60421 loss=1.646, ppl=3.13, wps=16715, ups=4.33, wpb=3857.5, bsz=117, num_updates=26900, lr=0.000192807, gnorm=1.511, loss_scale=4, train_wall=23, gb_free=10.7, wall=8201
2021-05-05 18:54:10 | INFO | train_inner | epoch 001:  27006 / 60421 loss=1.668, ppl=3.18, wps=16165.2, ups=4.33, wpb=3730.4, bsz=129, num_updates=27000, lr=0.00019245, gnorm=1.729, loss_scale=4, train_wall=23, gb_free=10.7, wall=8224
2021-05-05 18:54:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:54:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:54:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:54:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:54:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:54:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:54:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:54:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:54:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:54:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:55:16 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.804 | ppl 13.96 | bleu 25.87 | wps 2424.9 | wpb 2024.1 | bsz 97.5 | num_updates 27000 | best_bleu 25.87
2021-05-05 18:55:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 27000 updates
2021-05-05 18:55:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_27000.pt
2021-05-05 18:55:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_27000.pt
2021-05-05 18:55:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_27000.pt (epoch 1 @ 27000 updates, score 25.87) (writing took 14.636725813004887 seconds)
2021-05-05 18:55:54 | INFO | train_inner | epoch 001:  27106 / 60421 loss=1.633, ppl=3.1, wps=3646.6, ups=0.96, wpb=3779.6, bsz=145.5, num_updates=27100, lr=0.000192095, gnorm=1.737, loss_scale=4, train_wall=23, gb_free=10.8, wall=8328
2021-05-05 18:56:17 | INFO | train_inner | epoch 001:  27206 / 60421 loss=1.735, ppl=3.33, wps=16356.5, ups=4.41, wpb=3710.3, bsz=117.6, num_updates=27200, lr=0.000191741, gnorm=2.053, loss_scale=4, train_wall=22, gb_free=10.9, wall=8351
2021-05-05 18:56:40 | INFO | train_inner | epoch 001:  27306 / 60421 loss=1.597, ppl=3.02, wps=16231.9, ups=4.41, wpb=3680.8, bsz=137.4, num_updates=27300, lr=0.00019139, gnorm=1.785, loss_scale=4, train_wall=22, gb_free=10.8, wall=8373
2021-05-05 18:57:02 | INFO | train_inner | epoch 001:  27406 / 60421 loss=1.547, ppl=2.92, wps=16346.8, ups=4.4, wpb=3716.2, bsz=153, num_updates=27400, lr=0.00019104, gnorm=1.848, loss_scale=4, train_wall=23, gb_free=10.9, wall=8396
2021-05-05 18:57:25 | INFO | train_inner | epoch 001:  27506 / 60421 loss=1.57, ppl=2.97, wps=16597.9, ups=4.42, wpb=3755.2, bsz=136.9, num_updates=27500, lr=0.000190693, gnorm=1.598, loss_scale=4, train_wall=22, gb_free=10.6, wall=8419
2021-05-05 18:57:47 | INFO | train_inner | epoch 001:  27606 / 60421 loss=1.874, ppl=3.66, wps=16393.2, ups=4.43, wpb=3698.6, bsz=114.5, num_updates=27600, lr=0.000190347, gnorm=2.211, loss_scale=4, train_wall=22, gb_free=10.7, wall=8441
2021-05-05 18:58:10 | INFO | train_inner | epoch 001:  27706 / 60421 loss=1.831, ppl=3.56, wps=16354.3, ups=4.4, wpb=3714.3, bsz=132, num_updates=27700, lr=0.000190003, gnorm=2.139, loss_scale=4, train_wall=23, gb_free=11, wall=8464
2021-05-05 18:58:33 | INFO | train_inner | epoch 001:  27806 / 60421 loss=1.639, ppl=3.11, wps=16556.2, ups=4.37, wpb=3788.6, bsz=147.8, num_updates=27800, lr=0.000189661, gnorm=1.672, loss_scale=4, train_wall=23, gb_free=10.7, wall=8487
2021-05-05 18:58:56 | INFO | train_inner | epoch 001:  27906 / 60421 loss=1.618, ppl=3.07, wps=16553.8, ups=4.37, wpb=3789.2, bsz=116.4, num_updates=27900, lr=0.000189321, gnorm=1.5, loss_scale=4, train_wall=23, gb_free=10.9, wall=8510
2021-05-05 18:59:19 | INFO | train_inner | epoch 001:  28006 / 60421 loss=1.601, ppl=3.03, wps=16272.7, ups=4.37, wpb=3726.7, bsz=123.4, num_updates=28000, lr=0.000188982, gnorm=1.7, loss_scale=4, train_wall=23, gb_free=10.7, wall=8533
2021-05-05 18:59:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 18:59:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:59:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:59:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:59:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:59:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:59:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:59:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:59:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:59:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:59:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:59:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:59:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:59:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:59:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:59:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 18:59:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 18:59:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 18:59:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:00:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.838 | ppl 14.3 | bleu 25.72 | wps 2432.1 | wpb 2024.1 | bsz 97.5 | num_updates 28000 | best_bleu 25.87
2021-05-05 19:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 28000 updates
2021-05-05 19:00:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_28000.pt
2021-05-05 19:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_28000.pt
2021-05-05 19:00:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_28000.pt (epoch 1 @ 28000 updates, score 25.72) (writing took 7.895826508989558 seconds)
2021-05-05 19:00:56 | INFO | train_inner | epoch 001:  28106 / 60421 loss=1.714, ppl=3.28, wps=3898.5, ups=1.03, wpb=3773.9, bsz=119.4, num_updates=28100, lr=0.000188646, gnorm=1.781, loss_scale=4, train_wall=23, gb_free=10.5, wall=8629
2021-05-05 19:01:19 | INFO | train_inner | epoch 001:  28206 / 60421 loss=1.649, ppl=3.14, wps=16064.8, ups=4.29, wpb=3740.7, bsz=143.4, num_updates=28200, lr=0.000188311, gnorm=1.877, loss_scale=4, train_wall=23, gb_free=10.7, wall=8653
2021-05-05 19:01:42 | INFO | train_inner | epoch 001:  28306 / 60421 loss=1.689, ppl=3.22, wps=16201.4, ups=4.43, wpb=3659.7, bsz=127.2, num_updates=28300, lr=0.000187978, gnorm=2.203, loss_scale=4, train_wall=22, gb_free=10.8, wall=8675
2021-05-05 19:02:04 | INFO | train_inner | epoch 001:  28406 / 60421 loss=1.652, ppl=3.14, wps=16569.1, ups=4.43, wpb=3741.9, bsz=125.8, num_updates=28400, lr=0.000187647, gnorm=1.689, loss_scale=4, train_wall=22, gb_free=10.8, wall=8698
2021-05-05 19:02:27 | INFO | train_inner | epoch 001:  28506 / 60421 loss=1.595, ppl=3.02, wps=16527.5, ups=4.43, wpb=3728.7, bsz=133.3, num_updates=28500, lr=0.000187317, gnorm=2.033, loss_scale=4, train_wall=22, gb_free=10.7, wall=8721
2021-05-05 19:02:49 | INFO | train_inner | epoch 001:  28606 / 60421 loss=1.611, ppl=3.05, wps=16638.6, ups=4.39, wpb=3788.9, bsz=110, num_updates=28600, lr=0.000186989, gnorm=1.627, loss_scale=4, train_wall=23, gb_free=10.6, wall=8743
2021-05-05 19:03:12 | INFO | train_inner | epoch 001:  28706 / 60421 loss=1.516, ppl=2.86, wps=16547.8, ups=4.4, wpb=3759.2, bsz=141, num_updates=28700, lr=0.000186663, gnorm=1.513, loss_scale=4, train_wall=23, gb_free=10.7, wall=8766
2021-05-05 19:03:35 | INFO | train_inner | epoch 001:  28806 / 60421 loss=1.604, ppl=3.04, wps=16254.2, ups=4.43, wpb=3667.3, bsz=121.9, num_updates=28800, lr=0.000186339, gnorm=1.874, loss_scale=4, train_wall=22, gb_free=10.9, wall=8789
2021-05-05 19:03:57 | INFO | train_inner | epoch 001:  28906 / 60421 loss=1.639, ppl=3.11, wps=16502, ups=4.41, wpb=3737.9, bsz=130.6, num_updates=28900, lr=0.000186016, gnorm=1.832, loss_scale=4, train_wall=22, gb_free=10.7, wall=8811
2021-05-05 19:04:20 | INFO | train_inner | epoch 001:  29006 / 60421 loss=1.638, ppl=3.11, wps=16289, ups=4.39, wpb=3712.8, bsz=125.3, num_updates=29000, lr=0.000185695, gnorm=1.758, loss_scale=4, train_wall=23, gb_free=10.8, wall=8834
2021-05-05 19:04:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:04:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:04:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:04:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:04:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:04:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:04:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:04:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:04:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:04:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:05:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.827 | ppl 14.19 | bleu 25.71 | wps 2475.7 | wpb 2024.1 | bsz 97.5 | num_updates 29000 | best_bleu 25.87
2021-05-05 19:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 29000 updates
2021-05-05 19:05:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_29000.pt
2021-05-05 19:05:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_29000.pt
2021-05-05 19:05:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_29000.pt (epoch 1 @ 29000 updates, score 25.71) (writing took 7.883270621998236 seconds)
2021-05-05 19:05:56 | INFO | train_inner | epoch 001:  29106 / 60421 loss=1.534, ppl=2.9, wps=3953, ups=1.05, wpb=3780.6, bsz=123.2, num_updates=29100, lr=0.000185376, gnorm=1.524, loss_scale=4, train_wall=23, gb_free=10.7, wall=8930
2021-05-05 19:06:19 | INFO | train_inner | epoch 001:  29206 / 60421 loss=1.54, ppl=2.91, wps=16412.1, ups=4.32, wpb=3802.1, bsz=146.8, num_updates=29200, lr=0.000185058, gnorm=1.508, loss_scale=4, train_wall=23, gb_free=10.6, wall=8953
2021-05-05 19:06:42 | INFO | train_inner | epoch 001:  29306 / 60421 loss=1.525, ppl=2.88, wps=16457.2, ups=4.36, wpb=3777.9, bsz=131.4, num_updates=29300, lr=0.000184742, gnorm=1.529, loss_scale=4, train_wall=23, gb_free=10.9, wall=8976
2021-05-05 19:07:05 | INFO | train_inner | epoch 001:  29406 / 60421 loss=1.645, ppl=3.13, wps=16270.9, ups=4.41, wpb=3686.1, bsz=152.6, num_updates=29400, lr=0.000184428, gnorm=1.931, loss_scale=4, train_wall=22, gb_free=10.8, wall=8998
2021-05-05 19:07:27 | INFO | train_inner | epoch 001:  29506 / 60421 loss=1.579, ppl=2.99, wps=16554.3, ups=4.42, wpb=3742, bsz=130.6, num_updates=29500, lr=0.000184115, gnorm=1.698, loss_scale=4, train_wall=22, gb_free=10.9, wall=9021
2021-05-05 19:07:50 | INFO | train_inner | epoch 001:  29606 / 60421 loss=1.531, ppl=2.89, wps=16566.8, ups=4.41, wpb=3754.2, bsz=125.4, num_updates=29600, lr=0.000183804, gnorm=1.615, loss_scale=4, train_wall=22, gb_free=10.9, wall=9044
2021-05-05 19:08:12 | INFO | train_inner | epoch 001:  29706 / 60421 loss=1.643, ppl=3.12, wps=16389.3, ups=4.45, wpb=3683.7, bsz=127.4, num_updates=29700, lr=0.000183494, gnorm=1.883, loss_scale=4, train_wall=22, gb_free=10.9, wall=9066
2021-05-05 19:08:35 | INFO | train_inner | epoch 001:  29806 / 60421 loss=1.555, ppl=2.94, wps=16372.9, ups=4.42, wpb=3700.3, bsz=127.6, num_updates=29800, lr=0.000183186, gnorm=1.887, loss_scale=4, train_wall=22, gb_free=11.1, wall=9089
2021-05-05 19:08:58 | INFO | train_inner | epoch 001:  29906 / 60421 loss=1.672, ppl=3.19, wps=16559.8, ups=4.35, wpb=3804.4, bsz=141.4, num_updates=29900, lr=0.000182879, gnorm=1.811, loss_scale=4, train_wall=23, gb_free=10.8, wall=9112
2021-05-05 19:09:21 | INFO | train_inner | epoch 001:  30006 / 60421 loss=1.622, ppl=3.08, wps=16176, ups=4.4, wpb=3672.9, bsz=141.6, num_updates=30000, lr=0.000182574, gnorm=2.056, loss_scale=4, train_wall=23, gb_free=10.9, wall=9134
2021-05-05 19:09:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:09:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:09:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:09:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:09:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:09:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:09:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:10:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.826 | ppl 14.18 | bleu 25.33 | wps 2470.9 | wpb 2024.1 | bsz 97.5 | num_updates 30000 | best_bleu 25.87
2021-05-05 19:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30000 updates
2021-05-05 19:10:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_30000.pt
2021-05-05 19:10:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_30000.pt
2021-05-05 19:10:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_30000.pt (epoch 1 @ 30000 updates, score 25.33) (writing took 7.864947372989263 seconds)
2021-05-05 19:10:56 | INFO | train_inner | epoch 001:  30106 / 60421 loss=1.537, ppl=2.9, wps=3920.1, ups=1.05, wpb=3746.5, bsz=138.8, num_updates=30100, lr=0.000182271, gnorm=1.98, loss_scale=4, train_wall=23, gb_free=11, wall=9230
2021-05-05 19:11:19 | INFO | train_inner | epoch 001:  30206 / 60421 loss=1.604, ppl=3.04, wps=16240.5, ups=4.36, wpb=3720.8, bsz=130.6, num_updates=30200, lr=0.000181969, gnorm=1.763, loss_scale=4, train_wall=23, gb_free=10.9, wall=9253
2021-05-05 19:11:42 | INFO | train_inner | epoch 001:  30306 / 60421 loss=1.557, ppl=2.94, wps=16269.3, ups=4.42, wpb=3684.9, bsz=135.7, num_updates=30300, lr=0.000181668, gnorm=1.793, loss_scale=4, train_wall=22, gb_free=11.1, wall=9276
2021-05-05 19:12:04 | INFO | train_inner | epoch 001:  30406 / 60421 loss=1.711, ppl=3.27, wps=16470.1, ups=4.42, wpb=3723.4, bsz=122.4, num_updates=30400, lr=0.000181369, gnorm=1.913, loss_scale=4, train_wall=22, gb_free=10.7, wall=9298
2021-05-05 19:12:27 | INFO | train_inner | epoch 001:  30506 / 60421 loss=1.592, ppl=3.02, wps=16522.7, ups=4.45, wpb=3716.7, bsz=127, num_updates=30500, lr=0.000181071, gnorm=1.834, loss_scale=4, train_wall=22, gb_free=10.8, wall=9321
2021-05-05 19:12:49 | INFO | train_inner | epoch 001:  30606 / 60421 loss=1.578, ppl=2.98, wps=16620, ups=4.43, wpb=3748.6, bsz=120.2, num_updates=30600, lr=0.000180775, gnorm=1.601, loss_scale=4, train_wall=22, gb_free=10.7, wall=9343
2021-05-05 19:13:12 | INFO | train_inner | epoch 001:  30706 / 60421 loss=1.712, ppl=3.28, wps=16287.4, ups=4.43, wpb=3676.7, bsz=133.4, num_updates=30700, lr=0.000180481, gnorm=2.127, loss_scale=4, train_wall=22, gb_free=10.7, wall=9366
2021-05-05 19:13:34 | INFO | train_inner | epoch 001:  30806 / 60421 loss=1.642, ppl=3.12, wps=16152.8, ups=4.47, wpb=3613.8, bsz=148.1, num_updates=30800, lr=0.000180187, gnorm=2.452, loss_scale=4, train_wall=22, gb_free=10.8, wall=9388
2021-05-05 19:13:57 | INFO | train_inner | epoch 001:  30906 / 60421 loss=1.619, ppl=3.07, wps=16246, ups=4.4, wpb=3692.7, bsz=140.9, num_updates=30900, lr=0.000179896, gnorm=1.972, loss_scale=4, train_wall=23, gb_free=10.8, wall=9411
2021-05-05 19:14:20 | INFO | train_inner | epoch 001:  31006 / 60421 loss=1.593, ppl=3.02, wps=16302.8, ups=4.37, wpb=3734.3, bsz=139.9, num_updates=31000, lr=0.000179605, gnorm=1.656, loss_scale=4, train_wall=23, gb_free=10.9, wall=9434
2021-05-05 19:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:14:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:14:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:14:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:14:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:14:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:14:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:14:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:14:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:14:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:14:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:14:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:14:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:15:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.842 | ppl 14.34 | bleu 25.87 | wps 2492.1 | wpb 2024.1 | bsz 97.5 | num_updates 31000 | best_bleu 25.87
2021-05-05 19:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 31000 updates
2021-05-05 19:15:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_31000.pt
2021-05-05 19:15:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_31000.pt
2021-05-05 19:15:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_31000.pt (epoch 1 @ 31000 updates, score 25.87) (writing took 14.421696577992407 seconds)
2021-05-05 19:16:02 | INFO | train_inner | epoch 001:  31106 / 60421 loss=1.548, ppl=2.92, wps=3687.3, ups=0.98, wpb=3754.6, bsz=133.5, num_updates=31100, lr=0.000179316, gnorm=1.66, loss_scale=4, train_wall=23, gb_free=10.7, wall=9536
2021-05-05 19:16:25 | INFO | train_inner | epoch 001:  31206 / 60421 loss=1.484, ppl=2.8, wps=16396.7, ups=4.35, wpb=3768.4, bsz=123.7, num_updates=31200, lr=0.000179029, gnorm=1.397, loss_scale=4, train_wall=23, gb_free=10.6, wall=9559
2021-05-05 19:16:47 | INFO | train_inner | epoch 001:  31306 / 60421 loss=1.543, ppl=2.91, wps=16537.8, ups=4.42, wpb=3742.8, bsz=112.6, num_updates=31300, lr=0.000178743, gnorm=1.557, loss_scale=4, train_wall=22, gb_free=10.8, wall=9581
2021-05-05 19:17:10 | INFO | train_inner | epoch 001:  31406 / 60421 loss=1.516, ppl=2.86, wps=16693.6, ups=4.39, wpb=3799.3, bsz=138.9, num_updates=31400, lr=0.000178458, gnorm=1.474, loss_scale=4, train_wall=23, gb_free=10.8, wall=9604
2021-05-05 19:17:33 | INFO | train_inner | epoch 001:  31506 / 60421 loss=1.545, ppl=2.92, wps=16572.5, ups=4.41, wpb=3757, bsz=134.4, num_updates=31500, lr=0.000178174, gnorm=1.576, loss_scale=4, train_wall=22, gb_free=10.8, wall=9627
2021-05-05 19:17:56 | INFO | train_inner | epoch 001:  31606 / 60421 loss=1.389, ppl=2.62, wps=16423.6, ups=4.41, wpb=3724.4, bsz=146.1, num_updates=31600, lr=0.000177892, gnorm=1.437, loss_scale=4, train_wall=22, gb_free=11.3, wall=9649
2021-05-05 19:18:19 | INFO | train_inner | epoch 001:  31706 / 60421 loss=1.587, ppl=3.01, wps=16604.4, ups=4.35, wpb=3813.1, bsz=150.9, num_updates=31700, lr=0.000177611, gnorm=1.558, loss_scale=4, train_wall=23, gb_free=10.8, wall=9672
2021-05-05 19:18:42 | INFO | train_inner | epoch 001:  31806 / 60421 loss=1.524, ppl=2.88, wps=16620.8, ups=4.32, wpb=3845, bsz=139.4, num_updates=31800, lr=0.000177332, gnorm=1.411, loss_scale=4, train_wall=23, gb_free=10.6, wall=9695
2021-05-05 19:19:05 | INFO | train_inner | epoch 001:  31906 / 60421 loss=1.512, ppl=2.85, wps=16276.1, ups=4.36, wpb=3730.6, bsz=131, num_updates=31900, lr=0.000177054, gnorm=1.545, loss_scale=4, train_wall=23, gb_free=10.9, wall=9718
2021-05-05 19:19:27 | INFO | train_inner | epoch 001:  32006 / 60421 loss=1.566, ppl=2.96, wps=16259.3, ups=4.41, wpb=3683.4, bsz=125.8, num_updates=32000, lr=0.000176777, gnorm=1.924, loss_scale=4, train_wall=22, gb_free=10.7, wall=9741
2021-05-05 19:19:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:19:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:19:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:19:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:19:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:19:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:19:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:19:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:19:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:19:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:19:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:19:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:19:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:20:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.857 | ppl 14.49 | bleu 25.46 | wps 2467.5 | wpb 2024.1 | bsz 97.5 | num_updates 32000 | best_bleu 25.87
2021-05-05 19:20:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 32000 updates
2021-05-05 19:20:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_32000.pt
2021-05-05 19:20:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_32000.pt
2021-05-05 19:20:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_32000.pt (epoch 1 @ 32000 updates, score 25.46) (writing took 7.883102146995952 seconds)
2021-05-05 19:21:03 | INFO | train_inner | epoch 001:  32106 / 60421 loss=1.641, ppl=3.12, wps=3820, ups=1.05, wpb=3643, bsz=146.1, num_updates=32100, lr=0.000176501, gnorm=2.105, loss_scale=4, train_wall=23, gb_free=11.2, wall=9836
2021-05-05 19:21:26 | INFO | train_inner | epoch 001:  32206 / 60421 loss=1.586, ppl=3, wps=16246.7, ups=4.36, wpb=3730.1, bsz=136.9, num_updates=32200, lr=0.000176227, gnorm=1.946, loss_scale=4, train_wall=23, gb_free=10.6, wall=9859
2021-05-05 19:21:48 | INFO | train_inner | epoch 001:  32306 / 60421 loss=1.573, ppl=2.98, wps=16521.1, ups=4.39, wpb=3763.1, bsz=134.9, num_updates=32300, lr=0.000175954, gnorm=1.731, loss_scale=4, train_wall=23, gb_free=10.8, wall=9882
2021-05-05 19:22:11 | INFO | train_inner | epoch 001:  32406 / 60421 loss=1.491, ppl=2.81, wps=16607.8, ups=4.42, wpb=3753.5, bsz=140.4, num_updates=32400, lr=0.000175682, gnorm=1.676, loss_scale=4, train_wall=22, gb_free=10.8, wall=9905
2021-05-05 19:22:34 | INFO | train_inner | epoch 001:  32506 / 60421 loss=1.531, ppl=2.89, wps=16452.8, ups=4.36, wpb=3772.5, bsz=147.1, num_updates=32500, lr=0.000175412, gnorm=1.66, loss_scale=4, train_wall=23, gb_free=10.8, wall=9928
2021-05-05 19:22:56 | INFO | train_inner | epoch 001:  32606 / 60421 loss=1.545, ppl=2.92, wps=16302.2, ups=4.43, wpb=3676.3, bsz=141.8, num_updates=32600, lr=0.000175142, gnorm=1.873, loss_scale=4, train_wall=22, gb_free=10.9, wall=9950
2021-05-05 19:23:19 | INFO | train_inner | epoch 001:  32706 / 60421 loss=1.572, ppl=2.97, wps=16499.8, ups=4.39, wpb=3758.4, bsz=119.5, num_updates=32700, lr=0.000174874, gnorm=1.728, loss_scale=4, train_wall=23, gb_free=10.8, wall=9973
2021-05-05 19:23:42 | INFO | train_inner | epoch 001:  32806 / 60421 loss=1.657, ppl=3.15, wps=16199.4, ups=4.41, wpb=3675.3, bsz=129, num_updates=32800, lr=0.000174608, gnorm=2.156, loss_scale=4, train_wall=22, gb_free=10.8, wall=9996
2021-05-05 19:24:05 | INFO | train_inner | epoch 001:  32906 / 60421 loss=1.545, ppl=2.92, wps=16264.6, ups=4.4, wpb=3693.1, bsz=126.3, num_updates=32900, lr=0.000174342, gnorm=1.783, loss_scale=4, train_wall=23, gb_free=10.8, wall=10018
2021-05-05 19:24:27 | INFO | train_inner | epoch 001:  33006 / 60421 loss=1.706, ppl=3.26, wps=16355.5, ups=4.41, wpb=3706.5, bsz=132.2, num_updates=33000, lr=0.000174078, gnorm=2.077, loss_scale=4, train_wall=22, gb_free=10.7, wall=10041
2021-05-05 19:24:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:24:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:24:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:24:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:24:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:24:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:24:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:24:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:24:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:24:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:24:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:24:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:24:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:24:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:24:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:24:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:24:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:24:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:24:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:24:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:24:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:24:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:25:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.889 | ppl 14.81 | bleu 25.84 | wps 2424.6 | wpb 2024.1 | bsz 97.5 | num_updates 33000 | best_bleu 25.87
2021-05-05 19:25:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 33000 updates
2021-05-05 19:25:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_33000.pt
2021-05-05 19:25:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_33000.pt
2021-05-05 19:25:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_33000.pt (epoch 1 @ 33000 updates, score 25.84) (writing took 7.917034486003104 seconds)
2021-05-05 19:26:04 | INFO | train_inner | epoch 001:  33106 / 60421 loss=1.54, ppl=2.91, wps=3820.5, ups=1.03, wpb=3697.3, bsz=136.6, num_updates=33100, lr=0.000173814, gnorm=1.681, loss_scale=4, train_wall=23, gb_free=10.6, wall=10138
2021-05-05 19:26:27 | INFO | train_inner | epoch 001:  33206 / 60421 loss=1.518, ppl=2.86, wps=15962.3, ups=4.4, wpb=3628.3, bsz=139.4, num_updates=33200, lr=0.000173553, gnorm=1.871, loss_scale=4, train_wall=23, gb_free=10.8, wall=10161
2021-05-05 19:26:50 | INFO | train_inner | epoch 001:  33306 / 60421 loss=1.576, ppl=2.98, wps=16437.2, ups=4.37, wpb=3765.6, bsz=138.2, num_updates=33300, lr=0.000173292, gnorm=1.868, loss_scale=4, train_wall=23, gb_free=10.8, wall=10184
2021-05-05 19:27:12 | INFO | train_inner | epoch 001:  33406 / 60421 loss=1.609, ppl=3.05, wps=16695.3, ups=4.42, wpb=3775.4, bsz=111.8, num_updates=33400, lr=0.000173032, gnorm=1.58, loss_scale=4, train_wall=22, gb_free=10.9, wall=10206
2021-05-05 19:27:35 | INFO | train_inner | epoch 001:  33506 / 60421 loss=1.504, ppl=2.84, wps=16460.4, ups=4.42, wpb=3724.9, bsz=112.7, num_updates=33500, lr=0.000172774, gnorm=1.683, loss_scale=4, train_wall=22, gb_free=10.7, wall=10229
2021-05-05 19:27:57 | INFO | train_inner | epoch 001:  33606 / 60421 loss=1.526, ppl=2.88, wps=16514.9, ups=4.44, wpb=3721.5, bsz=123.1, num_updates=33600, lr=0.000172516, gnorm=1.55, loss_scale=4, train_wall=22, gb_free=10.7, wall=10251
2021-05-05 19:28:20 | INFO | train_inner | epoch 001:  33706 / 60421 loss=1.566, ppl=2.96, wps=16447.8, ups=4.37, wpb=3762.8, bsz=127.4, num_updates=33700, lr=0.00017226, gnorm=1.832, loss_scale=4, train_wall=23, gb_free=10.6, wall=10274
2021-05-05 19:28:43 | INFO | train_inner | epoch 001:  33806 / 60421 loss=1.451, ppl=2.73, wps=16453.5, ups=4.36, wpb=3772.7, bsz=149, num_updates=33800, lr=0.000172005, gnorm=1.493, loss_scale=4, train_wall=23, gb_free=11, wall=10297
2021-05-05 19:29:06 | INFO | train_inner | epoch 001:  33906 / 60421 loss=1.488, ppl=2.8, wps=16292, ups=4.33, wpb=3759.7, bsz=122.2, num_updates=33900, lr=0.000171751, gnorm=1.608, loss_scale=4, train_wall=23, gb_free=11.7, wall=10320
2021-05-05 19:29:29 | INFO | train_inner | epoch 001:  34006 / 60421 loss=1.527, ppl=2.88, wps=16455.5, ups=4.32, wpb=3810.5, bsz=130.9, num_updates=34000, lr=0.000171499, gnorm=1.494, loss_scale=4, train_wall=23, gb_free=10.8, wall=10343
2021-05-05 19:29:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:29:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:29:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:29:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:29:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:29:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:29:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:29:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:29:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:29:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:29:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:29:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:29:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:30:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.876 | ppl 14.68 | bleu 26.6 | wps 2381.3 | wpb 2024.1 | bsz 97.5 | num_updates 34000 | best_bleu 26.6
2021-05-05 19:30:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 34000 updates
2021-05-05 19:30:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_34000.pt
2021-05-05 19:30:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_34000.pt
2021-05-05 19:30:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_34000.pt (epoch 1 @ 34000 updates, score 26.6) (writing took 14.618526784994174 seconds)
2021-05-05 19:31:14 | INFO | train_inner | epoch 001:  34106 / 60421 loss=1.488, ppl=2.81, wps=3557.2, ups=0.96, wpb=3709.9, bsz=128.2, num_updates=34100, lr=0.000171247, gnorm=1.584, loss_scale=4, train_wall=23, gb_free=10.8, wall=10448
2021-05-05 19:31:36 | INFO | train_inner | epoch 001:  34206 / 60421 loss=1.533, ppl=2.89, wps=16341.9, ups=4.41, wpb=3709.4, bsz=126.6, num_updates=34200, lr=0.000170996, gnorm=1.886, loss_scale=4, train_wall=23, gb_free=10.8, wall=10470
2021-05-05 19:31:59 | INFO | train_inner | epoch 001:  34306 / 60421 loss=1.663, ppl=3.17, wps=16431.1, ups=4.45, wpb=3696.3, bsz=126.8, num_updates=34300, lr=0.000170747, gnorm=2.14, loss_scale=4, train_wall=22, gb_free=11.7, wall=10493
2021-05-05 19:32:22 | INFO | train_inner | epoch 001:  34406 / 60421 loss=1.476, ppl=2.78, wps=16404.5, ups=4.38, wpb=3744, bsz=130.6, num_updates=34400, lr=0.000170499, gnorm=1.665, loss_scale=4, train_wall=23, gb_free=10.8, wall=10516
2021-05-05 19:32:45 | INFO | train_inner | epoch 001:  34506 / 60421 loss=1.542, ppl=2.91, wps=16492.1, ups=4.4, wpb=3746.1, bsz=138.9, num_updates=34500, lr=0.000170251, gnorm=1.774, loss_scale=4, train_wall=23, gb_free=10.7, wall=10538
2021-05-05 19:33:07 | INFO | train_inner | epoch 001:  34606 / 60421 loss=1.615, ppl=3.06, wps=16333.6, ups=4.38, wpb=3728.3, bsz=125, num_updates=34600, lr=0.000170005, gnorm=1.851, loss_scale=4, train_wall=23, gb_free=10.9, wall=10561
2021-05-05 19:33:30 | INFO | train_inner | epoch 001:  34706 / 60421 loss=1.512, ppl=2.85, wps=16475, ups=4.41, wpb=3735.8, bsz=127, num_updates=34700, lr=0.00016976, gnorm=1.654, loss_scale=4, train_wall=22, gb_free=11, wall=10584
2021-05-05 19:33:53 | INFO | train_inner | epoch 001:  34806 / 60421 loss=1.518, ppl=2.86, wps=16401.6, ups=4.38, wpb=3746.4, bsz=114.2, num_updates=34800, lr=0.000169516, gnorm=1.729, loss_scale=4, train_wall=23, gb_free=10.8, wall=10607
2021-05-05 19:34:16 | INFO | train_inner | epoch 001:  34906 / 60421 loss=1.566, ppl=2.96, wps=16164.9, ups=4.38, wpb=3693.9, bsz=146.6, num_updates=34900, lr=0.000169273, gnorm=2.048, loss_scale=4, train_wall=23, gb_free=10.7, wall=10630
2021-05-05 19:34:39 | INFO | train_inner | epoch 001:  35006 / 60421 loss=1.57, ppl=2.97, wps=16054.7, ups=4.34, wpb=3695.6, bsz=131, num_updates=35000, lr=0.000169031, gnorm=1.845, loss_scale=4, train_wall=23, gb_free=10.8, wall=10653
2021-05-05 19:34:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:34:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:34:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:34:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:34:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:34:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:34:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:34:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:34:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:34:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:35:44 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.83 | ppl 14.22 | bleu 26.29 | wps 2410.2 | wpb 2024.1 | bsz 97.5 | num_updates 35000 | best_bleu 26.6
2021-05-05 19:35:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 35000 updates
2021-05-05 19:35:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_35000.pt
2021-05-05 19:35:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_35000.pt
2021-05-05 19:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_35000.pt (epoch 1 @ 35000 updates, score 26.29) (writing took 7.861256667994894 seconds)
2021-05-05 19:36:16 | INFO | train_inner | epoch 001:  35106 / 60421 loss=1.573, ppl=2.98, wps=3906.8, ups=1.03, wpb=3787.4, bsz=133.1, num_updates=35100, lr=0.00016879, gnorm=1.781, loss_scale=4, train_wall=23, gb_free=10.7, wall=10750
2021-05-05 19:36:39 | INFO | train_inner | epoch 001:  35206 / 60421 loss=1.584, ppl=3, wps=16270, ups=4.38, wpb=3716.1, bsz=158.2, num_updates=35200, lr=0.00016855, gnorm=1.903, loss_scale=4, train_wall=23, gb_free=10.9, wall=10772
2021-05-05 19:37:01 | INFO | train_inner | epoch 001:  35306 / 60421 loss=1.612, ppl=3.06, wps=16466.5, ups=4.4, wpb=3744.1, bsz=125.6, num_updates=35300, lr=0.000168311, gnorm=1.782, loss_scale=4, train_wall=23, gb_free=11.3, wall=10795
2021-05-05 19:37:24 | INFO | train_inner | epoch 001:  35406 / 60421 loss=1.581, ppl=2.99, wps=16393.6, ups=4.45, wpb=3684.6, bsz=129.3, num_updates=35400, lr=0.000168073, gnorm=1.922, loss_scale=4, train_wall=22, gb_free=10.7, wall=10818
2021-05-05 19:37:46 | INFO | train_inner | epoch 001:  35506 / 60421 loss=1.451, ppl=2.73, wps=16541.8, ups=4.41, wpb=3751.7, bsz=132, num_updates=35500, lr=0.000167836, gnorm=1.783, loss_scale=4, train_wall=22, gb_free=10.7, wall=10840
2021-05-05 19:38:09 | INFO | train_inner | epoch 001:  35606 / 60421 loss=1.583, ppl=3, wps=16183.7, ups=4.47, wpb=3616.8, bsz=133.4, num_updates=35600, lr=0.0001676, gnorm=2.397, loss_scale=4, train_wall=22, gb_free=11, wall=10863
2021-05-05 19:38:31 | INFO | train_inner | epoch 001:  35706 / 60421 loss=1.571, ppl=2.97, wps=16275.4, ups=4.41, wpb=3687.3, bsz=124, num_updates=35700, lr=0.000167365, gnorm=1.743, loss_scale=4, train_wall=22, gb_free=10.8, wall=10885
2021-05-05 19:38:54 | INFO | train_inner | epoch 001:  35806 / 60421 loss=1.505, ppl=2.84, wps=16399.9, ups=4.35, wpb=3767.2, bsz=131.9, num_updates=35800, lr=0.000167132, gnorm=1.575, loss_scale=4, train_wall=23, gb_free=10.6, wall=10908
2021-05-05 19:39:17 | INFO | train_inner | epoch 001:  35906 / 60421 loss=1.478, ppl=2.79, wps=16223, ups=4.35, wpb=3733.4, bsz=133.8, num_updates=35900, lr=0.000166899, gnorm=1.461, loss_scale=4, train_wall=23, gb_free=10.8, wall=10931
2021-05-05 19:39:40 | INFO | train_inner | epoch 001:  36006 / 60421 loss=1.509, ppl=2.85, wps=16116.6, ups=4.37, wpb=3685, bsz=135.8, num_updates=36000, lr=0.000166667, gnorm=2.042, loss_scale=4, train_wall=23, gb_free=10.7, wall=10954
2021-05-05 19:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:39:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:39:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:39:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:40:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.963 | ppl 15.59 | bleu 23.03 | wps 2612.3 | wpb 2024.1 | bsz 97.5 | num_updates 36000 | best_bleu 26.6
2021-05-05 19:40:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 36000 updates
2021-05-05 19:40:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_36000.pt
2021-05-05 19:40:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_36000.pt
2021-05-05 19:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_36000.pt (epoch 1 @ 36000 updates, score 23.03) (writing took 7.88144480799383 seconds)
2021-05-05 19:41:12 | INFO | train_inner | epoch 001:  36106 / 60421 loss=1.5, ppl=2.83, wps=4087.6, ups=1.09, wpb=3758.8, bsz=132.4, num_updates=36100, lr=0.000166436, gnorm=1.59, loss_scale=4, train_wall=23, gb_free=10.6, wall=11046
2021-05-05 19:41:35 | INFO | train_inner | epoch 001:  36206 / 60421 loss=1.499, ppl=2.83, wps=16450.4, ups=4.38, wpb=3752.4, bsz=118.4, num_updates=36200, lr=0.000166206, gnorm=1.685, loss_scale=4, train_wall=23, gb_free=10.8, wall=11069
2021-05-05 19:41:58 | INFO | train_inner | epoch 001:  36306 / 60421 loss=1.483, ppl=2.8, wps=16436.2, ups=4.42, wpb=3720.5, bsz=131.2, num_updates=36300, lr=0.000165977, gnorm=1.648, loss_scale=4, train_wall=22, gb_free=10.9, wall=11092
2021-05-05 19:42:20 | INFO | train_inner | epoch 001:  36406 / 60421 loss=1.472, ppl=2.77, wps=16306.4, ups=4.45, wpb=3661.9, bsz=124.9, num_updates=36400, lr=0.000165748, gnorm=1.752, loss_scale=4, train_wall=22, gb_free=11.2, wall=11114
2021-05-05 19:42:43 | INFO | train_inner | epoch 001:  36506 / 60421 loss=1.489, ppl=2.81, wps=16525.6, ups=4.4, wpb=3754.4, bsz=133.2, num_updates=36500, lr=0.000165521, gnorm=1.675, loss_scale=4, train_wall=23, gb_free=10.7, wall=11137
2021-05-05 19:43:06 | INFO | train_inner | epoch 001:  36606 / 60421 loss=1.586, ppl=3, wps=16494.6, ups=4.38, wpb=3764.8, bsz=138.2, num_updates=36600, lr=0.000165295, gnorm=1.765, loss_scale=4, train_wall=23, gb_free=11, wall=11160
2021-05-05 19:43:28 | INFO | train_inner | epoch 001:  36706 / 60421 loss=1.581, ppl=2.99, wps=16299.3, ups=4.39, wpb=3716.2, bsz=118.2, num_updates=36700, lr=0.00016507, gnorm=1.861, loss_scale=4, train_wall=23, gb_free=10.8, wall=11182
2021-05-05 19:43:52 | INFO | train_inner | epoch 001:  36806 / 60421 loss=1.531, ppl=2.89, wps=16149.3, ups=4.34, wpb=3718.4, bsz=131.4, num_updates=36800, lr=0.000164845, gnorm=1.892, loss_scale=4, train_wall=23, gb_free=10.8, wall=11205
2021-05-05 19:44:15 | INFO | train_inner | epoch 001:  36906 / 60421 loss=1.542, ppl=2.91, wps=16299.6, ups=4.35, wpb=3746.2, bsz=136.9, num_updates=36900, lr=0.000164622, gnorm=1.746, loss_scale=4, train_wall=23, gb_free=10.9, wall=11228
2021-05-05 19:44:38 | INFO | train_inner | epoch 001:  37006 / 60421 loss=1.435, ppl=2.7, wps=16342.1, ups=4.32, wpb=3784.4, bsz=128.4, num_updates=37000, lr=0.000164399, gnorm=1.347, loss_scale=4, train_wall=23, gb_free=10.8, wall=11252
2021-05-05 19:44:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:44:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:44:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:44:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:44:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:44:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:44:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:44:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:44:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:44:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:44:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:44:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:44:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:44:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:44:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:44:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:45:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.901 | ppl 14.94 | bleu 26.51 | wps 2475.9 | wpb 2024.1 | bsz 97.5 | num_updates 37000 | best_bleu 26.6
2021-05-05 19:45:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 37000 updates
2021-05-05 19:45:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_37000.pt
2021-05-05 19:45:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_37000.pt
2021-05-05 19:45:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_37000.pt (epoch 1 @ 37000 updates, score 26.51) (writing took 7.88354330099537 seconds)
2021-05-05 19:46:13 | INFO | train_inner | epoch 001:  37106 / 60421 loss=1.647, ppl=3.13, wps=3915.6, ups=1.05, wpb=3722.9, bsz=113.3, num_updates=37100, lr=0.000164177, gnorm=2.072, loss_scale=4, train_wall=23, gb_free=11, wall=11347
2021-05-05 19:46:36 | INFO | train_inner | epoch 001:  37206 / 60421 loss=1.52, ppl=2.87, wps=16383.3, ups=4.34, wpb=3770.7, bsz=144.4, num_updates=37200, lr=0.000163956, gnorm=1.769, loss_scale=4, train_wall=23, gb_free=10.8, wall=11370
2021-05-05 19:46:58 | INFO | train_inner | epoch 001:  37306 / 60421 loss=1.528, ppl=2.88, wps=16477, ups=4.41, wpb=3734.3, bsz=118.1, num_updates=37300, lr=0.000163737, gnorm=1.671, loss_scale=4, train_wall=22, gb_free=10.8, wall=11392
2021-05-05 19:47:21 | INFO | train_inner | epoch 001:  37406 / 60421 loss=1.444, ppl=2.72, wps=16487.1, ups=4.42, wpb=3734, bsz=134.1, num_updates=37400, lr=0.000163517, gnorm=1.648, loss_scale=4, train_wall=22, gb_free=10.7, wall=11415
2021-05-05 19:47:44 | INFO | train_inner | epoch 001:  37506 / 60421 loss=1.395, ppl=2.63, wps=16505.7, ups=4.37, wpb=3779.3, bsz=149.1, num_updates=37500, lr=0.000163299, gnorm=1.451, loss_scale=4, train_wall=23, gb_free=11, wall=11438
2021-05-05 19:48:06 | INFO | train_inner | epoch 001:  37606 / 60421 loss=1.526, ppl=2.88, wps=16394.1, ups=4.44, wpb=3689.1, bsz=129, num_updates=37600, lr=0.000163082, gnorm=1.904, loss_scale=4, train_wall=22, gb_free=10.9, wall=11460
2021-05-05 19:48:29 | INFO | train_inner | epoch 001:  37706 / 60421 loss=1.502, ppl=2.83, wps=16401.2, ups=4.39, wpb=3735.1, bsz=121.1, num_updates=37700, lr=0.000162866, gnorm=1.575, loss_scale=4, train_wall=23, gb_free=10.9, wall=11483
2021-05-05 19:48:52 | INFO | train_inner | epoch 001:  37806 / 60421 loss=1.466, ppl=2.76, wps=16203.4, ups=4.36, wpb=3716.2, bsz=144, num_updates=37800, lr=0.00016265, gnorm=1.706, loss_scale=4, train_wall=23, gb_free=11, wall=11506
2021-05-05 19:49:15 | INFO | train_inner | epoch 001:  37906 / 60421 loss=1.534, ppl=2.9, wps=16176.3, ups=4.32, wpb=3748.4, bsz=118.6, num_updates=37900, lr=0.000162435, gnorm=1.814, loss_scale=4, train_wall=23, gb_free=10.6, wall=11529
2021-05-05 19:49:39 | INFO | train_inner | epoch 001:  38006 / 60421 loss=1.508, ppl=2.85, wps=16222.6, ups=4.31, wpb=3763.1, bsz=131.6, num_updates=38000, lr=0.000162221, gnorm=1.647, loss_scale=8, train_wall=23, gb_free=10.7, wall=11552
2021-05-05 19:49:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:49:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:49:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:49:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:49:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:49:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:49:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:49:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:49:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:49:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:49:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:49:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:49:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:49:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:49:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:49:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:50:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.827 | ppl 14.19 | bleu 26.84 | wps 2299.6 | wpb 2024.1 | bsz 97.5 | num_updates 38000 | best_bleu 26.84
2021-05-05 19:50:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 38000 updates
2021-05-05 19:50:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_38000.pt
2021-05-05 19:50:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_38000.pt
2021-05-05 19:51:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_38000.pt (epoch 1 @ 38000 updates, score 26.84) (writing took 14.624882256001001 seconds)
2021-05-05 19:51:25 | INFO | train_inner | epoch 001:  38106 / 60421 loss=1.49, ppl=2.81, wps=3490.6, ups=0.94, wpb=3717.2, bsz=134.7, num_updates=38100, lr=0.000162008, gnorm=1.663, loss_scale=8, train_wall=23, gb_free=10.6, wall=11659
2021-05-05 19:51:48 | INFO | train_inner | epoch 001:  38206 / 60421 loss=1.49, ppl=2.81, wps=16470.2, ups=4.35, wpb=3785.3, bsz=104.2, num_updates=38200, lr=0.000161796, gnorm=1.4, loss_scale=8, train_wall=23, gb_free=10.6, wall=11682
2021-05-05 19:52:11 | INFO | train_inner | epoch 001:  38306 / 60421 loss=1.668, ppl=3.18, wps=16420.2, ups=4.4, wpb=3731.2, bsz=118.2, num_updates=38300, lr=0.000161585, gnorm=1.908, loss_scale=8, train_wall=23, gb_free=10.9, wall=11705
2021-05-05 19:52:33 | INFO | train_inner | epoch 001:  38406 / 60421 loss=1.507, ppl=2.84, wps=16272, ups=4.44, wpb=3664.4, bsz=114.6, num_updates=38400, lr=0.000161374, gnorm=1.733, loss_scale=8, train_wall=22, gb_free=11, wall=11727
2021-05-05 19:52:56 | INFO | train_inner | epoch 001:  38506 / 60421 loss=1.495, ppl=2.82, wps=16556.1, ups=4.36, wpb=3797.2, bsz=150.5, num_updates=38500, lr=0.000161165, gnorm=1.61, loss_scale=8, train_wall=23, gb_free=10.8, wall=11750
2021-05-05 19:53:19 | INFO | train_inner | epoch 001:  38606 / 60421 loss=1.448, ppl=2.73, wps=16177.2, ups=4.38, wpb=3696.4, bsz=159.1, num_updates=38600, lr=0.000160956, gnorm=1.906, loss_scale=8, train_wall=23, gb_free=10.9, wall=11773
2021-05-05 19:53:42 | INFO | train_inner | epoch 001:  38706 / 60421 loss=1.425, ppl=2.68, wps=16414.6, ups=4.35, wpb=3775.2, bsz=127.6, num_updates=38700, lr=0.000160748, gnorm=1.687, loss_scale=8, train_wall=23, gb_free=10.7, wall=11796
2021-05-05 19:54:05 | INFO | train_inner | epoch 001:  38806 / 60421 loss=1.511, ppl=2.85, wps=16202.6, ups=4.33, wpb=3739.8, bsz=142, num_updates=38800, lr=0.00016054, gnorm=1.793, loss_scale=8, train_wall=23, gb_free=11.1, wall=11819
2021-05-05 19:54:28 | INFO | train_inner | epoch 001:  38906 / 60421 loss=1.447, ppl=2.73, wps=15934.7, ups=4.35, wpb=3662.8, bsz=137.5, num_updates=38900, lr=0.000160334, gnorm=1.785, loss_scale=8, train_wall=23, gb_free=10.8, wall=11842
2021-05-05 19:54:51 | INFO | train_inner | epoch 001:  39006 / 60421 loss=1.497, ppl=2.82, wps=15952.2, ups=4.29, wpb=3720.4, bsz=131.8, num_updates=39000, lr=0.000160128, gnorm=1.937, loss_scale=8, train_wall=23, gb_free=10.8, wall=11865
2021-05-05 19:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 19:55:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:55:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:55:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:55:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:55:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:55:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:55:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 19:55:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 19:55:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 19:55:57 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.951 | ppl 15.47 | bleu 26.62 | wps 2428.4 | wpb 2024.1 | bsz 97.5 | num_updates 39000 | best_bleu 26.84
2021-05-05 19:55:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 39000 updates
2021-05-05 19:55:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_39000.pt
2021-05-05 19:55:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_39000.pt
2021-05-05 19:56:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_39000.pt (epoch 1 @ 39000 updates, score 26.62) (writing took 7.956547405992751 seconds)
2021-05-05 19:56:28 | INFO | train_inner | epoch 001:  39106 / 60421 loss=1.428, ppl=2.69, wps=3892.3, ups=1.04, wpb=3758.2, bsz=147, num_updates=39100, lr=0.000159923, gnorm=1.637, loss_scale=8, train_wall=23, gb_free=10.7, wall=11962
2021-05-05 19:56:51 | INFO | train_inner | epoch 001:  39206 / 60421 loss=1.421, ppl=2.68, wps=16405.8, ups=4.4, wpb=3729.4, bsz=125.3, num_updates=39200, lr=0.000159719, gnorm=1.695, loss_scale=8, train_wall=23, gb_free=10.8, wall=11985
2021-05-05 19:57:13 | INFO | train_inner | epoch 001:  39306 / 60421 loss=1.457, ppl=2.75, wps=16477.4, ups=4.41, wpb=3739.3, bsz=126.6, num_updates=39300, lr=0.000159516, gnorm=1.663, loss_scale=8, train_wall=23, gb_free=11, wall=12007
2021-05-05 19:57:36 | INFO | train_inner | epoch 001:  39406 / 60421 loss=1.427, ppl=2.69, wps=16593.1, ups=4.35, wpb=3818.8, bsz=138.3, num_updates=39400, lr=0.000159313, gnorm=1.477, loss_scale=8, train_wall=23, gb_free=10.8, wall=12030
2021-05-05 19:57:59 | INFO | train_inner | epoch 001:  39506 / 60421 loss=1.557, ppl=2.94, wps=16196.7, ups=4.46, wpb=3628.2, bsz=116.8, num_updates=39500, lr=0.000159111, gnorm=2.133, loss_scale=8, train_wall=22, gb_free=10.7, wall=12053
2021-05-05 19:58:22 | INFO | train_inner | epoch 001:  39606 / 60421 loss=1.536, ppl=2.9, wps=16165.9, ups=4.37, wpb=3695.2, bsz=129.7, num_updates=39600, lr=0.00015891, gnorm=1.843, loss_scale=8, train_wall=23, gb_free=11.1, wall=12076
2021-05-05 19:58:45 | INFO | train_inner | epoch 001:  39706 / 60421 loss=1.568, ppl=2.96, wps=16339.6, ups=4.38, wpb=3726.8, bsz=132.9, num_updates=39700, lr=0.00015871, gnorm=1.957, loss_scale=8, train_wall=23, gb_free=11.1, wall=12098
2021-05-05 19:59:08 | INFO | train_inner | epoch 001:  39806 / 60421 loss=1.432, ppl=2.7, wps=16470.9, ups=4.33, wpb=3804.1, bsz=121.8, num_updates=39800, lr=0.000158511, gnorm=1.375, loss_scale=8, train_wall=23, gb_free=10.8, wall=12121
2021-05-05 19:59:31 | INFO | train_inner | epoch 001:  39906 / 60421 loss=1.511, ppl=2.85, wps=16134.9, ups=4.31, wpb=3739.8, bsz=132.2, num_updates=39900, lr=0.000158312, gnorm=1.831, loss_scale=8, train_wall=23, gb_free=10.8, wall=12145
2021-05-05 19:59:54 | INFO | train_inner | epoch 001:  40006 / 60421 loss=1.467, ppl=2.77, wps=16143.9, ups=4.33, wpb=3728.4, bsz=128.1, num_updates=40000, lr=0.000158114, gnorm=1.8, loss_scale=8, train_wall=23, gb_free=10.8, wall=12168
2021-05-05 19:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:00:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:00:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:00:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:00:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:00:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:00:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:00:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:00:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:00:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:00:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.916 | ppl 15.09 | bleu 26.74 | wps 2425.6 | wpb 2024.1 | bsz 97.5 | num_updates 40000 | best_bleu 26.84
2021-05-05 20:00:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 40000 updates
2021-05-05 20:00:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_40000.pt
2021-05-05 20:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_40000.pt
2021-05-05 20:01:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_40000.pt (epoch 1 @ 40000 updates, score 26.74) (writing took 7.891637504013488 seconds)
2021-05-05 20:01:30 | INFO | train_inner | epoch 001:  40106 / 60421 loss=1.547, ppl=2.92, wps=3795.6, ups=1.04, wpb=3642.6, bsz=114.2, num_updates=40100, lr=0.000157917, gnorm=2.336, loss_scale=8, train_wall=22, gb_free=10.5, wall=12264
2021-05-05 20:01:53 | INFO | train_inner | epoch 001:  40206 / 60421 loss=1.467, ppl=2.77, wps=16510.3, ups=4.4, wpb=3748.7, bsz=129.6, num_updates=40200, lr=0.00015772, gnorm=1.577, loss_scale=8, train_wall=23, gb_free=10.7, wall=12286
2021-05-05 20:02:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-05 20:02:15 | INFO | train_inner | epoch 001:  40307 / 60421 loss=1.389, ppl=2.62, wps=16181.5, ups=4.41, wpb=3666.9, bsz=129.4, num_updates=40300, lr=0.000157524, gnorm=1.64, loss_scale=4, train_wall=22, gb_free=10.7, wall=12309
2021-05-05 20:02:38 | INFO | train_inner | epoch 001:  40407 / 60421 loss=1.414, ppl=2.66, wps=16509.5, ups=4.37, wpb=3774.2, bsz=124.2, num_updates=40400, lr=0.000157329, gnorm=1.488, loss_scale=4, train_wall=23, gb_free=11, wall=12332
2021-05-05 20:03:01 | INFO | train_inner | epoch 001:  40507 / 60421 loss=1.495, ppl=2.82, wps=16495.7, ups=4.38, wpb=3768.8, bsz=146.7, num_updates=40500, lr=0.000157135, gnorm=1.818, loss_scale=4, train_wall=23, gb_free=10.7, wall=12355
2021-05-05 20:03:24 | INFO | train_inner | epoch 001:  40607 / 60421 loss=1.407, ppl=2.65, wps=16227.6, ups=4.38, wpb=3701.7, bsz=144.2, num_updates=40600, lr=0.000156941, gnorm=1.976, loss_scale=4, train_wall=23, gb_free=10.8, wall=12378
2021-05-05 20:03:47 | INFO | train_inner | epoch 001:  40707 / 60421 loss=1.485, ppl=2.8, wps=16290.3, ups=4.39, wpb=3708.7, bsz=139.6, num_updates=40700, lr=0.000156748, gnorm=1.87, loss_scale=4, train_wall=23, gb_free=10.8, wall=12400
2021-05-05 20:04:10 | INFO | train_inner | epoch 001:  40807 / 60421 loss=1.524, ppl=2.88, wps=16361, ups=4.35, wpb=3763.1, bsz=128.8, num_updates=40800, lr=0.000156556, gnorm=2.022, loss_scale=4, train_wall=23, gb_free=10.8, wall=12423
2021-05-05 20:04:33 | INFO | train_inner | epoch 001:  40907 / 60421 loss=1.384, ppl=2.61, wps=15990, ups=4.32, wpb=3702.1, bsz=138.2, num_updates=40900, lr=0.000156365, gnorm=1.482, loss_scale=4, train_wall=23, gb_free=10.8, wall=12447
2021-05-05 20:04:56 | INFO | train_inner | epoch 001:  41007 / 60421 loss=1.472, ppl=2.77, wps=16021.2, ups=4.31, wpb=3718.1, bsz=116.4, num_updates=41000, lr=0.000156174, gnorm=1.798, loss_scale=4, train_wall=23, gb_free=10.9, wall=12470
2021-05-05 20:04:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:05:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:05:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:05:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:05:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:05:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:05:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:05:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:05:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:05:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:05:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:05:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:05:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:06:02 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.962 | ppl 15.58 | bleu 27.23 | wps 2383.1 | wpb 2024.1 | bsz 97.5 | num_updates 41000 | best_bleu 27.23
2021-05-05 20:06:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 41000 updates
2021-05-05 20:06:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_41000.pt
2021-05-05 20:06:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_41000.pt
2021-05-05 20:06:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_41000.pt (epoch 1 @ 41000 updates, score 27.23) (writing took 14.649640268995427 seconds)
2021-05-05 20:06:40 | INFO | train_inner | epoch 001:  41107 / 60421 loss=1.447, ppl=2.73, wps=3566.7, ups=0.96, wpb=3709.3, bsz=137.9, num_updates=41100, lr=0.000155984, gnorm=1.957, loss_scale=4, train_wall=23, gb_free=10.9, wall=12574
2021-05-05 20:07:02 | INFO | train_inner | epoch 001:  41207 / 60421 loss=1.339, ppl=2.53, wps=16233, ups=4.43, wpb=3666.4, bsz=137.4, num_updates=41200, lr=0.000155794, gnorm=1.503, loss_scale=4, train_wall=22, gb_free=10.9, wall=12596
2021-05-05 20:07:25 | INFO | train_inner | epoch 001:  41307 / 60421 loss=1.578, ppl=2.98, wps=16503.2, ups=4.37, wpb=3775, bsz=130.2, num_updates=41300, lr=0.000155606, gnorm=1.873, loss_scale=4, train_wall=23, gb_free=11.1, wall=12619
2021-05-05 20:07:48 | INFO | train_inner | epoch 001:  41407 / 60421 loss=1.373, ppl=2.59, wps=16377.1, ups=4.39, wpb=3726.9, bsz=152.1, num_updates=41400, lr=0.000155417, gnorm=1.498, loss_scale=4, train_wall=23, gb_free=10.8, wall=12642
2021-05-05 20:08:11 | INFO | train_inner | epoch 001:  41507 / 60421 loss=1.443, ppl=2.72, wps=16479.7, ups=4.37, wpb=3770.5, bsz=123, num_updates=41500, lr=0.00015523, gnorm=1.906, loss_scale=4, train_wall=23, gb_free=10.8, wall=12665
2021-05-05 20:08:34 | INFO | train_inner | epoch 001:  41607 / 60421 loss=1.452, ppl=2.74, wps=16357.7, ups=4.35, wpb=3760.1, bsz=139.3, num_updates=41600, lr=0.000155043, gnorm=1.658, loss_scale=4, train_wall=23, gb_free=10.9, wall=12688
2021-05-05 20:08:57 | INFO | train_inner | epoch 001:  41707 / 60421 loss=1.337, ppl=2.53, wps=16243.3, ups=4.35, wpb=3736.6, bsz=139.6, num_updates=41700, lr=0.000154857, gnorm=1.367, loss_scale=4, train_wall=23, gb_free=10.8, wall=12711
2021-05-05 20:09:20 | INFO | train_inner | epoch 001:  41807 / 60421 loss=1.447, ppl=2.73, wps=16314.9, ups=4.31, wpb=3783.2, bsz=145.3, num_updates=41800, lr=0.000154672, gnorm=1.582, loss_scale=4, train_wall=23, gb_free=10.9, wall=12734
2021-05-05 20:09:43 | INFO | train_inner | epoch 001:  41907 / 60421 loss=1.36, ppl=2.57, wps=15997.1, ups=4.3, wpb=3723.1, bsz=144.8, num_updates=41900, lr=0.000154487, gnorm=1.624, loss_scale=4, train_wall=23, gb_free=10.8, wall=12757
2021-05-05 20:10:07 | INFO | train_inner | epoch 001:  42007 / 60421 loss=1.395, ppl=2.63, wps=16103.9, ups=4.23, wpb=3803.8, bsz=137.4, num_updates=42000, lr=0.000154303, gnorm=1.454, loss_scale=4, train_wall=23, gb_free=10.9, wall=12781
2021-05-05 20:10:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:10:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:10:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:10:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:10:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:10:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:10:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:10:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:10:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:10:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:10:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:10:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:10:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:11:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.904 | ppl 14.97 | bleu 26.92 | wps 2409.9 | wpb 2024.1 | bsz 97.5 | num_updates 42000 | best_bleu 27.23
2021-05-05 20:11:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 42000 updates
2021-05-05 20:11:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_42000.pt
2021-05-05 20:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_42000.pt
2021-05-05 20:11:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_42000.pt (epoch 1 @ 42000 updates, score 26.92) (writing took 7.866559218004113 seconds)
2021-05-05 20:11:44 | INFO | train_inner | epoch 001:  42107 / 60421 loss=1.443, ppl=2.72, wps=3860, ups=1.04, wpb=3725.1, bsz=130.6, num_updates=42100, lr=0.00015412, gnorm=1.694, loss_scale=4, train_wall=23, gb_free=11, wall=12877
2021-05-05 20:12:07 | INFO | train_inner | epoch 001:  42207 / 60421 loss=1.368, ppl=2.58, wps=16508.2, ups=4.36, wpb=3790.5, bsz=165.4, num_updates=42200, lr=0.000153937, gnorm=1.596, loss_scale=4, train_wall=23, gb_free=10.8, wall=12900
2021-05-05 20:12:29 | INFO | train_inner | epoch 001:  42307 / 60421 loss=1.408, ppl=2.65, wps=16411, ups=4.38, wpb=3749.6, bsz=122.1, num_updates=42300, lr=0.000153755, gnorm=1.56, loss_scale=4, train_wall=23, gb_free=10.7, wall=12923
2021-05-05 20:12:52 | INFO | train_inner | epoch 001:  42407 / 60421 loss=1.382, ppl=2.61, wps=16419.7, ups=4.39, wpb=3739.7, bsz=139.3, num_updates=42400, lr=0.000153574, gnorm=1.704, loss_scale=4, train_wall=23, gb_free=10.7, wall=12946
2021-05-05 20:13:15 | INFO | train_inner | epoch 001:  42507 / 60421 loss=1.39, ppl=2.62, wps=16463.8, ups=4.38, wpb=3758.4, bsz=122.2, num_updates=42500, lr=0.000153393, gnorm=1.475, loss_scale=4, train_wall=23, gb_free=11, wall=12969
2021-05-05 20:13:38 | INFO | train_inner | epoch 001:  42607 / 60421 loss=1.383, ppl=2.61, wps=16410.2, ups=4.38, wpb=3746.2, bsz=108.5, num_updates=42600, lr=0.000153213, gnorm=1.382, loss_scale=4, train_wall=23, gb_free=10.7, wall=12992
2021-05-05 20:14:01 | INFO | train_inner | epoch 001:  42707 / 60421 loss=1.32, ppl=2.5, wps=16196, ups=4.38, wpb=3699.8, bsz=135.8, num_updates=42700, lr=0.000153033, gnorm=1.595, loss_scale=4, train_wall=23, gb_free=10.7, wall=13015
2021-05-05 20:14:24 | INFO | train_inner | epoch 001:  42807 / 60421 loss=1.36, ppl=2.57, wps=16144.7, ups=4.38, wpb=3684.8, bsz=129.9, num_updates=42800, lr=0.000152854, gnorm=1.665, loss_scale=4, train_wall=23, gb_free=10.7, wall=13037
2021-05-05 20:14:47 | INFO | train_inner | epoch 001:  42907 / 60421 loss=1.417, ppl=2.67, wps=16076.5, ups=4.3, wpb=3739.7, bsz=122.3, num_updates=42900, lr=0.000152676, gnorm=1.571, loss_scale=4, train_wall=23, gb_free=10.7, wall=13061
2021-05-05 20:15:10 | INFO | train_inner | epoch 001:  43007 / 60421 loss=1.374, ppl=2.59, wps=16007, ups=4.24, wpb=3772.1, bsz=135.6, num_updates=43000, lr=0.000152499, gnorm=1.509, loss_scale=4, train_wall=23, gb_free=11.3, wall=13084
2021-05-05 20:15:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:15:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:15:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:15:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:15:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:15:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:15:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:15:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:15:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:15:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:15:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:15:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:15:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:16:19 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.996 | ppl 15.96 | bleu 27.18 | wps 2322.8 | wpb 2024.1 | bsz 97.5 | num_updates 43000 | best_bleu 27.23
2021-05-05 20:16:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 43000 updates
2021-05-05 20:16:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_43000.pt
2021-05-05 20:16:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_43000.pt
2021-05-05 20:16:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_43000.pt (epoch 1 @ 43000 updates, score 27.18) (writing took 7.843898852006532 seconds)
2021-05-05 20:16:49 | INFO | train_inner | epoch 001:  43107 / 60421 loss=1.398, ppl=2.64, wps=3715.2, ups=1.01, wpb=3668.9, bsz=126.9, num_updates=43100, lr=0.000152322, gnorm=1.678, loss_scale=4, train_wall=22, gb_free=11.2, wall=13183
2021-05-05 20:17:12 | INFO | train_inner | epoch 001:  43207 / 60421 loss=1.437, ppl=2.71, wps=16358.3, ups=4.42, wpb=3704.3, bsz=129.1, num_updates=43200, lr=0.000152145, gnorm=1.764, loss_scale=4, train_wall=22, gb_free=11.6, wall=13206
2021-05-05 20:17:34 | INFO | train_inner | epoch 001:  43307 / 60421 loss=1.449, ppl=2.73, wps=16606.4, ups=4.41, wpb=3767.7, bsz=138.8, num_updates=43300, lr=0.000151969, gnorm=1.597, loss_scale=4, train_wall=23, gb_free=10.8, wall=13228
2021-05-05 20:17:57 | INFO | train_inner | epoch 001:  43407 / 60421 loss=1.388, ppl=2.62, wps=16306.5, ups=4.43, wpb=3680.9, bsz=141.1, num_updates=43400, lr=0.000151794, gnorm=2.059, loss_scale=4, train_wall=22, gb_free=11.1, wall=13251
2021-05-05 20:18:20 | INFO | train_inner | epoch 001:  43507 / 60421 loss=1.34, ppl=2.53, wps=16537.8, ups=4.41, wpb=3750.5, bsz=125.3, num_updates=43500, lr=0.00015162, gnorm=1.45, loss_scale=4, train_wall=22, gb_free=10.7, wall=13274
2021-05-05 20:18:42 | INFO | train_inner | epoch 001:  43607 / 60421 loss=1.354, ppl=2.56, wps=16501.1, ups=4.41, wpb=3743.3, bsz=160.4, num_updates=43600, lr=0.000151446, gnorm=1.6, loss_scale=4, train_wall=22, gb_free=10.7, wall=13296
2021-05-05 20:19:05 | INFO | train_inner | epoch 001:  43707 / 60421 loss=1.366, ppl=2.58, wps=16555.1, ups=4.41, wpb=3750.2, bsz=153.6, num_updates=43700, lr=0.000151272, gnorm=1.612, loss_scale=4, train_wall=22, gb_free=10.8, wall=13319
2021-05-05 20:19:28 | INFO | train_inner | epoch 001:  43807 / 60421 loss=1.387, ppl=2.62, wps=16524.6, ups=4.41, wpb=3748.7, bsz=129.6, num_updates=43800, lr=0.000151099, gnorm=1.598, loss_scale=4, train_wall=22, gb_free=10.9, wall=13342
2021-05-05 20:19:50 | INFO | train_inner | epoch 001:  43907 / 60421 loss=1.454, ppl=2.74, wps=16583.1, ups=4.47, wpb=3711.4, bsz=118.4, num_updates=43900, lr=0.000150927, gnorm=1.653, loss_scale=4, train_wall=22, gb_free=10.7, wall=13364
2021-05-05 20:20:13 | INFO | train_inner | epoch 001:  44007 / 60421 loss=1.377, ppl=2.6, wps=16518.1, ups=4.38, wpb=3771.8, bsz=129.9, num_updates=44000, lr=0.000150756, gnorm=1.506, loss_scale=4, train_wall=23, gb_free=10.7, wall=13387
2021-05-05 20:20:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:20:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:20:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:20:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:20:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:20:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:20:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:20:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:20:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:20:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:20:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:20:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:20:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:21:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.982 | ppl 15.8 | bleu 27.15 | wps 2464.2 | wpb 2024.1 | bsz 97.5 | num_updates 44000 | best_bleu 27.23
2021-05-05 20:21:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 44000 updates
2021-05-05 20:21:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_44000.pt
2021-05-05 20:21:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_44000.pt
2021-05-05 20:21:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_44000.pt (epoch 1 @ 44000 updates, score 27.15) (writing took 10.30335410599946 seconds)
2021-05-05 20:21:50 | INFO | train_inner | epoch 001:  44107 / 60421 loss=1.301, ppl=2.46, wps=3812.6, ups=1.03, wpb=3716, bsz=156.3, num_updates=44100, lr=0.000150585, gnorm=1.515, loss_scale=4, train_wall=23, gb_free=11, wall=13484
2021-05-05 20:22:13 | INFO | train_inner | epoch 001:  44207 / 60421 loss=1.352, ppl=2.55, wps=16511.5, ups=4.42, wpb=3737.3, bsz=131.9, num_updates=44200, lr=0.000150414, gnorm=1.379, loss_scale=4, train_wall=22, gb_free=11.1, wall=13507
2021-05-05 20:22:36 | INFO | train_inner | epoch 001:  44307 / 60421 loss=1.367, ppl=2.58, wps=16549.7, ups=4.41, wpb=3752.4, bsz=125.4, num_updates=44300, lr=0.000150244, gnorm=1.546, loss_scale=4, train_wall=22, gb_free=10.7, wall=13530
2021-05-05 20:22:58 | INFO | train_inner | epoch 001:  44407 / 60421 loss=1.351, ppl=2.55, wps=16461.7, ups=4.43, wpb=3712.7, bsz=125.8, num_updates=44400, lr=0.000150075, gnorm=1.567, loss_scale=4, train_wall=22, gb_free=11.1, wall=13552
2021-05-05 20:23:21 | INFO | train_inner | epoch 001:  44507 / 60421 loss=1.534, ppl=2.9, wps=16629.2, ups=4.42, wpb=3760.2, bsz=125.5, num_updates=44500, lr=0.000149906, gnorm=1.85, loss_scale=4, train_wall=22, gb_free=11.6, wall=13575
2021-05-05 20:23:44 | INFO | train_inner | epoch 001:  44607 / 60421 loss=1.449, ppl=2.73, wps=16450.8, ups=4.41, wpb=3734.2, bsz=128.5, num_updates=44600, lr=0.000149738, gnorm=1.79, loss_scale=4, train_wall=23, gb_free=10.8, wall=13597
2021-05-05 20:24:06 | INFO | train_inner | epoch 001:  44707 / 60421 loss=1.425, ppl=2.69, wps=16424.8, ups=4.39, wpb=3737.8, bsz=123.5, num_updates=44700, lr=0.000149571, gnorm=1.656, loss_scale=4, train_wall=23, gb_free=10.8, wall=13620
2021-05-05 20:24:29 | INFO | train_inner | epoch 001:  44807 / 60421 loss=1.269, ppl=2.41, wps=16038.6, ups=4.32, wpb=3712.1, bsz=173.1, num_updates=44800, lr=0.000149404, gnorm=1.653, loss_scale=4, train_wall=23, gb_free=10.8, wall=13643
2021-05-05 20:24:53 | INFO | train_inner | epoch 001:  44907 / 60421 loss=1.427, ppl=2.69, wps=16430.9, ups=4.25, wpb=3866.9, bsz=128.6, num_updates=44900, lr=0.000149237, gnorm=1.462, loss_scale=4, train_wall=23, gb_free=10.9, wall=13667
2021-05-05 20:25:16 | INFO | train_inner | epoch 001:  45007 / 60421 loss=1.365, ppl=2.58, wps=16063.2, ups=4.29, wpb=3743.6, bsz=121.3, num_updates=45000, lr=0.000149071, gnorm=1.584, loss_scale=4, train_wall=23, gb_free=11, wall=13690
2021-05-05 20:25:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:25:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:25:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:25:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:26:22 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.014 | ppl 16.15 | bleu 27.05 | wps 2422.9 | wpb 2024.1 | bsz 97.5 | num_updates 45000 | best_bleu 27.23
2021-05-05 20:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 45000 updates
2021-05-05 20:26:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_45000.pt
2021-05-05 20:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_45000.pt
2021-05-05 20:26:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_45000.pt (epoch 1 @ 45000 updates, score 27.05) (writing took 8.43432482299977 seconds)
2021-05-05 20:26:53 | INFO | train_inner | epoch 001:  45107 / 60421 loss=1.384, ppl=2.61, wps=3895.1, ups=1.03, wpb=3774, bsz=141.2, num_updates=45100, lr=0.000148906, gnorm=1.648, loss_scale=4, train_wall=23, gb_free=10.7, wall=13787
2021-05-05 20:27:16 | INFO | train_inner | epoch 001:  45207 / 60421 loss=1.297, ppl=2.46, wps=16477.9, ups=4.37, wpb=3768.9, bsz=153.2, num_updates=45200, lr=0.000148741, gnorm=1.471, loss_scale=4, train_wall=23, gb_free=11.1, wall=13810
2021-05-05 20:27:39 | INFO | train_inner | epoch 001:  45307 / 60421 loss=1.397, ppl=2.63, wps=16351.2, ups=4.43, wpb=3690.5, bsz=114.2, num_updates=45300, lr=0.000148577, gnorm=1.782, loss_scale=4, train_wall=22, gb_free=11, wall=13833
2021-05-05 20:28:01 | INFO | train_inner | epoch 001:  45407 / 60421 loss=1.392, ppl=2.62, wps=16312.5, ups=4.39, wpb=3715.8, bsz=152.9, num_updates=45400, lr=0.000148413, gnorm=1.683, loss_scale=4, train_wall=23, gb_free=11, wall=13855
2021-05-05 20:28:24 | INFO | train_inner | epoch 001:  45507 / 60421 loss=1.432, ppl=2.7, wps=16383.8, ups=4.46, wpb=3672.1, bsz=119.2, num_updates=45500, lr=0.00014825, gnorm=1.894, loss_scale=4, train_wall=22, gb_free=10.7, wall=13878
2021-05-05 20:28:47 | INFO | train_inner | epoch 001:  45607 / 60421 loss=1.31, ppl=2.48, wps=16173.3, ups=4.35, wpb=3715, bsz=121.9, num_updates=45600, lr=0.000148087, gnorm=1.431, loss_scale=4, train_wall=23, gb_free=10.8, wall=13901
2021-05-05 20:29:10 | INFO | train_inner | epoch 001:  45707 / 60421 loss=1.369, ppl=2.58, wps=16294.3, ups=4.32, wpb=3770, bsz=115.8, num_updates=45700, lr=0.000147925, gnorm=1.551, loss_scale=4, train_wall=23, gb_free=11.6, wall=13924
2021-05-05 20:29:33 | INFO | train_inner | epoch 001:  45807 / 60421 loss=1.502, ppl=2.83, wps=16225.7, ups=4.33, wpb=3746.2, bsz=145.8, num_updates=45800, lr=0.000147764, gnorm=1.959, loss_scale=4, train_wall=23, gb_free=10.7, wall=13947
2021-05-05 20:29:56 | INFO | train_inner | epoch 001:  45907 / 60421 loss=1.383, ppl=2.61, wps=16042.3, ups=4.3, wpb=3731.7, bsz=121.1, num_updates=45900, lr=0.000147602, gnorm=1.546, loss_scale=4, train_wall=23, gb_free=10.9, wall=13970
2021-05-05 20:30:20 | INFO | train_inner | epoch 001:  46007 / 60421 loss=1.383, ppl=2.61, wps=15981.4, ups=4.26, wpb=3754.4, bsz=138.4, num_updates=46000, lr=0.000147442, gnorm=1.437, loss_scale=4, train_wall=23, gb_free=10.6, wall=13994
2021-05-05 20:30:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:30:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:30:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:30:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:30:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:30:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:30:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:30:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:30:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:30:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:30:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:30:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:30:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:31:26 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.024 | ppl 16.27 | bleu 27.4 | wps 2385.9 | wpb 2024.1 | bsz 97.5 | num_updates 46000 | best_bleu 27.4
2021-05-05 20:31:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 46000 updates
2021-05-05 20:31:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_46000.pt
2021-05-05 20:31:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_46000.pt
2021-05-05 20:31:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_46000.pt (epoch 1 @ 46000 updates, score 27.4) (writing took 14.603747937988373 seconds)
2021-05-05 20:32:03 | INFO | train_inner | epoch 001:  46107 / 60421 loss=1.361, ppl=2.57, wps=3571.2, ups=0.96, wpb=3701.1, bsz=125.6, num_updates=46100, lr=0.000147282, gnorm=1.546, loss_scale=4, train_wall=22, gb_free=10.8, wall=14097
2021-05-05 20:32:26 | INFO | train_inner | epoch 001:  46207 / 60421 loss=1.338, ppl=2.53, wps=16534.5, ups=4.44, wpb=3728, bsz=136, num_updates=46200, lr=0.000147122, gnorm=1.456, loss_scale=4, train_wall=22, gb_free=10.8, wall=14120
2021-05-05 20:32:49 | INFO | train_inner | epoch 001:  46307 / 60421 loss=1.381, ppl=2.6, wps=16566.6, ups=4.4, wpb=3762.5, bsz=145.1, num_updates=46300, lr=0.000146964, gnorm=1.669, loss_scale=4, train_wall=23, gb_free=10.7, wall=14143
2021-05-05 20:33:11 | INFO | train_inner | epoch 001:  46407 / 60421 loss=1.368, ppl=2.58, wps=16506.1, ups=4.42, wpb=3736.5, bsz=140.4, num_updates=46400, lr=0.000146805, gnorm=1.595, loss_scale=4, train_wall=22, gb_free=10.6, wall=14165
2021-05-05 20:33:34 | INFO | train_inner | epoch 001:  46507 / 60421 loss=1.35, ppl=2.55, wps=16484.3, ups=4.43, wpb=3722.2, bsz=137.5, num_updates=46500, lr=0.000146647, gnorm=1.566, loss_scale=4, train_wall=22, gb_free=11, wall=14188
2021-05-05 20:33:57 | INFO | train_inner | epoch 001:  46607 / 60421 loss=1.347, ppl=2.54, wps=16494.7, ups=4.38, wpb=3762.4, bsz=147, num_updates=46600, lr=0.00014649, gnorm=1.553, loss_scale=4, train_wall=23, gb_free=10.8, wall=14211
2021-05-05 20:34:19 | INFO | train_inner | epoch 001:  46707 / 60421 loss=1.344, ppl=2.54, wps=16590.6, ups=4.42, wpb=3754.8, bsz=132.8, num_updates=46700, lr=0.000146333, gnorm=1.466, loss_scale=4, train_wall=22, gb_free=10.7, wall=14233
2021-05-05 20:34:42 | INFO | train_inner | epoch 001:  46807 / 60421 loss=1.391, ppl=2.62, wps=16500, ups=4.44, wpb=3713.1, bsz=138.4, num_updates=46800, lr=0.000146176, gnorm=1.666, loss_scale=4, train_wall=22, gb_free=10.7, wall=14256
2021-05-05 20:35:04 | INFO | train_inner | epoch 001:  46907 / 60421 loss=1.426, ppl=2.69, wps=16745.5, ups=4.44, wpb=3775.6, bsz=124.3, num_updates=46900, lr=0.00014602, gnorm=1.605, loss_scale=4, train_wall=22, gb_free=10.8, wall=14278
2021-05-05 20:35:27 | INFO | train_inner | epoch 001:  47007 / 60421 loss=1.315, ppl=2.49, wps=16688.9, ups=4.43, wpb=3770, bsz=147.3, num_updates=47000, lr=0.000145865, gnorm=1.458, loss_scale=4, train_wall=22, gb_free=10.9, wall=14301
2021-05-05 20:35:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:35:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:35:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:35:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:35:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:35:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:35:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:35:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:35:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:35:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:35:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:35:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:35:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:36:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.024 | ppl 16.27 | bleu 27.3 | wps 2494.3 | wpb 2024.1 | bsz 97.5 | num_updates 47000 | best_bleu 27.4
2021-05-05 20:36:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 47000 updates
2021-05-05 20:36:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_47000.pt
2021-05-05 20:36:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_47000.pt
2021-05-05 20:36:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_47000.pt (epoch 1 @ 47000 updates, score 27.3) (writing took 8.44332807599858 seconds)
2021-05-05 20:37:01 | INFO | train_inner | epoch 001:  47107 / 60421 loss=1.513, ppl=2.85, wps=3929.6, ups=1.06, wpb=3711.2, bsz=120, num_updates=47100, lr=0.00014571, gnorm=1.899, loss_scale=4, train_wall=22, gb_free=11, wall=14395
2021-05-05 20:37:24 | INFO | train_inner | epoch 001:  47207 / 60421 loss=1.479, ppl=2.79, wps=16853.6, ups=4.38, wpb=3845.4, bsz=131.3, num_updates=47200, lr=0.000145556, gnorm=1.56, loss_scale=4, train_wall=23, gb_free=10.8, wall=14418
2021-05-05 20:37:47 | INFO | train_inner | epoch 001:  47307 / 60421 loss=1.352, ppl=2.55, wps=16634.6, ups=4.37, wpb=3809.5, bsz=135.4, num_updates=47300, lr=0.000145402, gnorm=1.537, loss_scale=4, train_wall=23, gb_free=11, wall=14441
2021-05-05 20:38:10 | INFO | train_inner | epoch 001:  47407 / 60421 loss=1.436, ppl=2.71, wps=16245.9, ups=4.43, wpb=3663.2, bsz=122.3, num_updates=47400, lr=0.000145248, gnorm=2.067, loss_scale=4, train_wall=22, gb_free=10.7, wall=14464
2021-05-05 20:38:32 | INFO | train_inner | epoch 001:  47507 / 60421 loss=1.306, ppl=2.47, wps=16564.3, ups=4.39, wpb=3772.8, bsz=123.7, num_updates=47500, lr=0.000145095, gnorm=1.454, loss_scale=4, train_wall=23, gb_free=10.7, wall=14486
2021-05-05 20:38:55 | INFO | train_inner | epoch 001:  47607 / 60421 loss=1.273, ppl=2.42, wps=16379.7, ups=4.36, wpb=3756.1, bsz=132.6, num_updates=47600, lr=0.000144943, gnorm=1.359, loss_scale=4, train_wall=23, gb_free=10.8, wall=14509
2021-05-05 20:39:18 | INFO | train_inner | epoch 001:  47707 / 60421 loss=1.409, ppl=2.66, wps=16226.4, ups=4.4, wpb=3689.3, bsz=119.3, num_updates=47700, lr=0.000144791, gnorm=1.781, loss_scale=4, train_wall=23, gb_free=10.8, wall=14532
2021-05-05 20:39:41 | INFO | train_inner | epoch 001:  47807 / 60421 loss=1.399, ppl=2.64, wps=16066.1, ups=4.38, wpb=3670.7, bsz=136.6, num_updates=47800, lr=0.000144639, gnorm=1.895, loss_scale=4, train_wall=23, gb_free=10.7, wall=14555
2021-05-05 20:40:04 | INFO | train_inner | epoch 001:  47907 / 60421 loss=1.333, ppl=2.52, wps=16068.9, ups=4.36, wpb=3684.1, bsz=120.5, num_updates=47900, lr=0.000144488, gnorm=1.702, loss_scale=4, train_wall=23, gb_free=10.8, wall=14578
2021-05-05 20:40:28 | INFO | train_inner | epoch 001:  48007 / 60421 loss=1.349, ppl=2.55, wps=16026.7, ups=4.24, wpb=3776.4, bsz=138.5, num_updates=48000, lr=0.000144338, gnorm=1.693, loss_scale=4, train_wall=23, gb_free=10.7, wall=14601
2021-05-05 20:40:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:40:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:40:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:40:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:40:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:40:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:40:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:40:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:40:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:40:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:40:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:40:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:40:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:40:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:40:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:40:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:41:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.062 | ppl 16.7 | bleu 27.02 | wps 2407.6 | wpb 2024.1 | bsz 97.5 | num_updates 48000 | best_bleu 27.4
2021-05-05 20:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 48000 updates
2021-05-05 20:41:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_48000.pt
2021-05-05 20:41:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_48000.pt
2021-05-05 20:41:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_48000.pt (epoch 1 @ 48000 updates, score 27.02) (writing took 8.45252533799794 seconds)
2021-05-05 20:42:05 | INFO | train_inner | epoch 001:  48107 / 60421 loss=1.404, ppl=2.65, wps=3898.4, ups=1.02, wpb=3803.6, bsz=129, num_updates=48100, lr=0.000144187, gnorm=1.644, loss_scale=4, train_wall=23, gb_free=10.8, wall=14699
2021-05-05 20:42:28 | INFO | train_inner | epoch 001:  48207 / 60421 loss=1.376, ppl=2.59, wps=16394.5, ups=4.37, wpb=3752.9, bsz=114.4, num_updates=48200, lr=0.000144038, gnorm=1.603, loss_scale=4, train_wall=23, gb_free=10.9, wall=14722
2021-05-05 20:42:51 | INFO | train_inner | epoch 001:  48307 / 60421 loss=1.365, ppl=2.58, wps=16596.8, ups=4.4, wpb=3771.6, bsz=128.2, num_updates=48300, lr=0.000143889, gnorm=1.685, loss_scale=4, train_wall=23, gb_free=10.9, wall=14745
2021-05-05 20:43:13 | INFO | train_inner | epoch 001:  48407 / 60421 loss=1.344, ppl=2.54, wps=16347.8, ups=4.39, wpb=3727.7, bsz=152.2, num_updates=48400, lr=0.00014374, gnorm=1.555, loss_scale=4, train_wall=23, gb_free=10.9, wall=14767
2021-05-05 20:43:36 | INFO | train_inner | epoch 001:  48507 / 60421 loss=1.396, ppl=2.63, wps=16311.8, ups=4.4, wpb=3707.3, bsz=116.3, num_updates=48500, lr=0.000143592, gnorm=1.742, loss_scale=4, train_wall=23, gb_free=10.9, wall=14790
2021-05-05 20:43:59 | INFO | train_inner | epoch 001:  48607 / 60421 loss=1.355, ppl=2.56, wps=16323.2, ups=4.37, wpb=3734, bsz=133.4, num_updates=48600, lr=0.000143444, gnorm=1.953, loss_scale=4, train_wall=23, gb_free=10.8, wall=14813
2021-05-05 20:44:22 | INFO | train_inner | epoch 001:  48707 / 60421 loss=1.362, ppl=2.57, wps=16185.9, ups=4.29, wpb=3773, bsz=117, num_updates=48700, lr=0.000143296, gnorm=1.551, loss_scale=4, train_wall=23, gb_free=11.1, wall=14836
2021-05-05 20:44:46 | INFO | train_inner | epoch 001:  48807 / 60421 loss=1.359, ppl=2.57, wps=16168, ups=4.29, wpb=3764.6, bsz=132, num_updates=48800, lr=0.00014315, gnorm=1.554, loss_scale=4, train_wall=23, gb_free=10.7, wall=14860
2021-05-05 20:45:09 | INFO | train_inner | epoch 001:  48907 / 60421 loss=1.348, ppl=2.55, wps=15946.4, ups=4.26, wpb=3746.7, bsz=126.8, num_updates=48900, lr=0.000143003, gnorm=1.584, loss_scale=4, train_wall=23, gb_free=10.8, wall=14883
2021-05-05 20:45:33 | INFO | train_inner | epoch 001:  49007 / 60421 loss=1.374, ppl=2.59, wps=15533.8, ups=4.29, wpb=3621, bsz=136.4, num_updates=49000, lr=0.000142857, gnorm=1.978, loss_scale=4, train_wall=23, gb_free=11.1, wall=14906
2021-05-05 20:45:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:45:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:45:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:45:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:45:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:45:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:45:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:45:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:45:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:45:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:45:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:45:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:45:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:45:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:45:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:45:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:46:37 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.049 | ppl 16.55 | bleu 27.31 | wps 2451.9 | wpb 2024.1 | bsz 97.5 | num_updates 49000 | best_bleu 27.4
2021-05-05 20:46:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 49000 updates
2021-05-05 20:46:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_49000.pt
2021-05-05 20:46:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_49000.pt
2021-05-05 20:46:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_49000.pt (epoch 1 @ 49000 updates, score 27.31) (writing took 7.879311053009587 seconds)
2021-05-05 20:47:08 | INFO | train_inner | epoch 001:  49107 / 60421 loss=1.369, ppl=2.58, wps=3963.4, ups=1.05, wpb=3785.4, bsz=142.6, num_updates=49100, lr=0.000142712, gnorm=1.597, loss_scale=4, train_wall=23, gb_free=10.8, wall=15002
2021-05-05 20:47:31 | INFO | train_inner | epoch 001:  49207 / 60421 loss=1.385, ppl=2.61, wps=16479, ups=4.44, wpb=3713.2, bsz=116.7, num_updates=49200, lr=0.000142566, gnorm=1.783, loss_scale=4, train_wall=22, gb_free=10.7, wall=15024
2021-05-05 20:47:54 | INFO | train_inner | epoch 001:  49307 / 60421 loss=1.265, ppl=2.4, wps=16464.9, ups=4.35, wpb=3783.2, bsz=134.5, num_updates=49300, lr=0.000142422, gnorm=1.404, loss_scale=4, train_wall=23, gb_free=10.8, wall=15047
2021-05-05 20:48:16 | INFO | train_inner | epoch 001:  49407 / 60421 loss=1.296, ppl=2.45, wps=16478.5, ups=4.36, wpb=3780.7, bsz=143.9, num_updates=49400, lr=0.000142278, gnorm=1.485, loss_scale=4, train_wall=23, gb_free=10.7, wall=15070
2021-05-05 20:48:39 | INFO | train_inner | epoch 001:  49507 / 60421 loss=1.325, ppl=2.51, wps=16440, ups=4.37, wpb=3764, bsz=120.6, num_updates=49500, lr=0.000142134, gnorm=1.375, loss_scale=4, train_wall=23, gb_free=10.8, wall=15093
2021-05-05 20:49:02 | INFO | train_inner | epoch 001:  49607 / 60421 loss=1.426, ppl=2.69, wps=16278.8, ups=4.36, wpb=3734.6, bsz=137.7, num_updates=49600, lr=0.00014199, gnorm=1.68, loss_scale=4, train_wall=23, gb_free=10.8, wall=15116
2021-05-05 20:49:26 | INFO | train_inner | epoch 001:  49707 / 60421 loss=1.363, ppl=2.57, wps=16251.9, ups=4.31, wpb=3770.4, bsz=115.5, num_updates=49700, lr=0.000141848, gnorm=1.658, loss_scale=4, train_wall=23, gb_free=10.9, wall=15139
2021-05-05 20:49:49 | INFO | train_inner | epoch 001:  49807 / 60421 loss=1.285, ppl=2.44, wps=16116.7, ups=4.3, wpb=3746, bsz=146.7, num_updates=49800, lr=0.000141705, gnorm=1.428, loss_scale=4, train_wall=23, gb_free=10.8, wall=15163
2021-05-05 20:50:12 | INFO | train_inner | epoch 001:  49907 / 60421 loss=1.287, ppl=2.44, wps=15955.1, ups=4.31, wpb=3704.4, bsz=139.7, num_updates=49900, lr=0.000141563, gnorm=1.724, loss_scale=4, train_wall=23, gb_free=10.6, wall=15186
2021-05-05 20:50:36 | INFO | train_inner | epoch 001:  50007 / 60421 loss=1.324, ppl=2.5, wps=15798.9, ups=4.21, wpb=3748.6, bsz=136.5, num_updates=50000, lr=0.000141421, gnorm=1.596, loss_scale=4, train_wall=24, gb_free=10.7, wall=15210
2021-05-05 20:50:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:50:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:50:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:50:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:50:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:50:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:50:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:50:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:50:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:50:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:50:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:50:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:50:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:50:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:50:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:50:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:51:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.119 | ppl 17.38 | bleu 26.95 | wps 2430.5 | wpb 2024.1 | bsz 97.5 | num_updates 50000 | best_bleu 27.4
2021-05-05 20:51:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 50000 updates
2021-05-05 20:51:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_50000.pt
2021-05-05 20:51:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_50000.pt
2021-05-05 20:51:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_50000.pt (epoch 1 @ 50000 updates, score 26.95) (writing took 7.945473956002388 seconds)
2021-05-05 20:52:12 | INFO | train_inner | epoch 001:  50107 / 60421 loss=1.3, ppl=2.46, wps=3883.9, ups=1.04, wpb=3727.4, bsz=131.6, num_updates=50100, lr=0.00014128, gnorm=1.536, loss_scale=4, train_wall=23, gb_free=10.7, wall=15306
2021-05-05 20:52:34 | INFO | train_inner | epoch 001:  50207 / 60421 loss=1.391, ppl=2.62, wps=16554.2, ups=4.45, wpb=3719.2, bsz=122.2, num_updates=50200, lr=0.000141139, gnorm=1.607, loss_scale=4, train_wall=22, gb_free=10.8, wall=15328
2021-05-05 20:52:57 | INFO | train_inner | epoch 001:  50307 / 60421 loss=1.309, ppl=2.48, wps=16509.4, ups=4.36, wpb=3782.4, bsz=131.8, num_updates=50300, lr=0.000140999, gnorm=1.443, loss_scale=4, train_wall=23, gb_free=11.1, wall=15351
2021-05-05 20:53:20 | INFO | train_inner | epoch 001:  50407 / 60421 loss=1.366, ppl=2.58, wps=16592.4, ups=4.38, wpb=3790.8, bsz=149.9, num_updates=50400, lr=0.000140859, gnorm=1.561, loss_scale=4, train_wall=23, gb_free=10.8, wall=15374
2021-05-05 20:53:43 | INFO | train_inner | epoch 001:  50507 / 60421 loss=1.414, ppl=2.66, wps=16226.7, ups=4.43, wpb=3666.2, bsz=130.2, num_updates=50500, lr=0.00014072, gnorm=2.014, loss_scale=4, train_wall=22, gb_free=10.8, wall=15396
2021-05-05 20:54:06 | INFO | train_inner | epoch 001:  50607 / 60421 loss=1.342, ppl=2.54, wps=16395.3, ups=4.31, wpb=3802.1, bsz=120, num_updates=50600, lr=0.00014058, gnorm=1.392, loss_scale=4, train_wall=23, gb_free=10.9, wall=15420
2021-05-05 20:54:29 | INFO | train_inner | epoch 001:  50707 / 60421 loss=1.229, ppl=2.34, wps=16249.8, ups=4.3, wpb=3780.8, bsz=135.2, num_updates=50700, lr=0.000140442, gnorm=1.319, loss_scale=4, train_wall=23, gb_free=10.8, wall=15443
2021-05-05 20:54:52 | INFO | train_inner | epoch 001:  50807 / 60421 loss=1.307, ppl=2.47, wps=16045.3, ups=4.26, wpb=3764.4, bsz=125.7, num_updates=50800, lr=0.000140303, gnorm=1.575, loss_scale=4, train_wall=23, gb_free=10.8, wall=15466
2021-05-05 20:55:16 | INFO | train_inner | epoch 001:  50907 / 60421 loss=1.432, ppl=2.7, wps=15857.6, ups=4.24, wpb=3737.9, bsz=149, num_updates=50900, lr=0.000140165, gnorm=1.958, loss_scale=4, train_wall=23, gb_free=10.8, wall=15490
2021-05-05 20:55:40 | INFO | train_inner | epoch 001:  51007 / 60421 loss=1.31, ppl=2.48, wps=15943.2, ups=4.17, wpb=3823.1, bsz=133.6, num_updates=51000, lr=0.000140028, gnorm=1.35, loss_scale=4, train_wall=24, gb_free=10.8, wall=15514
2021-05-05 20:55:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 20:55:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:55:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:55:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:55:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:55:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:55:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:55:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:55:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:55:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:55:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:55:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:55:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:55:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:55:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:55:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:56:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 20:56:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 20:56:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 20:56:46 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.117 | ppl 17.35 | bleu 27.82 | wps 2388.6 | wpb 2024.1 | bsz 97.5 | num_updates 51000 | best_bleu 27.82
2021-05-05 20:56:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 51000 updates
2021-05-05 20:56:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_51000.pt
2021-05-05 20:56:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_51000.pt
2021-05-05 20:57:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_51000.pt (epoch 1 @ 51000 updates, score 27.82) (writing took 14.624995446996763 seconds)
2021-05-05 20:57:23 | INFO | train_inner | epoch 001:  51107 / 60421 loss=1.437, ppl=2.71, wps=3552.1, ups=0.97, wpb=3673.9, bsz=131, num_updates=51100, lr=0.000139891, gnorm=1.958, loss_scale=4, train_wall=22, gb_free=10.8, wall=15617
2021-05-05 20:57:46 | INFO | train_inner | epoch 001:  51207 / 60421 loss=1.321, ppl=2.5, wps=16344.8, ups=4.36, wpb=3749.9, bsz=129.4, num_updates=51200, lr=0.000139754, gnorm=1.595, loss_scale=4, train_wall=23, gb_free=11, wall=15640
2021-05-05 20:58:09 | INFO | train_inner | epoch 001:  51307 / 60421 loss=1.289, ppl=2.44, wps=16387.4, ups=4.43, wpb=3698.2, bsz=131, num_updates=51300, lr=0.000139618, gnorm=1.811, loss_scale=4, train_wall=22, gb_free=10.8, wall=15663
2021-05-05 20:58:32 | INFO | train_inner | epoch 001:  51407 / 60421 loss=1.342, ppl=2.53, wps=16398.6, ups=4.33, wpb=3785, bsz=138.1, num_updates=51400, lr=0.000139482, gnorm=1.585, loss_scale=4, train_wall=23, gb_free=10.8, wall=15686
2021-05-05 20:58:55 | INFO | train_inner | epoch 001:  51507 / 60421 loss=1.382, ppl=2.61, wps=16211.2, ups=4.38, wpb=3700.7, bsz=145.5, num_updates=51500, lr=0.000139347, gnorm=1.879, loss_scale=4, train_wall=23, gb_free=10.7, wall=15709
2021-05-05 20:59:18 | INFO | train_inner | epoch 001:  51607 / 60421 loss=1.23, ppl=2.35, wps=16303.2, ups=4.33, wpb=3767.8, bsz=141.9, num_updates=51600, lr=0.000139212, gnorm=1.317, loss_scale=4, train_wall=23, gb_free=10.7, wall=15732
2021-05-05 20:59:41 | INFO | train_inner | epoch 001:  51707 / 60421 loss=1.299, ppl=2.46, wps=16095, ups=4.28, wpb=3758.2, bsz=147.2, num_updates=51700, lr=0.000139077, gnorm=1.643, loss_scale=4, train_wall=23, gb_free=10.7, wall=15755
2021-05-05 21:00:05 | INFO | train_inner | epoch 001:  51807 / 60421 loss=1.304, ppl=2.47, wps=16017.5, ups=4.28, wpb=3744.2, bsz=129.3, num_updates=51800, lr=0.000138943, gnorm=1.535, loss_scale=4, train_wall=23, gb_free=10.6, wall=15779
2021-05-05 21:00:28 | INFO | train_inner | epoch 001:  51907 / 60421 loss=1.364, ppl=2.57, wps=15751.8, ups=4.33, wpb=3637.4, bsz=142.5, num_updates=51900, lr=0.000138809, gnorm=2.038, loss_scale=4, train_wall=23, gb_free=10.9, wall=15802
2021-05-05 21:00:52 | INFO | train_inner | epoch 001:  52007 / 60421 loss=1.268, ppl=2.41, wps=15701.3, ups=4.18, wpb=3760.3, bsz=132.2, num_updates=52000, lr=0.000138675, gnorm=1.469, loss_scale=4, train_wall=24, gb_free=11.1, wall=15826
2021-05-05 21:00:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:01:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:01:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:01:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:01:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:01:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:01:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:01:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:01:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:01:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:01:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:01:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:01:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:01:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:01:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:01:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:01:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.084 | ppl 16.95 | bleu 27.03 | wps 2451.7 | wpb 2024.1 | bsz 97.5 | num_updates 52000 | best_bleu 27.82
2021-05-05 21:01:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 52000 updates
2021-05-05 21:01:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_52000.pt
2021-05-05 21:01:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_52000.pt
2021-05-05 21:02:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_52000.pt (epoch 1 @ 52000 updates, score 27.03) (writing took 8.478571448998991 seconds)
2021-05-05 21:02:27 | INFO | train_inner | epoch 001:  52107 / 60421 loss=1.403, ppl=2.64, wps=3829, ups=1.05, wpb=3664.1, bsz=131.2, num_updates=52100, lr=0.000138542, gnorm=1.959, loss_scale=4, train_wall=22, gb_free=10.7, wall=15921
2021-05-05 21:02:50 | INFO | train_inner | epoch 001:  52207 / 60421 loss=1.234, ppl=2.35, wps=16474.9, ups=4.34, wpb=3793.2, bsz=151.5, num_updates=52200, lr=0.000138409, gnorm=1.557, loss_scale=4, train_wall=23, gb_free=10.8, wall=15944
2021-05-05 21:03:13 | INFO | train_inner | epoch 001:  52307 / 60421 loss=1.252, ppl=2.38, wps=16278.4, ups=4.4, wpb=3700.7, bsz=134.4, num_updates=52300, lr=0.000138277, gnorm=1.461, loss_scale=4, train_wall=23, gb_free=10.8, wall=15967
2021-05-05 21:03:36 | INFO | train_inner | epoch 001:  52407 / 60421 loss=1.311, ppl=2.48, wps=16414.9, ups=4.4, wpb=3726.9, bsz=134.4, num_updates=52400, lr=0.000138145, gnorm=1.568, loss_scale=4, train_wall=23, gb_free=11, wall=15990
2021-05-05 21:03:59 | INFO | train_inner | epoch 001:  52507 / 60421 loss=1.27, ppl=2.41, wps=16281.6, ups=4.33, wpb=3759.6, bsz=133.8, num_updates=52500, lr=0.000138013, gnorm=1.423, loss_scale=4, train_wall=23, gb_free=10.6, wall=16013
2021-05-05 21:04:22 | INFO | train_inner | epoch 001:  52607 / 60421 loss=1.334, ppl=2.52, wps=16276.8, ups=4.36, wpb=3734.6, bsz=110.6, num_updates=52600, lr=0.000137882, gnorm=1.455, loss_scale=4, train_wall=23, gb_free=10.7, wall=16036
2021-05-05 21:04:45 | INFO | train_inner | epoch 001:  52707 / 60421 loss=1.376, ppl=2.6, wps=16100.4, ups=4.36, wpb=3696.9, bsz=118.6, num_updates=52700, lr=0.000137751, gnorm=1.782, loss_scale=4, train_wall=23, gb_free=10.8, wall=16059
2021-05-05 21:05:08 | INFO | train_inner | epoch 001:  52807 / 60421 loss=1.278, ppl=2.42, wps=16111.2, ups=4.26, wpb=3785.2, bsz=139.3, num_updates=52800, lr=0.00013762, gnorm=1.533, loss_scale=4, train_wall=23, gb_free=10.8, wall=16082
2021-05-05 21:05:32 | INFO | train_inner | epoch 001:  52907 / 60421 loss=1.317, ppl=2.49, wps=15800.7, ups=4.22, wpb=3742.2, bsz=114.4, num_updates=52900, lr=0.00013749, gnorm=1.637, loss_scale=4, train_wall=23, gb_free=11.3, wall=16106
2021-05-05 21:05:55 | INFO | train_inner | epoch 001:  53007 / 60421 loss=1.298, ppl=2.46, wps=15508.7, ups=4.27, wpb=3633.1, bsz=133.3, num_updates=53000, lr=0.000137361, gnorm=1.832, loss_scale=4, train_wall=23, gb_free=11, wall=16129
2021-05-05 21:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:06:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:06:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:06:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:06:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:06:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:06:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:06:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:06:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:06:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:06:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:06:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:06:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:06:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:06:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:06:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:07:00 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.121 | ppl 17.39 | bleu 27.51 | wps 2457.2 | wpb 2024.1 | bsz 97.5 | num_updates 53000 | best_bleu 27.82
2021-05-05 21:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 53000 updates
2021-05-05 21:07:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_53000.pt
2021-05-05 21:07:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_53000.pt
2021-05-05 21:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_53000.pt (epoch 1 @ 53000 updates, score 27.51) (writing took 8.49902978099999 seconds)
2021-05-05 21:07:31 | INFO | train_inner | epoch 001:  53107 / 60421 loss=1.45, ppl=2.73, wps=3930.2, ups=1.04, wpb=3767.4, bsz=116.6, num_updates=53100, lr=0.000137231, gnorm=1.792, loss_scale=4, train_wall=23, gb_free=10.7, wall=16225
2021-05-05 21:07:54 | INFO | train_inner | epoch 001:  53207 / 60421 loss=1.212, ppl=2.32, wps=16261.9, ups=4.39, wpb=3701.9, bsz=165.9, num_updates=53200, lr=0.000137102, gnorm=1.6, loss_scale=4, train_wall=23, gb_free=10.8, wall=16248
2021-05-05 21:08:17 | INFO | train_inner | epoch 001:  53307 / 60421 loss=1.284, ppl=2.44, wps=16493.5, ups=4.39, wpb=3757.6, bsz=140, num_updates=53300, lr=0.000136973, gnorm=1.611, loss_scale=4, train_wall=23, gb_free=10.8, wall=16271
2021-05-05 21:08:40 | INFO | train_inner | epoch 001:  53407 / 60421 loss=1.272, ppl=2.41, wps=16170.5, ups=4.42, wpb=3660.1, bsz=145.5, num_updates=53400, lr=0.000136845, gnorm=2.202, loss_scale=4, train_wall=22, gb_free=10.7, wall=16293
2021-05-05 21:09:03 | INFO | train_inner | epoch 001:  53507 / 60421 loss=1.301, ppl=2.46, wps=16376.8, ups=4.32, wpb=3789.1, bsz=118.2, num_updates=53500, lr=0.000136717, gnorm=1.365, loss_scale=4, train_wall=23, gb_free=10.8, wall=16317
2021-05-05 21:09:26 | INFO | train_inner | epoch 001:  53607 / 60421 loss=1.295, ppl=2.45, wps=16433.1, ups=4.28, wpb=3842.8, bsz=117, num_updates=53600, lr=0.00013659, gnorm=1.314, loss_scale=4, train_wall=23, gb_free=11, wall=16340
2021-05-05 21:09:49 | INFO | train_inner | epoch 001:  53707 / 60421 loss=1.315, ppl=2.49, wps=16077, ups=4.32, wpb=3724.8, bsz=127.9, num_updates=53700, lr=0.000136462, gnorm=1.792, loss_scale=4, train_wall=23, gb_free=11.1, wall=16363
2021-05-05 21:10:12 | INFO | train_inner | epoch 001:  53807 / 60421 loss=1.36, ppl=2.57, wps=15932.3, ups=4.3, wpb=3706.8, bsz=130.5, num_updates=53800, lr=0.000136335, gnorm=1.904, loss_scale=4, train_wall=23, gb_free=10.7, wall=16386
2021-05-05 21:10:37 | INFO | train_inner | epoch 001:  53907 / 60421 loss=1.275, ppl=2.42, wps=15989.9, ups=4.16, wpb=3844.6, bsz=140.5, num_updates=53900, lr=0.000136209, gnorm=1.302, loss_scale=4, train_wall=24, gb_free=10.8, wall=16410
2021-05-05 21:11:00 | INFO | train_inner | epoch 001:  54007 / 60421 loss=1.237, ppl=2.36, wps=15544.9, ups=4.19, wpb=3707.5, bsz=131, num_updates=54000, lr=0.000136083, gnorm=1.59, loss_scale=4, train_wall=24, gb_free=11.3, wall=16434
2021-05-05 21:11:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:11:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:11:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:11:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:11:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:11:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:11:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:11:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:11:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:11:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:12:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.125 | ppl 17.44 | bleu 27.46 | wps 2464.3 | wpb 2024.1 | bsz 97.5 | num_updates 54000 | best_bleu 27.82
2021-05-05 21:12:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 54000 updates
2021-05-05 21:12:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_54000.pt
2021-05-05 21:12:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_54000.pt
2021-05-05 21:12:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_54000.pt (epoch 1 @ 54000 updates, score 27.46) (writing took 7.8921848320023855 seconds)
2021-05-05 21:12:35 | INFO | train_inner | epoch 001:  54107 / 60421 loss=1.296, ppl=2.46, wps=3942, ups=1.05, wpb=3748.1, bsz=115.6, num_updates=54100, lr=0.000135957, gnorm=1.492, loss_scale=4, train_wall=23, gb_free=10.7, wall=16529
2021-05-05 21:12:58 | INFO | train_inner | epoch 001:  54207 / 60421 loss=1.315, ppl=2.49, wps=16623.7, ups=4.39, wpb=3787.9, bsz=121.8, num_updates=54200, lr=0.000135831, gnorm=1.416, loss_scale=4, train_wall=23, gb_free=11.2, wall=16552
2021-05-05 21:13:21 | INFO | train_inner | epoch 001:  54307 / 60421 loss=1.257, ppl=2.39, wps=16453.4, ups=4.36, wpb=3776.5, bsz=141.5, num_updates=54300, lr=0.000135706, gnorm=1.387, loss_scale=4, train_wall=23, gb_free=10.7, wall=16575
2021-05-05 21:13:44 | INFO | train_inner | epoch 001:  54407 / 60421 loss=1.219, ppl=2.33, wps=16255.7, ups=4.38, wpb=3713.5, bsz=135.7, num_updates=54400, lr=0.000135582, gnorm=1.391, loss_scale=4, train_wall=23, gb_free=10.8, wall=16598
2021-05-05 21:14:07 | INFO | train_inner | epoch 001:  54507 / 60421 loss=1.208, ppl=2.31, wps=16317.6, ups=4.33, wpb=3771.9, bsz=136.6, num_updates=54500, lr=0.000135457, gnorm=1.325, loss_scale=4, train_wall=23, gb_free=10.9, wall=16621
2021-05-05 21:14:30 | INFO | train_inner | epoch 001:  54607 / 60421 loss=1.253, ppl=2.38, wps=15998.1, ups=4.35, wpb=3677.3, bsz=130.9, num_updates=54600, lr=0.000135333, gnorm=1.7, loss_scale=4, train_wall=23, gb_free=10.8, wall=16644
2021-05-05 21:14:53 | INFO | train_inner | epoch 001:  54707 / 60421 loss=1.283, ppl=2.43, wps=16216.5, ups=4.3, wpb=3771.5, bsz=133.5, num_updates=54700, lr=0.000135209, gnorm=1.589, loss_scale=4, train_wall=23, gb_free=10.8, wall=16667
2021-05-05 21:15:17 | INFO | train_inner | epoch 001:  54807 / 60421 loss=1.367, ppl=2.58, wps=15967.7, ups=4.28, wpb=3732, bsz=146.8, num_updates=54800, lr=0.000135086, gnorm=1.784, loss_scale=4, train_wall=23, gb_free=10.9, wall=16691
2021-05-05 21:15:40 | INFO | train_inner | epoch 001:  54907 / 60421 loss=1.342, ppl=2.54, wps=15682.2, ups=4.29, wpb=3652.6, bsz=111.5, num_updates=54900, lr=0.000134963, gnorm=1.787, loss_scale=4, train_wall=23, gb_free=11.1, wall=16714
2021-05-05 21:16:04 | INFO | train_inner | epoch 001:  55007 / 60421 loss=1.295, ppl=2.45, wps=15616.1, ups=4.17, wpb=3747.4, bsz=138.2, num_updates=55000, lr=0.00013484, gnorm=1.702, loss_scale=4, train_wall=24, gb_free=11.5, wall=16738
2021-05-05 21:16:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:16:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:16:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:16:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:16:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:16:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:16:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:16:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:16:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:16:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:16:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:16:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:16:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:17:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.17 | ppl 18 | bleu 27.19 | wps 2434.5 | wpb 2024.1 | bsz 97.5 | num_updates 55000 | best_bleu 27.82
2021-05-05 21:17:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 55000 updates
2021-05-05 21:17:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_55000.pt
2021-05-05 21:17:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_55000.pt
2021-05-05 21:17:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_55000.pt (epoch 1 @ 55000 updates, score 27.19) (writing took 10.253118433989584 seconds)
2021-05-05 21:17:42 | INFO | train_inner | epoch 001:  55107 / 60421 loss=1.302, ppl=2.47, wps=3833.1, ups=1.02, wpb=3762.1, bsz=119.9, num_updates=55100, lr=0.000134718, gnorm=1.516, loss_scale=4, train_wall=23, gb_free=11.2, wall=16836
2021-05-05 21:18:05 | INFO | train_inner | epoch 001:  55207 / 60421 loss=1.283, ppl=2.43, wps=16522.1, ups=4.43, wpb=3726, bsz=133.1, num_updates=55200, lr=0.000134595, gnorm=1.555, loss_scale=4, train_wall=22, gb_free=11, wall=16859
2021-05-05 21:18:27 | INFO | train_inner | epoch 001:  55307 / 60421 loss=1.262, ppl=2.4, wps=16486.9, ups=4.42, wpb=3730.4, bsz=129.2, num_updates=55300, lr=0.000134474, gnorm=1.628, loss_scale=4, train_wall=22, gb_free=10.9, wall=16881
2021-05-05 21:18:50 | INFO | train_inner | epoch 001:  55407 / 60421 loss=1.31, ppl=2.48, wps=16600.4, ups=4.42, wpb=3758.3, bsz=139, num_updates=55400, lr=0.000134352, gnorm=1.606, loss_scale=4, train_wall=22, gb_free=10.5, wall=16904
2021-05-05 21:19:13 | INFO | train_inner | epoch 001:  55507 / 60421 loss=1.395, ppl=2.63, wps=16555.2, ups=4.4, wpb=3761.1, bsz=127.7, num_updates=55500, lr=0.000134231, gnorm=1.735, loss_scale=4, train_wall=23, gb_free=10.8, wall=16927
2021-05-05 21:19:35 | INFO | train_inner | epoch 001:  55607 / 60421 loss=1.28, ppl=2.43, wps=16582.3, ups=4.44, wpb=3738.8, bsz=127, num_updates=55600, lr=0.00013411, gnorm=1.596, loss_scale=4, train_wall=22, gb_free=10.7, wall=16949
2021-05-05 21:19:58 | INFO | train_inner | epoch 001:  55707 / 60421 loss=1.32, ppl=2.5, wps=16549, ups=4.41, wpb=3751.6, bsz=140.8, num_updates=55700, lr=0.00013399, gnorm=1.657, loss_scale=4, train_wall=22, gb_free=10.7, wall=16972
2021-05-05 21:20:21 | INFO | train_inner | epoch 001:  55807 / 60421 loss=1.211, ppl=2.31, wps=16611.2, ups=4.42, wpb=3755.2, bsz=145.9, num_updates=55800, lr=0.00013387, gnorm=1.409, loss_scale=4, train_wall=22, gb_free=11, wall=16994
2021-05-05 21:20:43 | INFO | train_inner | epoch 001:  55907 / 60421 loss=1.235, ppl=2.35, wps=16473.3, ups=4.44, wpb=3710, bsz=135.3, num_updates=55900, lr=0.00013375, gnorm=1.573, loss_scale=4, train_wall=22, gb_free=11, wall=17017
2021-05-05 21:21:06 | INFO | train_inner | epoch 001:  56007 / 60421 loss=1.394, ppl=2.63, wps=16369.6, ups=4.4, wpb=3721.2, bsz=122.6, num_updates=56000, lr=0.000133631, gnorm=2.26, loss_scale=4, train_wall=23, gb_free=10.8, wall=17040
2021-05-05 21:21:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:21:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:21:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:21:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:21:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:21:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:21:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:21:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:21:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:21:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:22:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.095 | ppl 17.09 | bleu 27.3 | wps 2454.6 | wpb 2024.1 | bsz 97.5 | num_updates 56000 | best_bleu 27.82
2021-05-05 21:22:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 56000 updates
2021-05-05 21:22:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_56000.pt
2021-05-05 21:22:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_56000.pt
2021-05-05 21:22:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_56000.pt (epoch 1 @ 56000 updates, score 27.3) (writing took 7.887994136006455 seconds)
2021-05-05 21:22:41 | INFO | train_inner | epoch 001:  56107 / 60421 loss=1.328, ppl=2.51, wps=3915.1, ups=1.05, wpb=3728.2, bsz=120, num_updates=56100, lr=0.000133511, gnorm=1.606, loss_scale=4, train_wall=22, gb_free=10.8, wall=17135
2021-05-05 21:23:04 | INFO | train_inner | epoch 001:  56207 / 60421 loss=1.242, ppl=2.37, wps=16291.6, ups=4.42, wpb=3685.8, bsz=127.2, num_updates=56200, lr=0.000133393, gnorm=1.611, loss_scale=4, train_wall=22, gb_free=10.9, wall=17158
2021-05-05 21:23:26 | INFO | train_inner | epoch 001:  56307 / 60421 loss=1.328, ppl=2.51, wps=16457, ups=4.4, wpb=3742.5, bsz=135.9, num_updates=56300, lr=0.000133274, gnorm=1.707, loss_scale=4, train_wall=23, gb_free=11, wall=17180
2021-05-05 21:23:49 | INFO | train_inner | epoch 001:  56407 / 60421 loss=1.253, ppl=2.38, wps=16245.2, ups=4.38, wpb=3705.6, bsz=136.6, num_updates=56400, lr=0.000133156, gnorm=1.582, loss_scale=4, train_wall=23, gb_free=10.7, wall=17203
2021-05-05 21:24:13 | INFO | train_inner | epoch 001:  56507 / 60421 loss=1.205, ppl=2.31, wps=16372.7, ups=4.28, wpb=3827.8, bsz=133.7, num_updates=56500, lr=0.000133038, gnorm=1.205, loss_scale=4, train_wall=23, gb_free=11.2, wall=17226
2021-05-05 21:24:36 | INFO | train_inner | epoch 001:  56607 / 60421 loss=1.272, ppl=2.42, wps=16178.7, ups=4.29, wpb=3771.5, bsz=140.3, num_updates=56600, lr=0.00013292, gnorm=1.554, loss_scale=4, train_wall=23, gb_free=10.4, wall=17250
2021-05-05 21:24:59 | INFO | train_inner | epoch 001:  56707 / 60421 loss=1.415, ppl=2.67, wps=15987.7, ups=4.31, wpb=3711.7, bsz=145.8, num_updates=56700, lr=0.000132803, gnorm=2.281, loss_scale=8, train_wall=23, gb_free=11, wall=17273
2021-05-05 21:25:23 | INFO | train_inner | epoch 001:  56807 / 60421 loss=1.345, ppl=2.54, wps=15711.1, ups=4.27, wpb=3676.4, bsz=129.8, num_updates=56800, lr=0.000132686, gnorm=1.835, loss_scale=8, train_wall=23, gb_free=11.9, wall=17296
2021-05-05 21:25:46 | INFO | train_inner | epoch 001:  56907 / 60421 loss=1.279, ppl=2.43, wps=15760.4, ups=4.19, wpb=3762.9, bsz=140.3, num_updates=56900, lr=0.00013257, gnorm=1.741, loss_scale=8, train_wall=24, gb_free=10.8, wall=17320
2021-05-05 21:26:10 | INFO | train_inner | epoch 001:  57007 / 60421 loss=1.366, ppl=2.58, wps=15682.9, ups=4.28, wpb=3664.1, bsz=114.5, num_updates=57000, lr=0.000132453, gnorm=1.907, loss_scale=8, train_wall=23, gb_free=10.8, wall=17344
2021-05-05 21:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:26:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:26:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:26:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:26:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:26:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:26:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:26:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:26:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:26:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:26:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:26:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:26:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:26:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:26:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:26:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:27:14 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.154 | ppl 17.8 | bleu 27.45 | wps 2457.9 | wpb 2024.1 | bsz 97.5 | num_updates 57000 | best_bleu 27.82
2021-05-05 21:27:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 57000 updates
2021-05-05 21:27:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_57000.pt
2021-05-05 21:27:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_57000.pt
2021-05-05 21:27:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_57000.pt (epoch 1 @ 57000 updates, score 27.45) (writing took 8.403440308000427 seconds)
2021-05-05 21:27:46 | INFO | train_inner | epoch 001:  57107 / 60421 loss=1.294, ppl=2.45, wps=3889.2, ups=1.04, wpb=3722.4, bsz=123.1, num_updates=57100, lr=0.000132337, gnorm=1.774, loss_scale=8, train_wall=23, gb_free=10.8, wall=17439
2021-05-05 21:28:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-05 21:28:09 | INFO | train_inner | epoch 001:  57208 / 60421 loss=1.275, ppl=2.42, wps=16351.6, ups=4.35, wpb=3758, bsz=137, num_updates=57200, lr=0.000132221, gnorm=1.593, loss_scale=4, train_wall=23, gb_free=10.9, wall=17462
2021-05-05 21:28:31 | INFO | train_inner | epoch 001:  57308 / 60421 loss=1.437, ppl=2.71, wps=16482.2, ups=4.37, wpb=3774.7, bsz=105.7, num_updates=57300, lr=0.000132106, gnorm=1.617, loss_scale=4, train_wall=23, gb_free=10.9, wall=17485
2021-05-05 21:28:54 | INFO | train_inner | epoch 001:  57408 / 60421 loss=1.362, ppl=2.57, wps=16309.2, ups=4.36, wpb=3742.5, bsz=126.7, num_updates=57400, lr=0.000131991, gnorm=1.714, loss_scale=4, train_wall=23, gb_free=10.7, wall=17508
2021-05-05 21:29:17 | INFO | train_inner | epoch 001:  57508 / 60421 loss=1.34, ppl=2.53, wps=16297.9, ups=4.36, wpb=3734.9, bsz=115.4, num_updates=57500, lr=0.000131876, gnorm=1.718, loss_scale=4, train_wall=23, gb_free=10.8, wall=17531
2021-05-05 21:29:40 | INFO | train_inner | epoch 001:  57608 / 60421 loss=1.26, ppl=2.39, wps=16181.3, ups=4.34, wpb=3724.8, bsz=125.2, num_updates=57600, lr=0.000131762, gnorm=1.294, loss_scale=4, train_wall=23, gb_free=10.8, wall=17554
2021-05-05 21:30:04 | INFO | train_inner | epoch 001:  57708 / 60421 loss=1.24, ppl=2.36, wps=16044.4, ups=4.29, wpb=3743.8, bsz=123, num_updates=57700, lr=0.000131647, gnorm=1.5, loss_scale=4, train_wall=23, gb_free=10.8, wall=17577
2021-05-05 21:30:27 | INFO | train_inner | epoch 001:  57808 / 60421 loss=1.194, ppl=2.29, wps=15891.7, ups=4.29, wpb=3704.8, bsz=129.2, num_updates=57800, lr=0.000131533, gnorm=1.403, loss_scale=4, train_wall=23, gb_free=10.7, wall=17601
2021-05-05 21:30:51 | INFO | train_inner | epoch 001:  57908 / 60421 loss=1.344, ppl=2.54, wps=15742.7, ups=4.19, wpb=3761.5, bsz=126, num_updates=57900, lr=0.00013142, gnorm=1.723, loss_scale=4, train_wall=24, gb_free=10.9, wall=17625
2021-05-05 21:31:14 | INFO | train_inner | epoch 001:  58008 / 60421 loss=1.217, ppl=2.32, wps=16092.6, ups=4.23, wpb=3802.6, bsz=130.3, num_updates=58000, lr=0.000131306, gnorm=1.32, loss_scale=4, train_wall=23, gb_free=10.9, wall=17648
2021-05-05 21:31:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:31:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:31:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:31:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:31:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:31:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:31:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:31:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:31:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:31:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:31:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:31:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:31:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:32:19 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.165 | ppl 17.94 | bleu 28.06 | wps 2441.5 | wpb 2024.1 | bsz 97.5 | num_updates 58000 | best_bleu 28.06
2021-05-05 21:32:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 58000 updates
2021-05-05 21:32:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_58000.pt
2021-05-05 21:32:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_58000.pt
2021-05-05 21:32:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_58000.pt (epoch 1 @ 58000 updates, score 28.06) (writing took 14.719339490999118 seconds)
2021-05-05 21:32:57 | INFO | train_inner | epoch 001:  58108 / 60421 loss=1.257, ppl=2.39, wps=3714.6, ups=0.97, wpb=3813.6, bsz=128.6, num_updates=58100, lr=0.000131193, gnorm=1.52, loss_scale=4, train_wall=23, gb_free=10.6, wall=17751
2021-05-05 21:33:20 | INFO | train_inner | epoch 001:  58208 / 60421 loss=1.225, ppl=2.34, wps=16478, ups=4.35, wpb=3784.3, bsz=147.8, num_updates=58200, lr=0.000131081, gnorm=1.487, loss_scale=4, train_wall=23, gb_free=10.9, wall=17774
2021-05-05 21:33:43 | INFO | train_inner | epoch 001:  58308 / 60421 loss=1.301, ppl=2.46, wps=16476.8, ups=4.36, wpb=3774.9, bsz=125.1, num_updates=58300, lr=0.000130968, gnorm=1.614, loss_scale=4, train_wall=23, gb_free=10.7, wall=17797
2021-05-05 21:34:06 | INFO | train_inner | epoch 001:  58408 / 60421 loss=1.258, ppl=2.39, wps=16121.1, ups=4.38, wpb=3681.6, bsz=138.6, num_updates=58400, lr=0.000130856, gnorm=1.863, loss_scale=4, train_wall=23, gb_free=10.8, wall=17820
2021-05-05 21:34:29 | INFO | train_inner | epoch 001:  58508 / 60421 loss=1.406, ppl=2.65, wps=16227.6, ups=4.36, wpb=3720.8, bsz=146, num_updates=58500, lr=0.000130744, gnorm=1.947, loss_scale=4, train_wall=23, gb_free=10.7, wall=17843
2021-05-05 21:34:52 | INFO | train_inner | epoch 001:  58608 / 60421 loss=1.297, ppl=2.46, wps=16106.2, ups=4.27, wpb=3771.3, bsz=127.8, num_updates=58600, lr=0.000130632, gnorm=1.553, loss_scale=4, train_wall=23, gb_free=10.6, wall=17866
2021-05-05 21:35:16 | INFO | train_inner | epoch 001:  58708 / 60421 loss=1.32, ppl=2.5, wps=15966.7, ups=4.26, wpb=3748.7, bsz=128.8, num_updates=58700, lr=0.000130521, gnorm=1.705, loss_scale=4, train_wall=23, gb_free=11, wall=17890
2021-05-05 21:35:39 | INFO | train_inner | epoch 001:  58808 / 60421 loss=1.315, ppl=2.49, wps=15558.7, ups=4.24, wpb=3670, bsz=133.6, num_updates=58800, lr=0.00013041, gnorm=1.98, loss_scale=4, train_wall=23, gb_free=10.9, wall=17913
2021-05-05 21:36:03 | INFO | train_inner | epoch 001:  58908 / 60421 loss=1.341, ppl=2.53, wps=15720.4, ups=4.2, wpb=3746.2, bsz=133.3, num_updates=58900, lr=0.000130299, gnorm=1.847, loss_scale=4, train_wall=24, gb_free=10.8, wall=17937
2021-05-05 21:36:26 | INFO | train_inner | epoch 001:  59008 / 60421 loss=1.232, ppl=2.35, wps=16319.5, ups=4.29, wpb=3800.3, bsz=142.6, num_updates=59000, lr=0.000130189, gnorm=1.213, loss_scale=4, train_wall=23, gb_free=11, wall=17960
2021-05-05 21:36:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:36:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:36:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:36:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:36:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:36:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:36:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:36:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:36:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:36:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:36:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:36:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:36:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:36:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:36:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:36:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:37:30 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.177 | ppl 18.09 | bleu 28.15 | wps 2473.4 | wpb 2024.1 | bsz 97.5 | num_updates 59000 | best_bleu 28.15
2021-05-05 21:37:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 59000 updates
2021-05-05 21:37:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_59000.pt
2021-05-05 21:37:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_59000.pt
2021-05-05 21:37:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_59000.pt (epoch 1 @ 59000 updates, score 28.15) (writing took 14.622047789991484 seconds)
2021-05-05 21:38:08 | INFO | train_inner | epoch 001:  59108 / 60421 loss=1.272, ppl=2.41, wps=3636.7, ups=0.99, wpb=3690.3, bsz=119.7, num_updates=59100, lr=0.000130079, gnorm=1.759, loss_scale=4, train_wall=23, gb_free=10.7, wall=18062
2021-05-05 21:38:31 | INFO | train_inner | epoch 001:  59208 / 60421 loss=1.211, ppl=2.32, wps=16526.1, ups=4.41, wpb=3743.8, bsz=140.6, num_updates=59200, lr=0.000129969, gnorm=1.413, loss_scale=4, train_wall=22, gb_free=10.8, wall=18084
2021-05-05 21:38:53 | INFO | train_inner | epoch 001:  59308 / 60421 loss=1.348, ppl=2.55, wps=16212.2, ups=4.42, wpb=3668.9, bsz=131.4, num_updates=59300, lr=0.000129859, gnorm=1.95, loss_scale=4, train_wall=22, gb_free=11.1, wall=18107
2021-05-05 21:39:16 | INFO | train_inner | epoch 001:  59408 / 60421 loss=1.249, ppl=2.38, wps=16417, ups=4.32, wpb=3798.6, bsz=127.6, num_updates=59400, lr=0.00012975, gnorm=1.273, loss_scale=4, train_wall=23, gb_free=10.8, wall=18130
2021-05-05 21:39:39 | INFO | train_inner | epoch 001:  59508 / 60421 loss=1.293, ppl=2.45, wps=16301.2, ups=4.34, wpb=3758, bsz=116, num_updates=59500, lr=0.000129641, gnorm=1.54, loss_scale=4, train_wall=23, gb_free=10.7, wall=18153
2021-05-05 21:40:03 | INFO | train_inner | epoch 001:  59608 / 60421 loss=1.21, ppl=2.31, wps=16043.4, ups=4.29, wpb=3736.9, bsz=112.8, num_updates=59600, lr=0.000129532, gnorm=1.279, loss_scale=4, train_wall=23, gb_free=10.6, wall=18177
2021-05-05 21:40:26 | INFO | train_inner | epoch 001:  59708 / 60421 loss=1.255, ppl=2.39, wps=15790.5, ups=4.25, wpb=3711.8, bsz=131.4, num_updates=59700, lr=0.000129423, gnorm=1.831, loss_scale=4, train_wall=23, gb_free=10.9, wall=18200
2021-05-05 21:40:50 | INFO | train_inner | epoch 001:  59808 / 60421 loss=1.343, ppl=2.54, wps=15451.6, ups=4.24, wpb=3647.2, bsz=115.6, num_updates=59800, lr=0.000129315, gnorm=2.166, loss_scale=4, train_wall=23, gb_free=10.8, wall=18224
2021-05-05 21:41:13 | INFO | train_inner | epoch 001:  59908 / 60421 loss=1.318, ppl=2.49, wps=15735.6, ups=4.23, wpb=3720.4, bsz=128.8, num_updates=59900, lr=0.000129207, gnorm=1.809, loss_scale=4, train_wall=23, gb_free=10.6, wall=18247
2021-05-05 21:41:36 | INFO | train_inner | epoch 001:  60008 / 60421 loss=1.364, ppl=2.57, wps=16121.9, ups=4.37, wpb=3685.6, bsz=140.9, num_updates=60000, lr=0.000129099, gnorm=2.304, loss_scale=4, train_wall=23, gb_free=11.1, wall=18270
2021-05-05 21:41:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:41:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:41:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:41:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:41:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:41:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:41:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:41:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:41:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:41:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:41:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:41:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:41:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:42:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.161 | ppl 17.89 | bleu 26.61 | wps 2448.3 | wpb 2024.1 | bsz 97.5 | num_updates 60000 | best_bleu 28.15
2021-05-05 21:42:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 60000 updates
2021-05-05 21:42:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_60000.pt
2021-05-05 21:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_60000.pt
2021-05-05 21:42:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_1_60000.pt (epoch 1 @ 60000 updates, score 26.61) (writing took 8.375441317999503 seconds)
2021-05-05 21:43:12 | INFO | train_inner | epoch 001:  60108 / 60421 loss=1.286, ppl=2.44, wps=3862.6, ups=1.04, wpb=3705.9, bsz=127.5, num_updates=60100, lr=0.000128992, gnorm=1.881, loss_scale=4, train_wall=22, gb_free=10.8, wall=18366
2021-05-05 21:43:35 | INFO | train_inner | epoch 001:  60208 / 60421 loss=1.303, ppl=2.47, wps=16576.7, ups=4.39, wpb=3778.7, bsz=138, num_updates=60200, lr=0.000128885, gnorm=1.501, loss_scale=4, train_wall=23, gb_free=10.8, wall=18389
2021-05-05 21:43:58 | INFO | train_inner | epoch 001:  60308 / 60421 loss=1.321, ppl=2.5, wps=16344.8, ups=4.36, wpb=3750.9, bsz=112.8, num_updates=60300, lr=0.000128778, gnorm=1.52, loss_scale=4, train_wall=23, gb_free=10.7, wall=18412
2021-05-05 21:44:21 | INFO | train_inner | epoch 001:  60408 / 60421 loss=1.339, ppl=2.53, wps=16270.2, ups=4.29, wpb=3789.1, bsz=123.9, num_updates=60400, lr=0.000128671, gnorm=1.671, loss_scale=4, train_wall=23, gb_free=10.7, wall=18435
2021-05-05 21:44:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:44:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:44:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:44:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:44:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:44:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:44:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:44:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:44:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:44:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:44:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:44:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:44:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:44:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:44:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:44:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:45:28 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.169 | ppl 17.99 | bleu 27.86 | wps 2492.5 | wpb 2024.1 | bsz 97.5 | num_updates 60413 | best_bleu 28.15
2021-05-05 21:45:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 60413 updates
2021-05-05 21:45:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint1.pt
2021-05-05 21:45:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint1.pt
2021-05-05 21:45:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint1.pt (epoch 1 @ 60413 updates, score 27.86) (writing took 8.09334430799936 seconds)
2021-05-05 21:45:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-05-05 21:45:36 | INFO | train | epoch 001 | loss 1.725 | ppl 3.31 | wps 12201.4 | ups 3.26 | wpb 3737.5 | bsz 132.4 | num_updates 60413 | lr 0.000128657 | gnorm 1.846 | loss_scale 4 | train_wall 13763 | gb_free 10.8 | wall 18510
2021-05-05 21:45:36 | INFO | fairseq.trainer | begin training epoch 2
2021-05-05 21:45:36 | INFO | fairseq_cli.train | Start iterating over samples
2021-05-05 21:45:57 | INFO | train_inner | epoch 002:     87 / 60421 loss=1.141, ppl=2.21, wps=3915.7, ups=1.05, wpb=3742.8, bsz=139.8, num_updates=60500, lr=0.000128565, gnorm=1.383, loss_scale=4, train_wall=23, gb_free=10.9, wall=18531
2021-05-05 21:46:20 | INFO | train_inner | epoch 002:    187 / 60421 loss=1.087, ppl=2.12, wps=16049, ups=4.34, wpb=3697.9, bsz=153.9, num_updates=60600, lr=0.000128459, gnorm=1.419, loss_scale=4, train_wall=23, gb_free=10.8, wall=18554
2021-05-05 21:46:43 | INFO | train_inner | epoch 002:    287 / 60421 loss=1.146, ppl=2.21, wps=16400.1, ups=4.34, wpb=3781, bsz=133.6, num_updates=60700, lr=0.000128353, gnorm=1.371, loss_scale=4, train_wall=23, gb_free=10.7, wall=18577
2021-05-05 21:47:06 | INFO | train_inner | epoch 002:    387 / 60421 loss=1.182, ppl=2.27, wps=16430.3, ups=4.4, wpb=3731, bsz=120.5, num_updates=60800, lr=0.000128247, gnorm=1.444, loss_scale=4, train_wall=23, gb_free=10.8, wall=18599
2021-05-05 21:47:28 | INFO | train_inner | epoch 002:    487 / 60421 loss=1.097, ppl=2.14, wps=16388.2, ups=4.41, wpb=3712.8, bsz=137.7, num_updates=60900, lr=0.000128142, gnorm=1.299, loss_scale=4, train_wall=22, gb_free=11.5, wall=18622
2021-05-05 21:47:51 | INFO | train_inner | epoch 002:    587 / 60421 loss=1.127, ppl=2.18, wps=16523.3, ups=4.34, wpb=3805.9, bsz=137.8, num_updates=61000, lr=0.000128037, gnorm=1.389, loss_scale=4, train_wall=23, gb_free=10.7, wall=18645
2021-05-05 21:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:48:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:48:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:48:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:48:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:48:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:48:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:48:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:48:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:48:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:48:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:48:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:48:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:48:57 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.241 | ppl 18.92 | bleu 28 | wps 2400 | wpb 2024.1 | bsz 97.5 | num_updates 61000 | best_bleu 28.15
2021-05-05 21:48:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 61000 updates
2021-05-05 21:48:57 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_61000.pt
2021-05-05 21:49:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_61000.pt
2021-05-05 21:49:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_61000.pt (epoch 2 @ 61000 updates, score 28.0) (writing took 8.550310416001594 seconds)
2021-05-05 21:49:29 | INFO | train_inner | epoch 002:    687 / 60421 loss=1.158, ppl=2.23, wps=3811.8, ups=1.03, wpb=3714.2, bsz=127.7, num_updates=61100, lr=0.000127932, gnorm=1.341, loss_scale=4, train_wall=23, gb_free=10.9, wall=18743
2021-05-05 21:49:52 | INFO | train_inner | epoch 002:    787 / 60421 loss=1.237, ppl=2.36, wps=16222.3, ups=4.3, wpb=3768.4, bsz=120.4, num_updates=61200, lr=0.000127827, gnorm=1.557, loss_scale=4, train_wall=23, gb_free=10.7, wall=18766
2021-05-05 21:50:15 | INFO | train_inner | epoch 002:    887 / 60421 loss=1.231, ppl=2.35, wps=16037.3, ups=4.28, wpb=3747.7, bsz=131.4, num_updates=61300, lr=0.000127723, gnorm=1.542, loss_scale=4, train_wall=23, gb_free=10.6, wall=18789
2021-05-05 21:50:39 | INFO | train_inner | epoch 002:    987 / 60421 loss=1.184, ppl=2.27, wps=15647.2, ups=4.22, wpb=3706.6, bsz=130.2, num_updates=61400, lr=0.000127619, gnorm=1.529, loss_scale=4, train_wall=24, gb_free=10.8, wall=18813
2021-05-05 21:51:03 | INFO | train_inner | epoch 002:   1087 / 60421 loss=1.117, ppl=2.17, wps=15698, ups=4.23, wpb=3713.5, bsz=155.8, num_updates=61500, lr=0.000127515, gnorm=1.499, loss_scale=4, train_wall=23, gb_free=10.9, wall=18837
2021-05-05 21:51:26 | INFO | train_inner | epoch 002:   1187 / 60421 loss=1.121, ppl=2.18, wps=16233.8, ups=4.29, wpb=3786.3, bsz=139.8, num_updates=61600, lr=0.000127412, gnorm=1.249, loss_scale=4, train_wall=23, gb_free=10.7, wall=18860
2021-05-05 21:51:49 | INFO | train_inner | epoch 002:   1287 / 60421 loss=1.246, ppl=2.37, wps=16117, ups=4.43, wpb=3636.3, bsz=133.8, num_updates=61700, lr=0.000127309, gnorm=1.99, loss_scale=4, train_wall=22, gb_free=11.1, wall=18882
2021-05-05 21:52:12 | INFO | train_inner | epoch 002:   1387 / 60421 loss=1.142, ppl=2.21, wps=16367.7, ups=4.36, wpb=3752.9, bsz=139.1, num_updates=61800, lr=0.000127205, gnorm=1.428, loss_scale=4, train_wall=23, gb_free=10.9, wall=18905
2021-05-05 21:52:34 | INFO | train_inner | epoch 002:   1487 / 60421 loss=1.176, ppl=2.26, wps=16314.1, ups=4.36, wpb=3743.1, bsz=131.8, num_updates=61900, lr=0.000127103, gnorm=1.537, loss_scale=4, train_wall=23, gb_free=10.8, wall=18928
2021-05-05 21:52:57 | INFO | train_inner | epoch 002:   1587 / 60421 loss=1.175, ppl=2.26, wps=16424.1, ups=4.37, wpb=3758.6, bsz=128.6, num_updates=62000, lr=0.000127, gnorm=1.395, loss_scale=4, train_wall=23, gb_free=10.7, wall=18951
2021-05-05 21:52:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:53:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:53:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:53:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:53:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:53:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:53:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:53:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:53:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:53:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:53:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:53:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:53:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:53:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:53:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:53:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:54:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.231 | ppl 18.77 | bleu 28.3 | wps 2460.5 | wpb 2024.1 | bsz 97.5 | num_updates 62000 | best_bleu 28.3
2021-05-05 21:54:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 62000 updates
2021-05-05 21:54:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_62000.pt
2021-05-05 21:54:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_62000.pt
2021-05-05 21:54:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_62000.pt (epoch 2 @ 62000 updates, score 28.3) (writing took 14.635473781003384 seconds)
2021-05-05 21:54:39 | INFO | train_inner | epoch 002:   1687 / 60421 loss=1.106, ppl=2.15, wps=3650.9, ups=0.98, wpb=3727.2, bsz=156.9, num_updates=62100, lr=0.000126898, gnorm=1.51, loss_scale=4, train_wall=23, gb_free=10.9, wall=19053
2021-05-05 21:55:02 | INFO | train_inner | epoch 002:   1787 / 60421 loss=1.18, ppl=2.27, wps=16229.3, ups=4.35, wpb=3734.5, bsz=126.2, num_updates=62200, lr=0.000126796, gnorm=1.447, loss_scale=4, train_wall=23, gb_free=11, wall=19076
2021-05-05 21:55:26 | INFO | train_inner | epoch 002:   1887 / 60421 loss=1.149, ppl=2.22, wps=16112.2, ups=4.29, wpb=3755, bsz=123.6, num_updates=62300, lr=0.000126694, gnorm=1.387, loss_scale=4, train_wall=23, gb_free=11, wall=19100
2021-05-05 21:55:50 | INFO | train_inner | epoch 002:   1987 / 60421 loss=1.094, ppl=2.14, wps=15859, ups=4.19, wpb=3781.5, bsz=144.3, num_updates=62400, lr=0.000126592, gnorm=1.325, loss_scale=4, train_wall=24, gb_free=10.9, wall=19123
2021-05-05 21:56:13 | INFO | train_inner | epoch 002:   2087 / 60421 loss=1.276, ppl=2.42, wps=16185.9, ups=4.25, wpb=3806.8, bsz=111.5, num_updates=62500, lr=0.000126491, gnorm=1.525, loss_scale=4, train_wall=23, gb_free=10.6, wall=19147
2021-05-05 21:56:36 | INFO | train_inner | epoch 002:   2187 / 60421 loss=1.21, ppl=2.31, wps=16284.6, ups=4.37, wpb=3722.8, bsz=117.3, num_updates=62600, lr=0.00012639, gnorm=1.516, loss_scale=4, train_wall=23, gb_free=10.9, wall=19170
2021-05-05 21:56:59 | INFO | train_inner | epoch 002:   2287 / 60421 loss=1.141, ppl=2.21, wps=16396, ups=4.32, wpb=3792, bsz=130, num_updates=62700, lr=0.000126289, gnorm=1.524, loss_scale=4, train_wall=23, gb_free=10.7, wall=19193
2021-05-05 21:57:22 | INFO | train_inner | epoch 002:   2387 / 60421 loss=1.211, ppl=2.32, wps=16392, ups=4.4, wpb=3722.1, bsz=129.8, num_updates=62800, lr=0.000126189, gnorm=1.733, loss_scale=4, train_wall=23, gb_free=10.7, wall=19216
2021-05-05 21:57:45 | INFO | train_inner | epoch 002:   2487 / 60421 loss=1.167, ppl=2.25, wps=16521.5, ups=4.39, wpb=3767.5, bsz=124.3, num_updates=62900, lr=0.000126088, gnorm=1.48, loss_scale=4, train_wall=23, gb_free=11, wall=19239
2021-05-05 21:58:08 | INFO | train_inner | epoch 002:   2587 / 60421 loss=1.186, ppl=2.27, wps=16251, ups=4.35, wpb=3733.2, bsz=135.4, num_updates=63000, lr=0.000125988, gnorm=1.502, loss_scale=4, train_wall=23, gb_free=10.7, wall=19261
2021-05-05 21:58:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 21:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:58:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 21:58:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 21:58:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 21:59:12 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.324 | ppl 20.03 | bleu 27.96 | wps 2466.4 | wpb 2024.1 | bsz 97.5 | num_updates 63000 | best_bleu 28.3
2021-05-05 21:59:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 63000 updates
2021-05-05 21:59:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_63000.pt
2021-05-05 21:59:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_63000.pt
2021-05-05 21:59:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_63000.pt (epoch 2 @ 63000 updates, score 27.96) (writing took 10.306822214988642 seconds)
2021-05-05 21:59:45 | INFO | train_inner | epoch 002:   2687 / 60421 loss=1.17, ppl=2.25, wps=3821.3, ups=1.02, wpb=3735.6, bsz=126.9, num_updates=63100, lr=0.000125888, gnorm=1.469, loss_scale=4, train_wall=23, gb_free=10.7, wall=19359
2021-05-05 22:00:09 | INFO | train_inner | epoch 002:   2787 / 60421 loss=1.153, ppl=2.22, wps=16290.2, ups=4.25, wpb=3836.4, bsz=139.1, num_updates=63200, lr=0.000125789, gnorm=1.387, loss_scale=4, train_wall=23, gb_free=10.7, wall=19383
2021-05-05 22:00:32 | INFO | train_inner | epoch 002:   2887 / 60421 loss=1.127, ppl=2.18, wps=15775.6, ups=4.26, wpb=3703.2, bsz=131.1, num_updates=63300, lr=0.000125689, gnorm=1.619, loss_scale=4, train_wall=23, gb_free=10.8, wall=19406
2021-05-05 22:00:56 | INFO | train_inner | epoch 002:   2987 / 60421 loss=1.209, ppl=2.31, wps=15752.7, ups=4.22, wpb=3736.2, bsz=121.2, num_updates=63400, lr=0.00012559, gnorm=1.374, loss_scale=4, train_wall=24, gb_free=10.7, wall=19430
2021-05-05 22:01:20 | INFO | train_inner | epoch 002:   3087 / 60421 loss=1.163, ppl=2.24, wps=16144.1, ups=4.27, wpb=3785, bsz=129.2, num_updates=63500, lr=0.000125491, gnorm=1.288, loss_scale=4, train_wall=23, gb_free=10.9, wall=19453
2021-05-05 22:01:43 | INFO | train_inner | epoch 002:   3187 / 60421 loss=1.142, ppl=2.21, wps=16212.2, ups=4.35, wpb=3730.8, bsz=145.2, num_updates=63600, lr=0.000125392, gnorm=1.563, loss_scale=4, train_wall=23, gb_free=10.9, wall=19476
2021-05-05 22:02:06 | INFO | train_inner | epoch 002:   3287 / 60421 loss=1.134, ppl=2.19, wps=16491.5, ups=4.33, wpb=3806.2, bsz=141.4, num_updates=63700, lr=0.000125294, gnorm=1.204, loss_scale=4, train_wall=23, gb_free=10.7, wall=19500
2021-05-05 22:02:28 | INFO | train_inner | epoch 002:   3387 / 60421 loss=1.128, ppl=2.19, wps=16222.5, ups=4.44, wpb=3652.8, bsz=132.2, num_updates=63800, lr=0.000125196, gnorm=1.483, loss_scale=4, train_wall=22, gb_free=10.7, wall=19522
2021-05-05 22:02:51 | INFO | train_inner | epoch 002:   3487 / 60421 loss=1.194, ppl=2.29, wps=16600.4, ups=4.38, wpb=3791.7, bsz=133.4, num_updates=63900, lr=0.000125098, gnorm=1.501, loss_scale=4, train_wall=23, gb_free=11, wall=19545
2021-05-05 22:03:14 | INFO | train_inner | epoch 002:   3587 / 60421 loss=1.148, ppl=2.22, wps=16361, ups=4.36, wpb=3753.1, bsz=133.6, num_updates=64000, lr=0.000125, gnorm=1.322, loss_scale=4, train_wall=23, gb_free=10.7, wall=19568
2021-05-05 22:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:03:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:03:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:03:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:03:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:03:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:03:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:03:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:03:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:03:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:03:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:03:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:03:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:04:17 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.294 | ppl 19.61 | bleu 27.87 | wps 2495.5 | wpb 2024.1 | bsz 97.5 | num_updates 64000 | best_bleu 28.3
2021-05-05 22:04:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 64000 updates
2021-05-05 22:04:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_64000.pt
2021-05-05 22:04:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_64000.pt
2021-05-05 22:04:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_64000.pt (epoch 2 @ 64000 updates, score 27.87) (writing took 8.409949058011989 seconds)
2021-05-05 22:04:49 | INFO | train_inner | epoch 002:   3687 / 60421 loss=1.21, ppl=2.31, wps=3992.1, ups=1.05, wpb=3799.2, bsz=124.7, num_updates=64100, lr=0.000124902, gnorm=1.423, loss_scale=4, train_wall=23, gb_free=10.8, wall=19663
2021-05-05 22:05:12 | INFO | train_inner | epoch 002:   3787 / 60421 loss=1.204, ppl=2.3, wps=16112.2, ups=4.33, wpb=3717.6, bsz=102.4, num_updates=64200, lr=0.000124805, gnorm=1.411, loss_scale=4, train_wall=23, gb_free=10.8, wall=19686
2021-05-05 22:05:36 | INFO | train_inner | epoch 002:   3887 / 60421 loss=1.107, ppl=2.15, wps=15967.3, ups=4.29, wpb=3726.1, bsz=128.3, num_updates=64300, lr=0.000124708, gnorm=1.444, loss_scale=4, train_wall=23, gb_free=10.8, wall=19709
2021-05-05 22:05:59 | INFO | train_inner | epoch 002:   3987 / 60421 loss=1.181, ppl=2.27, wps=15824.9, ups=4.21, wpb=3760.4, bsz=117.5, num_updates=64400, lr=0.000124611, gnorm=1.345, loss_scale=4, train_wall=24, gb_free=10.8, wall=19733
2021-05-05 22:06:23 | INFO | train_inner | epoch 002:   4087 / 60421 loss=1.055, ppl=2.08, wps=15991.5, ups=4.28, wpb=3737.9, bsz=163.1, num_updates=64500, lr=0.000124515, gnorm=1.416, loss_scale=4, train_wall=23, gb_free=10.8, wall=19757
2021-05-05 22:06:46 | INFO | train_inner | epoch 002:   4187 / 60421 loss=1.18, ppl=2.27, wps=16300.8, ups=4.38, wpb=3718.9, bsz=139, num_updates=64600, lr=0.000124418, gnorm=1.611, loss_scale=4, train_wall=23, gb_free=10.8, wall=19779
2021-05-05 22:07:08 | INFO | train_inner | epoch 002:   4287 / 60421 loss=1.101, ppl=2.14, wps=16385.8, ups=4.37, wpb=3748.8, bsz=149.4, num_updates=64700, lr=0.000124322, gnorm=1.564, loss_scale=4, train_wall=23, gb_free=10.8, wall=19802
2021-05-05 22:07:31 | INFO | train_inner | epoch 002:   4387 / 60421 loss=1.253, ppl=2.38, wps=16449.4, ups=4.41, wpb=3731.2, bsz=115.8, num_updates=64800, lr=0.000124226, gnorm=1.727, loss_scale=4, train_wall=22, gb_free=10.8, wall=19825
2021-05-05 22:07:54 | INFO | train_inner | epoch 002:   4487 / 60421 loss=1.152, ppl=2.22, wps=16565.9, ups=4.37, wpb=3792.8, bsz=129, num_updates=64900, lr=0.00012413, gnorm=1.326, loss_scale=4, train_wall=23, gb_free=10.8, wall=19848
2021-05-05 22:08:17 | INFO | train_inner | epoch 002:   4587 / 60421 loss=1.127, ppl=2.18, wps=16234, ups=4.4, wpb=3687.3, bsz=134.2, num_updates=65000, lr=0.000124035, gnorm=1.547, loss_scale=4, train_wall=23, gb_free=11.5, wall=19871
2021-05-05 22:08:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:08:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:08:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:08:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:08:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:08:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:08:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:08:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:08:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:08:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:08:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:08:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:08:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:08:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:08:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:08:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:08:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:08:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:08:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:09:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.329 | ppl 20.09 | bleu 28.09 | wps 2466 | wpb 2024.1 | bsz 97.5 | num_updates 65000 | best_bleu 28.3
2021-05-05 22:09:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 65000 updates
2021-05-05 22:09:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_65000.pt
2021-05-05 22:09:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_65000.pt
2021-05-05 22:09:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_65000.pt (epoch 2 @ 65000 updates, score 28.09) (writing took 8.207077084996854 seconds)
2021-05-05 22:09:53 | INFO | train_inner | epoch 002:   4687 / 60421 loss=1.132, ppl=2.19, wps=3912, ups=1.04, wpb=3750.8, bsz=126.4, num_updates=65100, lr=0.000123939, gnorm=1.354, loss_scale=4, train_wall=23, gb_free=11.2, wall=19966
2021-05-05 22:10:16 | INFO | train_inner | epoch 002:   4787 / 60421 loss=1.167, ppl=2.24, wps=16215.4, ups=4.29, wpb=3780.4, bsz=129.5, num_updates=65200, lr=0.000123844, gnorm=1.269, loss_scale=4, train_wall=23, gb_free=10.9, wall=19990
2021-05-05 22:10:40 | INFO | train_inner | epoch 002:   4887 / 60421 loss=1.081, ppl=2.12, wps=15877.1, ups=4.22, wpb=3760.3, bsz=145.8, num_updates=65300, lr=0.000123749, gnorm=1.169, loss_scale=4, train_wall=23, gb_free=11, wall=20013
2021-05-05 22:11:03 | INFO | train_inner | epoch 002:   4987 / 60421 loss=1.139, ppl=2.2, wps=15801.4, ups=4.21, wpb=3751.6, bsz=136, num_updates=65400, lr=0.000123655, gnorm=1.325, loss_scale=4, train_wall=24, gb_free=10.8, wall=20037
2021-05-05 22:11:26 | INFO | train_inner | epoch 002:   5087 / 60421 loss=1.136, ppl=2.2, wps=16231.9, ups=4.31, wpb=3763.8, bsz=135.4, num_updates=65500, lr=0.00012356, gnorm=1.285, loss_scale=4, train_wall=23, gb_free=10.8, wall=20060
2021-05-05 22:11:49 | INFO | train_inner | epoch 002:   5187 / 60421 loss=1.143, ppl=2.21, wps=16360.5, ups=4.38, wpb=3733.8, bsz=128.5, num_updates=65600, lr=0.000123466, gnorm=1.619, loss_scale=4, train_wall=23, gb_free=10.9, wall=20083
2021-05-05 22:12:12 | INFO | train_inner | epoch 002:   5287 / 60421 loss=1.198, ppl=2.29, wps=16682.4, ups=4.37, wpb=3821.8, bsz=142.4, num_updates=65700, lr=0.000123372, gnorm=1.438, loss_scale=4, train_wall=23, gb_free=10.8, wall=20106
2021-05-05 22:12:35 | INFO | train_inner | epoch 002:   5387 / 60421 loss=1.154, ppl=2.22, wps=16452.1, ups=4.45, wpb=3700.8, bsz=149, num_updates=65800, lr=0.000123278, gnorm=1.595, loss_scale=4, train_wall=22, gb_free=10.8, wall=20129
2021-05-05 22:12:58 | INFO | train_inner | epoch 002:   5487 / 60421 loss=1.193, ppl=2.29, wps=16341.9, ups=4.38, wpb=3734.1, bsz=136.3, num_updates=65900, lr=0.000123185, gnorm=1.561, loss_scale=4, train_wall=23, gb_free=10.7, wall=20151
2021-05-05 22:13:21 | INFO | train_inner | epoch 002:   5587 / 60421 loss=1.135, ppl=2.2, wps=16481.3, ups=4.35, wpb=3785.7, bsz=135.3, num_updates=66000, lr=0.000123091, gnorm=1.309, loss_scale=4, train_wall=23, gb_free=10.8, wall=20174
2021-05-05 22:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:13:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:13:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:13:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:13:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:13:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:13:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:13:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:13:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:13:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:13:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:13:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:13:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:13:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:13:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:13:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:14:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.328 | ppl 20.08 | bleu 28.17 | wps 2476.8 | wpb 2024.1 | bsz 97.5 | num_updates 66000 | best_bleu 28.3
2021-05-05 22:14:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 66000 updates
2021-05-05 22:14:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_66000.pt
2021-05-05 22:14:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_66000.pt
2021-05-05 22:14:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_66000.pt (epoch 2 @ 66000 updates, score 28.17) (writing took 8.541700739006046 seconds)
2021-05-05 22:14:56 | INFO | train_inner | epoch 002:   5687 / 60421 loss=1.126, ppl=2.18, wps=3915.1, ups=1.04, wpb=3751.9, bsz=129.9, num_updates=66100, lr=0.000122998, gnorm=1.314, loss_scale=4, train_wall=23, gb_free=10.8, wall=20270
2021-05-05 22:15:20 | INFO | train_inner | epoch 002:   5787 / 60421 loss=1.088, ppl=2.13, wps=15936.8, ups=4.31, wpb=3697.2, bsz=141.4, num_updates=66200, lr=0.000122905, gnorm=1.374, loss_scale=4, train_wall=23, gb_free=10.7, wall=20293
2021-05-05 22:15:43 | INFO | train_inner | epoch 002:   5887 / 60421 loss=1.214, ppl=2.32, wps=15789.8, ups=4.31, wpb=3666.6, bsz=123.8, num_updates=66300, lr=0.000122813, gnorm=1.786, loss_scale=4, train_wall=23, gb_free=10.7, wall=20317
2021-05-05 22:16:07 | INFO | train_inner | epoch 002:   5987 / 60421 loss=1.163, ppl=2.24, wps=15894.4, ups=4.21, wpb=3777.5, bsz=120.1, num_updates=66400, lr=0.00012272, gnorm=1.408, loss_scale=4, train_wall=24, gb_free=10.7, wall=20340
2021-05-05 22:16:30 | INFO | train_inner | epoch 002:   6087 / 60421 loss=1.138, ppl=2.2, wps=16235.4, ups=4.36, wpb=3724.6, bsz=136, num_updates=66500, lr=0.000122628, gnorm=1.369, loss_scale=4, train_wall=23, gb_free=10.8, wall=20363
2021-05-05 22:16:52 | INFO | train_inner | epoch 002:   6187 / 60421 loss=1.163, ppl=2.24, wps=16389.6, ups=4.41, wpb=3720, bsz=127, num_updates=66600, lr=0.000122536, gnorm=1.43, loss_scale=4, train_wall=23, gb_free=10.7, wall=20386
2021-05-05 22:17:15 | INFO | train_inner | epoch 002:   6287 / 60421 loss=1.087, ppl=2.12, wps=16485.4, ups=4.38, wpb=3767.4, bsz=132.3, num_updates=66700, lr=0.000122444, gnorm=1.291, loss_scale=4, train_wall=23, gb_free=10.9, wall=20409
2021-05-05 22:17:38 | INFO | train_inner | epoch 002:   6387 / 60421 loss=1.13, ppl=2.19, wps=16506.6, ups=4.37, wpb=3775.2, bsz=133.6, num_updates=66800, lr=0.000122352, gnorm=1.294, loss_scale=4, train_wall=23, gb_free=10.9, wall=20432
2021-05-05 22:18:01 | INFO | train_inner | epoch 002:   6487 / 60421 loss=1.147, ppl=2.21, wps=16587.8, ups=4.36, wpb=3804.8, bsz=150.2, num_updates=66900, lr=0.000122261, gnorm=1.379, loss_scale=4, train_wall=23, gb_free=10.8, wall=20455
2021-05-05 22:18:23 | INFO | train_inner | epoch 002:   6587 / 60421 loss=1.143, ppl=2.21, wps=16209.9, ups=4.45, wpb=3646.7, bsz=122.6, num_updates=67000, lr=0.000122169, gnorm=1.662, loss_scale=4, train_wall=22, gb_free=10.7, wall=20477
2021-05-05 22:18:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:18:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:18:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:18:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:18:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:18:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:18:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:18:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:18:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:18:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:18:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:18:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:18:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:19:28 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.406 | ppl 21.21 | bleu 28.2 | wps 2454.1 | wpb 2024.1 | bsz 97.5 | num_updates 67000 | best_bleu 28.3
2021-05-05 22:19:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 67000 updates
2021-05-05 22:19:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_67000.pt
2021-05-05 22:19:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_67000.pt
2021-05-05 22:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_67000.pt (epoch 2 @ 67000 updates, score 28.2) (writing took 7.912293530986062 seconds)
2021-05-05 22:19:59 | INFO | train_inner | epoch 002:   6687 / 60421 loss=1.228, ppl=2.34, wps=3940, ups=1.04, wpb=3781.3, bsz=123, num_updates=67100, lr=0.000122078, gnorm=1.456, loss_scale=4, train_wall=23, gb_free=10.7, wall=20573
2021-05-05 22:20:23 | INFO | train_inner | epoch 002:   6787 / 60421 loss=1.158, ppl=2.23, wps=15886.3, ups=4.28, wpb=3711.3, bsz=125.3, num_updates=67200, lr=0.000121988, gnorm=1.518, loss_scale=4, train_wall=23, gb_free=10.8, wall=20597
2021-05-05 22:20:46 | INFO | train_inner | epoch 002:   6887 / 60421 loss=1.135, ppl=2.2, wps=15819, ups=4.26, wpb=3713.3, bsz=129.9, num_updates=67300, lr=0.000121897, gnorm=1.545, loss_scale=4, train_wall=23, gb_free=10.9, wall=20620
2021-05-05 22:21:10 | INFO | train_inner | epoch 002:   6987 / 60421 loss=1.112, ppl=2.16, wps=15760.7, ups=4.26, wpb=3697.9, bsz=143.3, num_updates=67400, lr=0.000121806, gnorm=1.466, loss_scale=4, train_wall=23, gb_free=10.9, wall=20644
2021-05-05 22:21:33 | INFO | train_inner | epoch 002:   7087 / 60421 loss=1.164, ppl=2.24, wps=16216.6, ups=4.31, wpb=3763.3, bsz=141.8, num_updates=67500, lr=0.000121716, gnorm=1.528, loss_scale=4, train_wall=23, gb_free=10.6, wall=20667
2021-05-05 22:21:56 | INFO | train_inner | epoch 002:   7187 / 60421 loss=1.148, ppl=2.22, wps=16420.4, ups=4.37, wpb=3754.5, bsz=138.9, num_updates=67600, lr=0.000121626, gnorm=1.565, loss_scale=4, train_wall=23, gb_free=10.9, wall=20690
2021-05-05 22:22:18 | INFO | train_inner | epoch 002:   7287 / 60421 loss=1.185, ppl=2.27, wps=16357.4, ups=4.39, wpb=3722.1, bsz=122.9, num_updates=67700, lr=0.000121536, gnorm=1.508, loss_scale=4, train_wall=23, gb_free=11.3, wall=20712
2021-05-05 22:22:41 | INFO | train_inner | epoch 002:   7387 / 60421 loss=1.122, ppl=2.18, wps=16266.9, ups=4.44, wpb=3661.8, bsz=141.4, num_updates=67800, lr=0.000121447, gnorm=1.65, loss_scale=4, train_wall=22, gb_free=10.8, wall=20735
2021-05-05 22:23:04 | INFO | train_inner | epoch 002:   7487 / 60421 loss=1.121, ppl=2.17, wps=16435.5, ups=4.36, wpb=3772.3, bsz=135.6, num_updates=67900, lr=0.000121357, gnorm=1.257, loss_scale=4, train_wall=23, gb_free=10.7, wall=20758
2021-05-05 22:23:27 | INFO | train_inner | epoch 002:   7587 / 60421 loss=1.14, ppl=2.2, wps=16378, ups=4.32, wpb=3789.1, bsz=131.6, num_updates=68000, lr=0.000121268, gnorm=1.249, loss_scale=4, train_wall=23, gb_free=10.9, wall=20781
2021-05-05 22:23:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:23:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:23:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:23:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:23:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:23:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:23:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:24:32 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.385 | ppl 20.9 | bleu 28.48 | wps 2441.2 | wpb 2024.1 | bsz 97.5 | num_updates 68000 | best_bleu 28.48
2021-05-05 22:24:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 68000 updates
2021-05-05 22:24:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_68000.pt
2021-05-05 22:24:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_68000.pt
2021-05-05 22:24:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_68000.pt (epoch 2 @ 68000 updates, score 28.48) (writing took 14.613375084998552 seconds)
2021-05-05 22:25:10 | INFO | train_inner | epoch 002:   7687 / 60421 loss=1.17, ppl=2.25, wps=3620.9, ups=0.97, wpb=3718.8, bsz=135.5, num_updates=68100, lr=0.000121179, gnorm=1.675, loss_scale=4, train_wall=23, gb_free=10.9, wall=20884
2021-05-05 22:25:33 | INFO | train_inner | epoch 002:   7787 / 60421 loss=1.147, ppl=2.21, wps=15985.4, ups=4.26, wpb=3748.4, bsz=132.7, num_updates=68200, lr=0.00012109, gnorm=1.597, loss_scale=4, train_wall=23, gb_free=10.8, wall=20907
2021-05-05 22:25:57 | INFO | train_inner | epoch 002:   7887 / 60421 loss=1.102, ppl=2.15, wps=15862.2, ups=4.19, wpb=3787.7, bsz=153.8, num_updates=68300, lr=0.000121001, gnorm=1.289, loss_scale=4, train_wall=24, gb_free=10.9, wall=20931
2021-05-05 22:26:20 | INFO | train_inner | epoch 002:   7987 / 60421 loss=1.156, ppl=2.23, wps=16269.3, ups=4.3, wpb=3785.8, bsz=133.8, num_updates=68400, lr=0.000120913, gnorm=1.453, loss_scale=4, train_wall=23, gb_free=11, wall=20954
2021-05-05 22:26:43 | INFO | train_inner | epoch 002:   8087 / 60421 loss=1.032, ppl=2.04, wps=16219, ups=4.36, wpb=3722.8, bsz=142.2, num_updates=68500, lr=0.000120824, gnorm=1.241, loss_scale=4, train_wall=23, gb_free=10.7, wall=20977
2021-05-05 22:27:06 | INFO | train_inner | epoch 002:   8187 / 60421 loss=1.107, ppl=2.15, wps=16561, ups=4.36, wpb=3799.2, bsz=145.3, num_updates=68600, lr=0.000120736, gnorm=1.216, loss_scale=4, train_wall=23, gb_free=10.9, wall=21000
2021-05-05 22:27:29 | INFO | train_inner | epoch 002:   8287 / 60421 loss=1.146, ppl=2.21, wps=16353.1, ups=4.43, wpb=3693.3, bsz=131.3, num_updates=68700, lr=0.000120648, gnorm=1.44, loss_scale=4, train_wall=22, gb_free=10.8, wall=21023
2021-05-05 22:27:52 | INFO | train_inner | epoch 002:   8387 / 60421 loss=1.161, ppl=2.24, wps=16329.6, ups=4.37, wpb=3740.4, bsz=127, num_updates=68800, lr=0.000120561, gnorm=1.594, loss_scale=4, train_wall=23, gb_free=11.2, wall=21046
2021-05-05 22:28:15 | INFO | train_inner | epoch 002:   8487 / 60421 loss=1.164, ppl=2.24, wps=16310.7, ups=4.38, wpb=3725.2, bsz=118.9, num_updates=68900, lr=0.000120473, gnorm=1.49, loss_scale=4, train_wall=23, gb_free=10.8, wall=21068
2021-05-05 22:28:38 | INFO | train_inner | epoch 002:   8587 / 60421 loss=1.079, ppl=2.11, wps=16515.2, ups=4.31, wpb=3829, bsz=150.6, num_updates=69000, lr=0.000120386, gnorm=1.147, loss_scale=4, train_wall=23, gb_free=10.9, wall=21092
2021-05-05 22:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:28:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:28:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:28:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:28:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:28:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:28:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:28:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:28:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:28:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:28:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:28:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:28:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:29:42 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.365 | ppl 20.6 | bleu 28.5 | wps 2472.5 | wpb 2024.1 | bsz 97.5 | num_updates 69000 | best_bleu 28.5
2021-05-05 22:29:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 69000 updates
2021-05-05 22:29:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_69000.pt
2021-05-05 22:29:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_69000.pt
2021-05-05 22:29:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_69000.pt (epoch 2 @ 69000 updates, score 28.5) (writing took 14.814598056997056 seconds)
2021-05-05 22:30:20 | INFO | train_inner | epoch 002:   8687 / 60421 loss=1.127, ppl=2.18, wps=3612.7, ups=0.98, wpb=3692, bsz=139.3, num_updates=69100, lr=0.000120299, gnorm=1.393, loss_scale=4, train_wall=23, gb_free=11, wall=21194
2021-05-05 22:30:44 | INFO | train_inner | epoch 002:   8787 / 60421 loss=1.104, ppl=2.15, wps=15947.3, ups=4.22, wpb=3780.9, bsz=141.8, num_updates=69200, lr=0.000120212, gnorm=1.418, loss_scale=4, train_wall=24, gb_free=10.7, wall=21218
2021-05-05 22:31:07 | INFO | train_inner | epoch 002:   8887 / 60421 loss=1.187, ppl=2.28, wps=15955.8, ups=4.23, wpb=3767.9, bsz=129.1, num_updates=69300, lr=0.000120125, gnorm=1.606, loss_scale=4, train_wall=23, gb_free=10.8, wall=21241
2021-05-05 22:31:30 | INFO | train_inner | epoch 002:   8987 / 60421 loss=1.058, ppl=2.08, wps=16137.4, ups=4.38, wpb=3686.5, bsz=135.8, num_updates=69400, lr=0.000120038, gnorm=1.49, loss_scale=4, train_wall=23, gb_free=10.8, wall=21264
2021-05-05 22:31:53 | INFO | train_inner | epoch 002:   9087 / 60421 loss=1.065, ppl=2.09, wps=16402.1, ups=4.39, wpb=3738.8, bsz=161, num_updates=69500, lr=0.000119952, gnorm=1.234, loss_scale=4, train_wall=23, gb_free=11.2, wall=21287
2021-05-05 22:32:16 | INFO | train_inner | epoch 002:   9187 / 60421 loss=1.143, ppl=2.21, wps=16466.8, ups=4.4, wpb=3744.3, bsz=136.5, num_updates=69600, lr=0.000119866, gnorm=1.462, loss_scale=4, train_wall=23, gb_free=10.8, wall=21310
2021-05-05 22:32:38 | INFO | train_inner | epoch 002:   9287 / 60421 loss=1.14, ppl=2.2, wps=16361.8, ups=4.43, wpb=3697.1, bsz=136.2, num_updates=69700, lr=0.00011978, gnorm=1.605, loss_scale=4, train_wall=22, gb_free=10.9, wall=21332
2021-05-05 22:33:01 | INFO | train_inner | epoch 002:   9387 / 60421 loss=1.135, ppl=2.2, wps=16492.8, ups=4.38, wpb=3761.8, bsz=129.8, num_updates=69800, lr=0.000119694, gnorm=1.31, loss_scale=4, train_wall=23, gb_free=10.8, wall=21355
2021-05-05 22:33:24 | INFO | train_inner | epoch 002:   9487 / 60421 loss=1.177, ppl=2.26, wps=16302.9, ups=4.36, wpb=3735.6, bsz=128.8, num_updates=69900, lr=0.000119608, gnorm=1.57, loss_scale=4, train_wall=23, gb_free=10.7, wall=21378
2021-05-05 22:33:47 | INFO | train_inner | epoch 002:   9587 / 60421 loss=1.142, ppl=2.21, wps=16433.2, ups=4.36, wpb=3770.9, bsz=127.8, num_updates=70000, lr=0.000119523, gnorm=1.322, loss_scale=4, train_wall=23, gb_free=10.9, wall=21401
2021-05-05 22:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:34:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:34:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:34:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:34:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:34:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:34:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:34:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:34:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:34:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:34:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:34:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:34:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:34:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.424 | ppl 21.47 | bleu 28.5 | wps 2382.6 | wpb 2024.1 | bsz 97.5 | num_updates 70000 | best_bleu 28.5
2021-05-05 22:34:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 70000 updates
2021-05-05 22:34:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_70000.pt
2021-05-05 22:34:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_70000.pt
2021-05-05 22:35:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_70000.pt (epoch 2 @ 70000 updates, score 28.5) (writing took 14.630304406993673 seconds)
2021-05-05 22:35:31 | INFO | train_inner | epoch 002:   9687 / 60421 loss=1.224, ppl=2.34, wps=3553.4, ups=0.96, wpb=3710.8, bsz=114.2, num_updates=70100, lr=0.000119438, gnorm=1.877, loss_scale=4, train_wall=23, gb_free=10.9, wall=21505
2021-05-05 22:35:55 | INFO | train_inner | epoch 002:   9787 / 60421 loss=1.101, ppl=2.15, wps=15986.3, ups=4.22, wpb=3790.5, bsz=150.6, num_updates=70200, lr=0.000119352, gnorm=1.381, loss_scale=4, train_wall=24, gb_free=10.8, wall=21529
2021-05-05 22:36:18 | INFO | train_inner | epoch 002:   9887 / 60421 loss=1.183, ppl=2.27, wps=16316.1, ups=4.32, wpb=3777.1, bsz=125.4, num_updates=70300, lr=0.000119268, gnorm=1.485, loss_scale=4, train_wall=23, gb_free=10.7, wall=21552
2021-05-05 22:36:41 | INFO | train_inner | epoch 002:   9987 / 60421 loss=1.111, ppl=2.16, wps=16463.1, ups=4.34, wpb=3795.1, bsz=141.6, num_updates=70400, lr=0.000119183, gnorm=1.287, loss_scale=4, train_wall=23, gb_free=10.7, wall=21575
2021-05-05 22:37:04 | INFO | train_inner | epoch 002:  10087 / 60421 loss=1.13, ppl=2.19, wps=16340.6, ups=4.38, wpb=3731.3, bsz=124.3, num_updates=70500, lr=0.000119098, gnorm=1.478, loss_scale=4, train_wall=23, gb_free=10.9, wall=21598
2021-05-05 22:37:27 | INFO | train_inner | epoch 002:  10187 / 60421 loss=1.118, ppl=2.17, wps=16478.3, ups=4.4, wpb=3744.4, bsz=135.1, num_updates=70600, lr=0.000119014, gnorm=1.275, loss_scale=4, train_wall=23, gb_free=11, wall=21621
2021-05-05 22:37:50 | INFO | train_inner | epoch 002:  10287 / 60421 loss=1.073, ppl=2.1, wps=16410, ups=4.41, wpb=3723.6, bsz=138.7, num_updates=70700, lr=0.00011893, gnorm=1.308, loss_scale=4, train_wall=23, gb_free=10.7, wall=21643
2021-05-05 22:38:12 | INFO | train_inner | epoch 002:  10387 / 60421 loss=1.105, ppl=2.15, wps=16424.6, ups=4.36, wpb=3763.8, bsz=146.3, num_updates=70800, lr=0.000118846, gnorm=1.404, loss_scale=4, train_wall=23, gb_free=10.6, wall=21666
2021-05-05 22:38:35 | INFO | train_inner | epoch 002:  10487 / 60421 loss=1.136, ppl=2.2, wps=16409, ups=4.39, wpb=3738.8, bsz=137.8, num_updates=70900, lr=0.000118762, gnorm=1.369, loss_scale=4, train_wall=23, gb_free=10.9, wall=21689
2021-05-05 22:38:58 | INFO | train_inner | epoch 002:  10587 / 60421 loss=1.124, ppl=2.18, wps=16266.7, ups=4.37, wpb=3722.7, bsz=124.9, num_updates=71000, lr=0.000118678, gnorm=1.447, loss_scale=4, train_wall=23, gb_free=10.6, wall=21712
2021-05-05 22:38:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:39:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:39:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:39:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:39:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:39:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:39:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:39:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:39:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:39:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:39:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:39:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:39:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:40:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.446 | ppl 21.79 | bleu 28.55 | wps 2445.3 | wpb 2024.1 | bsz 97.5 | num_updates 71000 | best_bleu 28.55
2021-05-05 22:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 71000 updates
2021-05-05 22:40:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_71000.pt
2021-05-05 22:40:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_71000.pt
2021-05-05 22:40:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_71000.pt (epoch 2 @ 71000 updates, score 28.55) (writing took 14.625151455999003 seconds)
2021-05-05 22:40:41 | INFO | train_inner | epoch 002:  10687 / 60421 loss=1.13, ppl=2.19, wps=3622.5, ups=0.97, wpb=3729.5, bsz=131.5, num_updates=71100, lr=0.000118595, gnorm=1.44, loss_scale=4, train_wall=23, gb_free=10.8, wall=21815
2021-05-05 22:41:05 | INFO | train_inner | epoch 002:  10787 / 60421 loss=1.084, ppl=2.12, wps=15843.3, ups=4.26, wpb=3718.4, bsz=136.9, num_updates=71200, lr=0.000118511, gnorm=1.281, loss_scale=4, train_wall=23, gb_free=10.9, wall=21838
2021-05-05 22:41:28 | INFO | train_inner | epoch 002:  10887 / 60421 loss=1.201, ppl=2.3, wps=16162.3, ups=4.35, wpb=3716.1, bsz=138.2, num_updates=71300, lr=0.000118428, gnorm=1.55, loss_scale=4, train_wall=23, gb_free=11.1, wall=21861
2021-05-05 22:41:51 | INFO | train_inner | epoch 002:  10987 / 60421 loss=1.092, ppl=2.13, wps=16345.9, ups=4.34, wpb=3770.6, bsz=144.3, num_updates=71400, lr=0.000118345, gnorm=1.393, loss_scale=4, train_wall=23, gb_free=10.8, wall=21885
2021-05-05 22:42:13 | INFO | train_inner | epoch 002:  11087 / 60421 loss=1.155, ppl=2.23, wps=16471.2, ups=4.38, wpb=3757.8, bsz=142.6, num_updates=71500, lr=0.000118262, gnorm=1.597, loss_scale=4, train_wall=23, gb_free=10.9, wall=21907
2021-05-05 22:42:36 | INFO | train_inner | epoch 002:  11187 / 60421 loss=1.162, ppl=2.24, wps=16349, ups=4.38, wpb=3730.8, bsz=126.1, num_updates=71600, lr=0.00011818, gnorm=1.525, loss_scale=4, train_wall=23, gb_free=10.7, wall=21930
2021-05-05 22:42:59 | INFO | train_inner | epoch 002:  11287 / 60421 loss=1.098, ppl=2.14, wps=16458.7, ups=4.42, wpb=3724.7, bsz=129.8, num_updates=71700, lr=0.000118097, gnorm=1.47, loss_scale=4, train_wall=22, gb_free=11.1, wall=21953
2021-05-05 22:43:22 | INFO | train_inner | epoch 002:  11387 / 60421 loss=1.229, ppl=2.34, wps=16589.2, ups=4.4, wpb=3770.8, bsz=114.8, num_updates=71800, lr=0.000118015, gnorm=1.472, loss_scale=4, train_wall=23, gb_free=10.9, wall=21976
2021-05-05 22:43:44 | INFO | train_inner | epoch 002:  11487 / 60421 loss=1.286, ppl=2.44, wps=16381, ups=4.38, wpb=3740, bsz=141, num_updates=71900, lr=0.000117933, gnorm=1.889, loss_scale=4, train_wall=23, gb_free=11, wall=21998
2021-05-05 22:44:08 | INFO | train_inner | epoch 002:  11587 / 60421 loss=1.102, ppl=2.15, wps=16263.2, ups=4.33, wpb=3754.7, bsz=127.3, num_updates=72000, lr=0.000117851, gnorm=1.305, loss_scale=4, train_wall=23, gb_free=10.6, wall=22021
2021-05-05 22:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:44:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:44:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:44:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:44:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:44:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:44:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:44:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:44:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:44:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:44:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:44:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:44:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:45:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.43 | ppl 21.56 | bleu 28.6 | wps 2440.3 | wpb 2024.1 | bsz 97.5 | num_updates 72000 | best_bleu 28.6
2021-05-05 22:45:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 72000 updates
2021-05-05 22:45:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_72000.pt
2021-05-05 22:45:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_72000.pt
2021-05-05 22:45:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_72000.pt (epoch 2 @ 72000 updates, score 28.6) (writing took 14.597334220001358 seconds)
2021-05-05 22:45:51 | INFO | train_inner | epoch 002:  11687 / 60421 loss=1.117, ppl=2.17, wps=3617.3, ups=0.97, wpb=3732.6, bsz=148.2, num_updates=72100, lr=0.000117769, gnorm=1.517, loss_scale=4, train_wall=23, gb_free=10.9, wall=22125
2021-05-05 22:46:14 | INFO | train_inner | epoch 002:  11787 / 60421 loss=1.113, ppl=2.16, wps=16216.7, ups=4.26, wpb=3803.5, bsz=127.6, num_updates=72200, lr=0.000117688, gnorm=1.291, loss_scale=4, train_wall=23, gb_free=10.9, wall=22148
2021-05-05 22:46:37 | INFO | train_inner | epoch 002:  11887 / 60421 loss=1.182, ppl=2.27, wps=16369.7, ups=4.37, wpb=3745.1, bsz=129.3, num_updates=72300, lr=0.000117606, gnorm=1.571, loss_scale=4, train_wall=23, gb_free=10.9, wall=22171
2021-05-05 22:47:00 | INFO | train_inner | epoch 002:  11987 / 60421 loss=1.138, ppl=2.2, wps=16442.5, ups=4.37, wpb=3763.4, bsz=123.1, num_updates=72400, lr=0.000117525, gnorm=1.51, loss_scale=4, train_wall=23, gb_free=10.7, wall=22194
2021-05-05 22:47:23 | INFO | train_inner | epoch 002:  12087 / 60421 loss=1.15, ppl=2.22, wps=16463.4, ups=4.41, wpb=3729.4, bsz=141.4, num_updates=72500, lr=0.000117444, gnorm=1.734, loss_scale=4, train_wall=22, gb_free=10.9, wall=22216
2021-05-05 22:47:46 | INFO | train_inner | epoch 002:  12187 / 60421 loss=1.13, ppl=2.19, wps=16553, ups=4.36, wpb=3799, bsz=137.9, num_updates=72600, lr=0.000117363, gnorm=1.286, loss_scale=4, train_wall=23, gb_free=10.9, wall=22239
2021-05-05 22:48:08 | INFO | train_inner | epoch 002:  12287 / 60421 loss=1.151, ppl=2.22, wps=16385.2, ups=4.41, wpb=3715.1, bsz=122.2, num_updates=72700, lr=0.000117282, gnorm=1.498, loss_scale=4, train_wall=22, gb_free=10.9, wall=22262
2021-05-05 22:48:31 | INFO | train_inner | epoch 002:  12387 / 60421 loss=1.138, ppl=2.2, wps=16434.1, ups=4.36, wpb=3772.9, bsz=123.8, num_updates=72800, lr=0.000117202, gnorm=1.368, loss_scale=4, train_wall=23, gb_free=10.6, wall=22285
2021-05-05 22:48:54 | INFO | train_inner | epoch 002:  12487 / 60421 loss=1.116, ppl=2.17, wps=16279.2, ups=4.39, wpb=3710.8, bsz=120.4, num_updates=72900, lr=0.000117121, gnorm=1.373, loss_scale=4, train_wall=23, gb_free=10.9, wall=22308
2021-05-05 22:49:17 | INFO | train_inner | epoch 002:  12587 / 60421 loss=1.17, ppl=2.25, wps=16156, ups=4.36, wpb=3708.4, bsz=132.7, num_updates=73000, lr=0.000117041, gnorm=1.705, loss_scale=4, train_wall=23, gb_free=10.8, wall=22331
2021-05-05 22:49:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:49:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:49:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:49:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:49:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:49:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:49:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:49:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:49:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:49:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:49:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:49:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:49:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:50:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.446 | ppl 21.8 | bleu 28.26 | wps 2458.2 | wpb 2024.1 | bsz 97.5 | num_updates 73000 | best_bleu 28.6
2021-05-05 22:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 73000 updates
2021-05-05 22:50:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_73000.pt
2021-05-05 22:50:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_73000.pt
2021-05-05 22:50:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_73000.pt (epoch 2 @ 73000 updates, score 28.26) (writing took 8.422527125992929 seconds)
2021-05-05 22:50:53 | INFO | train_inner | epoch 002:  12687 / 60421 loss=1.123, ppl=2.18, wps=3850.4, ups=1.04, wpb=3714.9, bsz=131.4, num_updates=73100, lr=0.000116961, gnorm=1.571, loss_scale=4, train_wall=23, gb_free=10.8, wall=22427
2021-05-05 22:51:17 | INFO | train_inner | epoch 002:  12787 / 60421 loss=1.161, ppl=2.24, wps=16195.6, ups=4.28, wpb=3781.2, bsz=134.1, num_updates=73200, lr=0.000116881, gnorm=1.417, loss_scale=4, train_wall=23, gb_free=10.8, wall=22451
2021-05-05 22:51:40 | INFO | train_inner | epoch 002:  12887 / 60421 loss=1.166, ppl=2.24, wps=16412.7, ups=4.34, wpb=3785.1, bsz=129.3, num_updates=73300, lr=0.000116801, gnorm=1.382, loss_scale=4, train_wall=23, gb_free=10.9, wall=22474
2021-05-05 22:52:03 | INFO | train_inner | epoch 002:  12987 / 60421 loss=1.15, ppl=2.22, wps=16554, ups=4.36, wpb=3799.4, bsz=141.3, num_updates=73400, lr=0.000116722, gnorm=1.353, loss_scale=4, train_wall=23, gb_free=10.8, wall=22497
2021-05-05 22:52:25 | INFO | train_inner | epoch 002:  13087 / 60421 loss=1.093, ppl=2.13, wps=16349.5, ups=4.42, wpb=3695.1, bsz=132.3, num_updates=73500, lr=0.000116642, gnorm=1.55, loss_scale=4, train_wall=22, gb_free=10.8, wall=22519
2021-05-05 22:52:48 | INFO | train_inner | epoch 002:  13187 / 60421 loss=1.178, ppl=2.26, wps=16363.1, ups=4.4, wpb=3723.1, bsz=113.7, num_updates=73600, lr=0.000116563, gnorm=1.613, loss_scale=8, train_wall=23, gb_free=10.8, wall=22542
2021-05-05 22:53:11 | INFO | train_inner | epoch 002:  13287 / 60421 loss=1.179, ppl=2.26, wps=16224.3, ups=4.33, wpb=3749, bsz=116.3, num_updates=73700, lr=0.000116484, gnorm=1.479, loss_scale=8, train_wall=23, gb_free=10.7, wall=22565
2021-05-05 22:53:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-05 22:53:34 | INFO | train_inner | epoch 002:  13388 / 60421 loss=1.153, ppl=2.22, wps=16174.2, ups=4.34, wpb=3728.1, bsz=132.4, num_updates=73800, lr=0.000116405, gnorm=1.439, loss_scale=4, train_wall=23, gb_free=10.7, wall=22588
2021-05-05 22:53:58 | INFO | train_inner | epoch 002:  13488 / 60421 loss=1.133, ppl=2.19, wps=16308.5, ups=4.29, wpb=3804.6, bsz=138.2, num_updates=73900, lr=0.000116326, gnorm=1.198, loss_scale=4, train_wall=23, gb_free=10.7, wall=22612
2021-05-05 22:54:21 | INFO | train_inner | epoch 002:  13588 / 60421 loss=1.125, ppl=2.18, wps=16160.6, ups=4.33, wpb=3734.5, bsz=152, num_updates=74000, lr=0.000116248, gnorm=1.404, loss_scale=4, train_wall=23, gb_free=10.8, wall=22635
2021-05-05 22:54:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:54:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:54:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:54:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:54:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:54:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:54:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:54:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:54:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:54:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:55:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.435 | ppl 21.64 | bleu 28.78 | wps 2450.2 | wpb 2024.1 | bsz 97.5 | num_updates 74000 | best_bleu 28.78
2021-05-05 22:55:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 74000 updates
2021-05-05 22:55:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_74000.pt
2021-05-05 22:55:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_74000.pt
2021-05-05 22:55:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_74000.pt (epoch 2 @ 74000 updates, score 28.78) (writing took 14.606457162997685 seconds)
2021-05-05 22:56:04 | INFO | train_inner | epoch 002:  13688 / 60421 loss=1.11, ppl=2.16, wps=3606.3, ups=0.97, wpb=3709.3, bsz=120.7, num_updates=74100, lr=0.000116169, gnorm=1.334, loss_scale=4, train_wall=23, gb_free=10.9, wall=22737
2021-05-05 22:56:27 | INFO | train_inner | epoch 002:  13788 / 60421 loss=1.147, ppl=2.21, wps=16364.7, ups=4.34, wpb=3767, bsz=115.2, num_updates=74200, lr=0.000116091, gnorm=1.209, loss_scale=4, train_wall=23, gb_free=10.7, wall=22761
2021-05-05 22:56:49 | INFO | train_inner | epoch 002:  13888 / 60421 loss=1.153, ppl=2.22, wps=16394.1, ups=4.38, wpb=3742.2, bsz=121.7, num_updates=74300, lr=0.000116013, gnorm=1.406, loss_scale=4, train_wall=23, gb_free=10.9, wall=22783
2021-05-05 22:57:12 | INFO | train_inner | epoch 002:  13988 / 60421 loss=1.156, ppl=2.23, wps=16301.4, ups=4.42, wpb=3687.7, bsz=122.2, num_updates=74400, lr=0.000115935, gnorm=1.67, loss_scale=4, train_wall=22, gb_free=10.8, wall=22806
2021-05-05 22:57:35 | INFO | train_inner | epoch 002:  14088 / 60421 loss=1.167, ppl=2.25, wps=16354.6, ups=4.37, wpb=3742.3, bsz=123.8, num_updates=74500, lr=0.000115857, gnorm=1.557, loss_scale=4, train_wall=23, gb_free=10.8, wall=22829
2021-05-05 22:57:58 | INFO | train_inner | epoch 002:  14188 / 60421 loss=1.164, ppl=2.24, wps=16339.2, ups=4.4, wpb=3716.1, bsz=147, num_updates=74600, lr=0.000115779, gnorm=1.657, loss_scale=4, train_wall=23, gb_free=10.8, wall=22852
2021-05-05 22:58:20 | INFO | train_inner | epoch 002:  14288 / 60421 loss=1.112, ppl=2.16, wps=16225.3, ups=4.42, wpb=3669.5, bsz=128.3, num_updates=74700, lr=0.000115702, gnorm=1.635, loss_scale=4, train_wall=22, gb_free=10.8, wall=22874
2021-05-05 22:58:43 | INFO | train_inner | epoch 002:  14388 / 60421 loss=1.076, ppl=2.11, wps=16143.8, ups=4.39, wpb=3675.9, bsz=145, num_updates=74800, lr=0.000115624, gnorm=1.639, loss_scale=4, train_wall=23, gb_free=10.8, wall=22897
2021-05-05 22:59:06 | INFO | train_inner | epoch 002:  14488 / 60421 loss=1.112, ppl=2.16, wps=16362.6, ups=4.32, wpb=3787.6, bsz=150.7, num_updates=74900, lr=0.000115547, gnorm=1.366, loss_scale=4, train_wall=23, gb_free=10.8, wall=22920
2021-05-05 22:59:29 | INFO | train_inner | epoch 002:  14588 / 60421 loss=1.13, ppl=2.19, wps=16261, ups=4.31, wpb=3772.6, bsz=129.7, num_updates=75000, lr=0.00011547, gnorm=1.429, loss_scale=4, train_wall=23, gb_free=10.7, wall=22943
2021-05-05 22:59:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 22:59:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:59:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:59:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:59:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:59:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:59:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:59:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:59:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:59:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:59:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:59:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:59:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 22:59:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 22:59:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 22:59:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:00:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.462 | ppl 22.04 | bleu 28 | wps 2425.3 | wpb 2024.1 | bsz 97.5 | num_updates 75000 | best_bleu 28.78
2021-05-05 23:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 75000 updates
2021-05-05 23:00:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_75000.pt
2021-05-05 23:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_75000.pt
2021-05-05 23:00:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_75000.pt (epoch 2 @ 75000 updates, score 28.0) (writing took 8.540746455997578 seconds)
2021-05-05 23:01:07 | INFO | train_inner | epoch 002:  14688 / 60421 loss=1.122, ppl=2.18, wps=3830.9, ups=1.02, wpb=3738.8, bsz=132.2, num_updates=75100, lr=0.000115393, gnorm=1.538, loss_scale=4, train_wall=23, gb_free=10.8, wall=23041
2021-05-05 23:01:30 | INFO | train_inner | epoch 002:  14788 / 60421 loss=1.132, ppl=2.19, wps=16283.4, ups=4.34, wpb=3750.3, bsz=130.7, num_updates=75200, lr=0.000115316, gnorm=1.357, loss_scale=4, train_wall=23, gb_free=10.8, wall=23064
2021-05-05 23:01:53 | INFO | train_inner | epoch 002:  14888 / 60421 loss=1.11, ppl=2.16, wps=16341.9, ups=4.43, wpb=3692.6, bsz=134.3, num_updates=75300, lr=0.00011524, gnorm=1.605, loss_scale=4, train_wall=22, gb_free=10.9, wall=23087
2021-05-05 23:02:16 | INFO | train_inner | epoch 002:  14988 / 60421 loss=1.109, ppl=2.16, wps=16544.9, ups=4.37, wpb=3785.5, bsz=144.9, num_updates=75400, lr=0.000115163, gnorm=1.303, loss_scale=4, train_wall=23, gb_free=11.1, wall=23109
2021-05-05 23:02:38 | INFO | train_inner | epoch 002:  15088 / 60421 loss=1.114, ppl=2.16, wps=16396, ups=4.4, wpb=3728.7, bsz=113.8, num_updates=75500, lr=0.000115087, gnorm=1.346, loss_scale=4, train_wall=23, gb_free=10.7, wall=23132
2021-05-05 23:03:01 | INFO | train_inner | epoch 002:  15188 / 60421 loss=1.071, ppl=2.1, wps=16391.6, ups=4.35, wpb=3768.2, bsz=153.2, num_updates=75600, lr=0.000115011, gnorm=1.208, loss_scale=4, train_wall=23, gb_free=10.8, wall=23155
2021-05-05 23:03:24 | INFO | train_inner | epoch 002:  15288 / 60421 loss=1.055, ppl=2.08, wps=16512.2, ups=4.35, wpb=3800.1, bsz=137.7, num_updates=75700, lr=0.000114935, gnorm=1.24, loss_scale=4, train_wall=23, gb_free=10.7, wall=23178
2021-05-05 23:03:47 | INFO | train_inner | epoch 002:  15388 / 60421 loss=1.09, ppl=2.13, wps=16131.9, ups=4.33, wpb=3721.6, bsz=147.1, num_updates=75800, lr=0.000114859, gnorm=1.484, loss_scale=4, train_wall=23, gb_free=10.8, wall=23201
2021-05-05 23:04:11 | INFO | train_inner | epoch 002:  15488 / 60421 loss=1.052, ppl=2.07, wps=16318.5, ups=4.31, wpb=3787.2, bsz=132.2, num_updates=75900, lr=0.000114783, gnorm=1.204, loss_scale=4, train_wall=23, gb_free=10.7, wall=23224
2021-05-05 23:04:34 | INFO | train_inner | epoch 002:  15588 / 60421 loss=1.051, ppl=2.07, wps=16159.8, ups=4.31, wpb=3747.3, bsz=151.6, num_updates=76000, lr=0.000114708, gnorm=1.523, loss_scale=4, train_wall=23, gb_free=10.8, wall=23248
2021-05-05 23:04:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:04:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:04:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:04:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:04:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:04:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:04:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:04:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:04:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:04:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:04:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:04:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:04:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:05:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.498 | ppl 22.6 | bleu 28.62 | wps 2434.1 | wpb 2024.1 | bsz 97.5 | num_updates 76000 | best_bleu 28.78
2021-05-05 23:05:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 76000 updates
2021-05-05 23:05:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_76000.pt
2021-05-05 23:05:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_76000.pt
2021-05-05 23:05:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_76000.pt (epoch 2 @ 76000 updates, score 28.62) (writing took 7.884653134009568 seconds)
2021-05-05 23:06:10 | INFO | train_inner | epoch 002:  15688 / 60421 loss=1.113, ppl=2.16, wps=3861.2, ups=1.04, wpb=3722.5, bsz=126.9, num_updates=76100, lr=0.000114632, gnorm=1.39, loss_scale=4, train_wall=23, gb_free=11.1, wall=23344
2021-05-05 23:06:33 | INFO | train_inner | epoch 002:  15788 / 60421 loss=1.125, ppl=2.18, wps=16479.4, ups=4.37, wpb=3771, bsz=124, num_updates=76200, lr=0.000114557, gnorm=1.354, loss_scale=4, train_wall=23, gb_free=10.7, wall=23367
2021-05-05 23:06:56 | INFO | train_inner | epoch 002:  15888 / 60421 loss=1.072, ppl=2.1, wps=16503.2, ups=4.34, wpb=3803.1, bsz=128.6, num_updates=76300, lr=0.000114482, gnorm=1.168, loss_scale=4, train_wall=23, gb_free=10.8, wall=23390
2021-05-05 23:07:19 | INFO | train_inner | epoch 002:  15988 / 60421 loss=1.192, ppl=2.28, wps=16484.8, ups=4.44, wpb=3716.6, bsz=140.2, num_updates=76400, lr=0.000114407, gnorm=1.8, loss_scale=4, train_wall=22, gb_free=11.3, wall=23413
2021-05-05 23:07:42 | INFO | train_inner | epoch 002:  16088 / 60421 loss=1.212, ppl=2.32, wps=16665, ups=4.32, wpb=3856.9, bsz=138.2, num_updates=76500, lr=0.000114332, gnorm=1.458, loss_scale=4, train_wall=23, gb_free=10.7, wall=23436
2021-05-05 23:08:04 | INFO | train_inner | epoch 002:  16188 / 60421 loss=1.146, ppl=2.21, wps=16307.4, ups=4.42, wpb=3691.8, bsz=118.6, num_updates=76600, lr=0.000114258, gnorm=1.535, loss_scale=4, train_wall=22, gb_free=10.8, wall=23458
2021-05-05 23:08:27 | INFO | train_inner | epoch 002:  16288 / 60421 loss=1.135, ppl=2.2, wps=16192.3, ups=4.42, wpb=3667.1, bsz=133.8, num_updates=76700, lr=0.000114183, gnorm=1.619, loss_scale=4, train_wall=22, gb_free=11, wall=23481
2021-05-05 23:08:50 | INFO | train_inner | epoch 002:  16388 / 60421 loss=1.009, ppl=2.01, wps=16184.2, ups=4.38, wpb=3698.1, bsz=132.6, num_updates=76800, lr=0.000114109, gnorm=1.328, loss_scale=4, train_wall=23, gb_free=10.9, wall=23504
2021-05-05 23:09:13 | INFO | train_inner | epoch 002:  16488 / 60421 loss=1.101, ppl=2.14, wps=16331.8, ups=4.32, wpb=3782.7, bsz=121.6, num_updates=76900, lr=0.000114035, gnorm=1.326, loss_scale=4, train_wall=23, gb_free=10.7, wall=23527
2021-05-05 23:09:36 | INFO | train_inner | epoch 002:  16588 / 60421 loss=1.059, ppl=2.08, wps=16210.1, ups=4.3, wpb=3765.5, bsz=138, num_updates=77000, lr=0.000113961, gnorm=1.245, loss_scale=4, train_wall=23, gb_free=10.7, wall=23550
2021-05-05 23:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:09:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:09:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:09:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:09:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:09:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:09:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:09:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:09:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:09:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:09:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:09:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:09:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:09:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:09:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:09:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:10:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.485 | ppl 22.39 | bleu 28.46 | wps 2455 | wpb 2024.1 | bsz 97.5 | num_updates 77000 | best_bleu 28.78
2021-05-05 23:10:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 77000 updates
2021-05-05 23:10:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_77000.pt
2021-05-05 23:10:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_77000.pt
2021-05-05 23:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_77000.pt (epoch 2 @ 77000 updates, score 28.46) (writing took 7.871630834997632 seconds)
2021-05-05 23:11:12 | INFO | train_inner | epoch 002:  16688 / 60421 loss=1.176, ppl=2.26, wps=3947.6, ups=1.04, wpb=3786, bsz=138.6, num_updates=77100, lr=0.000113887, gnorm=1.541, loss_scale=4, train_wall=23, gb_free=10.9, wall=23646
2021-05-05 23:11:35 | INFO | train_inner | epoch 002:  16788 / 60421 loss=1.016, ppl=2.02, wps=16218.4, ups=4.38, wpb=3703, bsz=135.9, num_updates=77200, lr=0.000113813, gnorm=1.359, loss_scale=4, train_wall=23, gb_free=11.3, wall=23669
2021-05-05 23:11:58 | INFO | train_inner | epoch 002:  16888 / 60421 loss=1.16, ppl=2.23, wps=16420.2, ups=4.45, wpb=3691.2, bsz=115.7, num_updates=77300, lr=0.000113739, gnorm=1.733, loss_scale=4, train_wall=22, gb_free=10.8, wall=23691
2021-05-05 23:12:20 | INFO | train_inner | epoch 002:  16988 / 60421 loss=1.11, ppl=2.16, wps=16530.7, ups=4.37, wpb=3784.3, bsz=140.5, num_updates=77400, lr=0.000113666, gnorm=1.411, loss_scale=4, train_wall=23, gb_free=10.8, wall=23714
2021-05-05 23:12:43 | INFO | train_inner | epoch 002:  17088 / 60421 loss=1.132, ppl=2.19, wps=16340.6, ups=4.38, wpb=3729.5, bsz=125.1, num_updates=77500, lr=0.000113592, gnorm=1.642, loss_scale=4, train_wall=23, gb_free=11.3, wall=23737
2021-05-05 23:13:06 | INFO | train_inner | epoch 002:  17188 / 60421 loss=1.106, ppl=2.15, wps=16479.6, ups=4.39, wpb=3755.4, bsz=129.1, num_updates=77600, lr=0.000113519, gnorm=1.254, loss_scale=4, train_wall=23, gb_free=10.7, wall=23760
2021-05-05 23:13:29 | INFO | train_inner | epoch 002:  17288 / 60421 loss=1.118, ppl=2.17, wps=16510, ups=4.42, wpb=3733.8, bsz=136.3, num_updates=77700, lr=0.000113446, gnorm=1.426, loss_scale=4, train_wall=22, gb_free=10.9, wall=23783
2021-05-05 23:13:52 | INFO | train_inner | epoch 002:  17388 / 60421 loss=1.101, ppl=2.15, wps=16265.6, ups=4.34, wpb=3748, bsz=127.7, num_updates=77800, lr=0.000113373, gnorm=1.308, loss_scale=4, train_wall=23, gb_free=11.1, wall=23806
2021-05-05 23:14:15 | INFO | train_inner | epoch 002:  17488 / 60421 loss=1.089, ppl=2.13, wps=16428, ups=4.36, wpb=3771.4, bsz=126.6, num_updates=77900, lr=0.0001133, gnorm=1.214, loss_scale=4, train_wall=23, gb_free=10.9, wall=23829
2021-05-05 23:14:38 | INFO | train_inner | epoch 002:  17588 / 60421 loss=1.075, ppl=2.11, wps=16180.9, ups=4.31, wpb=3757.2, bsz=133.4, num_updates=78000, lr=0.000113228, gnorm=1.265, loss_scale=4, train_wall=23, gb_free=11, wall=23852
2021-05-05 23:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:14:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:14:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:14:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:14:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:14:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:14:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:14:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:14:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:14:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:14:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:14:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:14:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:15:42 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.533 | ppl 23.15 | bleu 28.53 | wps 2457.2 | wpb 2024.1 | bsz 97.5 | num_updates 78000 | best_bleu 28.78
2021-05-05 23:15:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 78000 updates
2021-05-05 23:15:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_78000.pt
2021-05-05 23:15:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_78000.pt
2021-05-05 23:15:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_78000.pt (epoch 2 @ 78000 updates, score 28.53) (writing took 7.90244142600568 seconds)
2021-05-05 23:16:14 | INFO | train_inner | epoch 002:  17688 / 60421 loss=1.196, ppl=2.29, wps=3921.2, ups=1.04, wpb=3756.9, bsz=119.5, num_updates=78100, lr=0.000113155, gnorm=1.488, loss_scale=4, train_wall=23, gb_free=10.9, wall=23948
2021-05-05 23:16:36 | INFO | train_inner | epoch 002:  17788 / 60421 loss=1.086, ppl=2.12, wps=16279.3, ups=4.4, wpb=3700.5, bsz=143.5, num_updates=78200, lr=0.000113083, gnorm=1.297, loss_scale=4, train_wall=23, gb_free=10.7, wall=23970
2021-05-05 23:16:59 | INFO | train_inner | epoch 002:  17888 / 60421 loss=1.118, ppl=2.17, wps=16456, ups=4.4, wpb=3737.6, bsz=135.9, num_updates=78300, lr=0.000113011, gnorm=1.404, loss_scale=4, train_wall=23, gb_free=11.1, wall=23993
2021-05-05 23:17:22 | INFO | train_inner | epoch 002:  17988 / 60421 loss=1.151, ppl=2.22, wps=16433.7, ups=4.47, wpb=3676.8, bsz=128.1, num_updates=78400, lr=0.000112938, gnorm=1.731, loss_scale=4, train_wall=22, gb_free=10.8, wall=24015
2021-05-05 23:17:44 | INFO | train_inner | epoch 002:  18088 / 60421 loss=1.091, ppl=2.13, wps=16569.2, ups=4.4, wpb=3762.7, bsz=131.2, num_updates=78500, lr=0.000112867, gnorm=1.424, loss_scale=4, train_wall=23, gb_free=10.8, wall=24038
2021-05-05 23:18:07 | INFO | train_inner | epoch 002:  18188 / 60421 loss=1.199, ppl=2.3, wps=16427.4, ups=4.47, wpb=3677.3, bsz=127.8, num_updates=78600, lr=0.000112795, gnorm=1.715, loss_scale=4, train_wall=22, gb_free=10.9, wall=24061
2021-05-05 23:18:29 | INFO | train_inner | epoch 002:  18288 / 60421 loss=1.08, ppl=2.11, wps=16402.8, ups=4.42, wpb=3708.9, bsz=133.3, num_updates=78700, lr=0.000112723, gnorm=1.273, loss_scale=4, train_wall=22, gb_free=10.8, wall=24083
2021-05-05 23:18:52 | INFO | train_inner | epoch 002:  18388 / 60421 loss=1.071, ppl=2.1, wps=16731.2, ups=4.42, wpb=3784.2, bsz=135.7, num_updates=78800, lr=0.000112651, gnorm=1.322, loss_scale=4, train_wall=22, gb_free=10.7, wall=24106
2021-05-05 23:19:15 | INFO | train_inner | epoch 002:  18488 / 60421 loss=1.119, ppl=2.17, wps=16639, ups=4.39, wpb=3790.6, bsz=152.4, num_updates=78900, lr=0.00011258, gnorm=1.448, loss_scale=4, train_wall=23, gb_free=10.7, wall=24129
2021-05-05 23:19:37 | INFO | train_inner | epoch 002:  18588 / 60421 loss=1.197, ppl=2.29, wps=16706.2, ups=4.42, wpb=3777.4, bsz=110, num_updates=79000, lr=0.000112509, gnorm=1.395, loss_scale=4, train_wall=22, gb_free=10.9, wall=24151
2021-05-05 23:19:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:19:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:19:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:19:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:19:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:19:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:19:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:19:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:19:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:19:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:19:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:19:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:19:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:19:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:19:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:19:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:20:42 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.493 | ppl 22.52 | bleu 28.54 | wps 2454.2 | wpb 2024.1 | bsz 97.5 | num_updates 79000 | best_bleu 28.78
2021-05-05 23:20:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 79000 updates
2021-05-05 23:20:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_79000.pt
2021-05-05 23:20:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_79000.pt
2021-05-05 23:20:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_79000.pt (epoch 2 @ 79000 updates, score 28.54) (writing took 7.8993463090009755 seconds)
2021-05-05 23:21:13 | INFO | train_inner | epoch 002:  18688 / 60421 loss=1.104, ppl=2.15, wps=3946.9, ups=1.05, wpb=3769.5, bsz=123.5, num_updates=79100, lr=0.000112438, gnorm=1.457, loss_scale=4, train_wall=23, gb_free=10.8, wall=24247
2021-05-05 23:21:35 | INFO | train_inner | epoch 002:  18788 / 60421 loss=1.212, ppl=2.32, wps=16658, ups=4.41, wpb=3776.3, bsz=137.1, num_updates=79200, lr=0.000112367, gnorm=1.523, loss_scale=4, train_wall=22, gb_free=10.7, wall=24269
2021-05-05 23:21:58 | INFO | train_inner | epoch 002:  18888 / 60421 loss=1.089, ppl=2.13, wps=16651.7, ups=4.39, wpb=3797.2, bsz=123.5, num_updates=79300, lr=0.000112296, gnorm=1.107, loss_scale=4, train_wall=23, gb_free=10.9, wall=24292
2021-05-05 23:22:21 | INFO | train_inner | epoch 002:  18988 / 60421 loss=1.137, ppl=2.2, wps=16467.1, ups=4.44, wpb=3707.7, bsz=118.4, num_updates=79400, lr=0.000112225, gnorm=1.507, loss_scale=4, train_wall=22, gb_free=10.9, wall=24315
2021-05-05 23:22:44 | INFO | train_inner | epoch 002:  19088 / 60421 loss=1.009, ppl=2.01, wps=16512.3, ups=4.38, wpb=3765.8, bsz=152.7, num_updates=79500, lr=0.000112154, gnorm=1.251, loss_scale=4, train_wall=23, gb_free=10.8, wall=24337
2021-05-05 23:23:06 | INFO | train_inner | epoch 002:  19188 / 60421 loss=1.062, ppl=2.09, wps=16502.8, ups=4.39, wpb=3761.1, bsz=135.5, num_updates=79600, lr=0.000112084, gnorm=1.22, loss_scale=4, train_wall=23, gb_free=10.8, wall=24360
2021-05-05 23:23:29 | INFO | train_inner | epoch 002:  19288 / 60421 loss=1.18, ppl=2.27, wps=16526.3, ups=4.4, wpb=3752.2, bsz=112.8, num_updates=79700, lr=0.000112014, gnorm=1.495, loss_scale=4, train_wall=23, gb_free=10.7, wall=24383
2021-05-05 23:23:52 | INFO | train_inner | epoch 002:  19388 / 60421 loss=1.055, ppl=2.08, wps=16265.4, ups=4.39, wpb=3702.5, bsz=137, num_updates=79800, lr=0.000111943, gnorm=1.313, loss_scale=4, train_wall=23, gb_free=10.9, wall=24406
2021-05-05 23:24:15 | INFO | train_inner | epoch 002:  19488 / 60421 loss=1.063, ppl=2.09, wps=16185.7, ups=4.41, wpb=3674.2, bsz=130.6, num_updates=79900, lr=0.000111873, gnorm=1.647, loss_scale=4, train_wall=23, gb_free=10.8, wall=24428
2021-05-05 23:24:38 | INFO | train_inner | epoch 002:  19588 / 60421 loss=1.11, ppl=2.16, wps=16064.9, ups=4.33, wpb=3711.3, bsz=135.1, num_updates=80000, lr=0.000111803, gnorm=1.468, loss_scale=4, train_wall=23, gb_free=10.7, wall=24452
2021-05-05 23:24:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:24:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:24:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:24:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:24:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:24:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:24:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:24:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:24:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:24:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:24:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:24:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:24:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:24:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:24:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:24:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:25:43 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.511 | ppl 22.81 | bleu 28.6 | wps 2410.9 | wpb 2024.1 | bsz 97.5 | num_updates 80000 | best_bleu 28.78
2021-05-05 23:25:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 80000 updates
2021-05-05 23:25:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_80000.pt
2021-05-05 23:25:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_80000.pt
2021-05-05 23:25:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_80000.pt (epoch 2 @ 80000 updates, score 28.6) (writing took 7.8884894529910525 seconds)
2021-05-05 23:26:14 | INFO | train_inner | epoch 002:  19688 / 60421 loss=1.147, ppl=2.21, wps=3792.2, ups=1.03, wpb=3666.4, bsz=130, num_updates=80100, lr=0.000111734, gnorm=1.677, loss_scale=4, train_wall=23, gb_free=10.8, wall=24548
2021-05-05 23:26:37 | INFO | train_inner | epoch 002:  19788 / 60421 loss=1.135, ppl=2.2, wps=16397.2, ups=4.33, wpb=3784.4, bsz=143, num_updates=80200, lr=0.000111664, gnorm=1.355, loss_scale=4, train_wall=23, gb_free=10.9, wall=24571
2021-05-05 23:27:00 | INFO | train_inner | epoch 002:  19888 / 60421 loss=1.074, ppl=2.11, wps=16307.4, ups=4.43, wpb=3685.1, bsz=145.5, num_updates=80300, lr=0.000111594, gnorm=1.777, loss_scale=4, train_wall=22, gb_free=10.7, wall=24594
2021-05-05 23:27:23 | INFO | train_inner | epoch 002:  19988 / 60421 loss=1.088, ppl=2.13, wps=16585.8, ups=4.36, wpb=3803.1, bsz=139.9, num_updates=80400, lr=0.000111525, gnorm=1.233, loss_scale=4, train_wall=23, gb_free=10.7, wall=24617
2021-05-05 23:27:46 | INFO | train_inner | epoch 002:  20088 / 60421 loss=1.184, ppl=2.27, wps=16508.6, ups=4.4, wpb=3749.5, bsz=135.5, num_updates=80500, lr=0.000111456, gnorm=1.682, loss_scale=4, train_wall=23, gb_free=10.9, wall=24640
2021-05-05 23:28:09 | INFO | train_inner | epoch 002:  20188 / 60421 loss=1.125, ppl=2.18, wps=16584.4, ups=4.38, wpb=3790, bsz=122.7, num_updates=80600, lr=0.000111386, gnorm=1.255, loss_scale=4, train_wall=23, gb_free=10.9, wall=24662
2021-05-05 23:28:31 | INFO | train_inner | epoch 002:  20288 / 60421 loss=1.105, ppl=2.15, wps=16107.7, ups=4.46, wpb=3610.1, bsz=119.2, num_updates=80700, lr=0.000111317, gnorm=1.661, loss_scale=4, train_wall=22, gb_free=10.9, wall=24685
2021-05-05 23:28:54 | INFO | train_inner | epoch 002:  20388 / 60421 loss=1.072, ppl=2.1, wps=16105.7, ups=4.39, wpb=3666.5, bsz=134.6, num_updates=80800, lr=0.000111249, gnorm=1.621, loss_scale=4, train_wall=23, gb_free=10.6, wall=24708
2021-05-05 23:29:17 | INFO | train_inner | epoch 002:  20488 / 60421 loss=1.09, ppl=2.13, wps=16165.7, ups=4.36, wpb=3706.5, bsz=140.9, num_updates=80900, lr=0.00011118, gnorm=1.385, loss_scale=4, train_wall=23, gb_free=10.9, wall=24730
2021-05-05 23:29:40 | INFO | train_inner | epoch 002:  20588 / 60421 loss=1.129, ppl=2.19, wps=16163, ups=4.37, wpb=3697.7, bsz=109.5, num_updates=81000, lr=0.000111111, gnorm=1.399, loss_scale=4, train_wall=23, gb_free=10.8, wall=24753
2021-05-05 23:29:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:29:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:29:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:29:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:29:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:29:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:29:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:29:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:29:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:29:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:30:45 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.551 | ppl 23.44 | bleu 29.03 | wps 2429.2 | wpb 2024.1 | bsz 97.5 | num_updates 81000 | best_bleu 29.03
2021-05-05 23:30:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 81000 updates
2021-05-05 23:30:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_81000.pt
2021-05-05 23:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_81000.pt
2021-05-05 23:30:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_81000.pt (epoch 2 @ 81000 updates, score 29.03) (writing took 14.605058896995615 seconds)
2021-05-05 23:31:23 | INFO | train_inner | epoch 002:  20688 / 60421 loss=1.078, ppl=2.11, wps=3628, ups=0.97, wpb=3742.2, bsz=129.6, num_updates=81100, lr=0.000111043, gnorm=1.514, loss_scale=4, train_wall=23, gb_free=10.8, wall=24857
2021-05-05 23:31:46 | INFO | train_inner | epoch 002:  20788 / 60421 loss=1.152, ppl=2.22, wps=16375.3, ups=4.35, wpb=3766, bsz=102.5, num_updates=81200, lr=0.000110974, gnorm=1.326, loss_scale=4, train_wall=23, gb_free=10.7, wall=24880
2021-05-05 23:32:08 | INFO | train_inner | epoch 002:  20888 / 60421 loss=1.139, ppl=2.2, wps=16428.4, ups=4.38, wpb=3748.6, bsz=119.5, num_updates=81300, lr=0.000110906, gnorm=1.291, loss_scale=4, train_wall=23, gb_free=10.8, wall=24902
2021-05-05 23:32:31 | INFO | train_inner | epoch 002:  20988 / 60421 loss=1.088, ppl=2.13, wps=16606.6, ups=4.35, wpb=3814.9, bsz=125, num_updates=81400, lr=0.000110838, gnorm=1.123, loss_scale=4, train_wall=23, gb_free=11.1, wall=24925
2021-05-05 23:32:54 | INFO | train_inner | epoch 002:  21088 / 60421 loss=1.16, ppl=2.23, wps=16342.7, ups=4.44, wpb=3680.6, bsz=124.2, num_updates=81500, lr=0.00011077, gnorm=1.664, loss_scale=4, train_wall=22, gb_free=10.9, wall=24948
2021-05-05 23:33:17 | INFO | train_inner | epoch 002:  21188 / 60421 loss=1.089, ppl=2.13, wps=16544.9, ups=4.38, wpb=3774.3, bsz=128.7, num_updates=81600, lr=0.000110702, gnorm=1.195, loss_scale=4, train_wall=23, gb_free=10.8, wall=24971
2021-05-05 23:33:40 | INFO | train_inner | epoch 002:  21288 / 60421 loss=1.097, ppl=2.14, wps=16486.2, ups=4.33, wpb=3807.1, bsz=119.6, num_updates=81700, lr=0.000110634, gnorm=1.17, loss_scale=4, train_wall=23, gb_free=10.7, wall=24994
2021-05-05 23:34:03 | INFO | train_inner | epoch 002:  21388 / 60421 loss=1.118, ppl=2.17, wps=16359.6, ups=4.35, wpb=3765.1, bsz=127.9, num_updates=81800, lr=0.000110566, gnorm=1.28, loss_scale=4, train_wall=23, gb_free=10.7, wall=25017
2021-05-05 23:34:26 | INFO | train_inner | epoch 002:  21488 / 60421 loss=1.077, ppl=2.11, wps=16224.9, ups=4.35, wpb=3729, bsz=123.4, num_updates=81900, lr=0.000110499, gnorm=1.396, loss_scale=4, train_wall=23, gb_free=10.6, wall=25040
2021-05-05 23:34:49 | INFO | train_inner | epoch 002:  21588 / 60421 loss=1.073, ppl=2.1, wps=16019.3, ups=4.34, wpb=3690.6, bsz=126.2, num_updates=82000, lr=0.000110432, gnorm=1.497, loss_scale=4, train_wall=23, gb_free=10.9, wall=25063
2021-05-05 23:34:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:35:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:35:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:35:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:35:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:35:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:35:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:35:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:35:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:35:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:35:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:35:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:35:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:35:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:35:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:35:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:35:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.569 | ppl 23.74 | bleu 28.76 | wps 2439.5 | wpb 2024.1 | bsz 97.5 | num_updates 82000 | best_bleu 29.03
2021-05-05 23:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 82000 updates
2021-05-05 23:35:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_82000.pt
2021-05-05 23:35:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_82000.pt
2021-05-05 23:36:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_82000.pt (epoch 2 @ 82000 updates, score 28.76) (writing took 7.889899118003086 seconds)
2021-05-05 23:36:25 | INFO | train_inner | epoch 002:  21688 / 60421 loss=1.192, ppl=2.28, wps=3913.3, ups=1.04, wpb=3762.4, bsz=132.3, num_updates=82100, lr=0.000110364, gnorm=1.506, loss_scale=4, train_wall=23, gb_free=10.9, wall=25159
2021-05-05 23:36:48 | INFO | train_inner | epoch 002:  21788 / 60421 loss=1.069, ppl=2.1, wps=16451.7, ups=4.41, wpb=3734.1, bsz=139.3, num_updates=82200, lr=0.000110297, gnorm=1.513, loss_scale=4, train_wall=23, gb_free=10.9, wall=25182
2021-05-05 23:37:10 | INFO | train_inner | epoch 002:  21888 / 60421 loss=1.184, ppl=2.27, wps=16470.5, ups=4.42, wpb=3722.2, bsz=118.2, num_updates=82300, lr=0.00011023, gnorm=1.586, loss_scale=4, train_wall=22, gb_free=10.8, wall=25204
2021-05-05 23:37:33 | INFO | train_inner | epoch 002:  21988 / 60421 loss=1.162, ppl=2.24, wps=16531.3, ups=4.41, wpb=3747.8, bsz=120.4, num_updates=82400, lr=0.000110163, gnorm=1.441, loss_scale=4, train_wall=22, gb_free=11.1, wall=25227
2021-05-05 23:37:56 | INFO | train_inner | epoch 002:  22088 / 60421 loss=1.019, ppl=2.03, wps=16367.4, ups=4.4, wpb=3723.5, bsz=154.2, num_updates=82500, lr=0.000110096, gnorm=1.295, loss_scale=4, train_wall=23, gb_free=10.8, wall=25250
2021-05-05 23:38:18 | INFO | train_inner | epoch 002:  22188 / 60421 loss=1.1, ppl=2.14, wps=16290.3, ups=4.44, wpb=3669.5, bsz=146.5, num_updates=82600, lr=0.00011003, gnorm=1.869, loss_scale=4, train_wall=22, gb_free=11, wall=25272
2021-05-05 23:38:41 | INFO | train_inner | epoch 002:  22288 / 60421 loss=1.11, ppl=2.16, wps=16461, ups=4.39, wpb=3753.7, bsz=129.1, num_updates=82700, lr=0.000109963, gnorm=1.345, loss_scale=4, train_wall=23, gb_free=10.8, wall=25295
2021-05-05 23:39:04 | INFO | train_inner | epoch 002:  22388 / 60421 loss=1.052, ppl=2.07, wps=16116.3, ups=4.35, wpb=3707.3, bsz=130.6, num_updates=82800, lr=0.000109897, gnorm=1.277, loss_scale=4, train_wall=23, gb_free=10.8, wall=25318
2021-05-05 23:39:27 | INFO | train_inner | epoch 002:  22488 / 60421 loss=1.109, ppl=2.16, wps=16279.2, ups=4.31, wpb=3776.4, bsz=132.6, num_updates=82900, lr=0.00010983, gnorm=1.473, loss_scale=4, train_wall=23, gb_free=10.9, wall=25341
2021-05-05 23:39:50 | INFO | train_inner | epoch 002:  22588 / 60421 loss=1.137, ppl=2.2, wps=16242.1, ups=4.36, wpb=3725.6, bsz=118.6, num_updates=83000, lr=0.000109764, gnorm=1.512, loss_scale=4, train_wall=23, gb_free=10.7, wall=25364
2021-05-05 23:39:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:40:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:40:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:40:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:40:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:40:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:40:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:40:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:40:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:40:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:40:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.543 | ppl 23.32 | bleu 28.42 | wps 2413 | wpb 2024.1 | bsz 97.5 | num_updates 83000 | best_bleu 29.03
2021-05-05 23:40:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 83000 updates
2021-05-05 23:40:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_83000.pt
2021-05-05 23:40:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_83000.pt
2021-05-05 23:41:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_83000.pt (epoch 2 @ 83000 updates, score 28.42) (writing took 8.488656047004042 seconds)
2021-05-05 23:41:28 | INFO | train_inner | epoch 002:  22688 / 60421 loss=1.08, ppl=2.11, wps=3851.9, ups=1.03, wpb=3751.3, bsz=140.5, num_updates=83100, lr=0.000109698, gnorm=1.419, loss_scale=4, train_wall=23, gb_free=10.8, wall=25462
2021-05-05 23:41:50 | INFO | train_inner | epoch 002:  22788 / 60421 loss=1.066, ppl=2.09, wps=16226.3, ups=4.41, wpb=3680.7, bsz=142.7, num_updates=83200, lr=0.000109632, gnorm=1.462, loss_scale=4, train_wall=22, gb_free=10.6, wall=25484
2021-05-05 23:42:13 | INFO | train_inner | epoch 002:  22888 / 60421 loss=1.104, ppl=2.15, wps=16296.9, ups=4.47, wpb=3646.1, bsz=129.8, num_updates=83300, lr=0.000109566, gnorm=1.741, loss_scale=4, train_wall=22, gb_free=10.7, wall=25507
2021-05-05 23:42:36 | INFO | train_inner | epoch 002:  22988 / 60421 loss=1.096, ppl=2.14, wps=16486.5, ups=4.38, wpb=3762.6, bsz=143.2, num_updates=83400, lr=0.000109501, gnorm=1.421, loss_scale=4, train_wall=23, gb_free=10.8, wall=25529
2021-05-05 23:42:58 | INFO | train_inner | epoch 002:  23088 / 60421 loss=1.101, ppl=2.14, wps=16324.1, ups=4.42, wpb=3693, bsz=142.4, num_updates=83500, lr=0.000109435, gnorm=1.567, loss_scale=4, train_wall=22, gb_free=11.2, wall=25552
2021-05-05 23:43:21 | INFO | train_inner | epoch 002:  23188 / 60421 loss=1.09, ppl=2.13, wps=16549.3, ups=4.37, wpb=3789.3, bsz=138, num_updates=83600, lr=0.00010937, gnorm=1.286, loss_scale=4, train_wall=23, gb_free=10.8, wall=25575
2021-05-05 23:43:44 | INFO | train_inner | epoch 002:  23288 / 60421 loss=1.195, ppl=2.29, wps=16458.7, ups=4.32, wpb=3808.6, bsz=125.3, num_updates=83700, lr=0.000109304, gnorm=1.591, loss_scale=4, train_wall=23, gb_free=10.8, wall=25598
2021-05-05 23:44:07 | INFO | train_inner | epoch 002:  23388 / 60421 loss=1.179, ppl=2.26, wps=16509.8, ups=4.33, wpb=3812.8, bsz=126, num_updates=83800, lr=0.000109239, gnorm=1.43, loss_scale=4, train_wall=23, gb_free=10.7, wall=25621
2021-05-05 23:44:30 | INFO | train_inner | epoch 002:  23488 / 60421 loss=1.054, ppl=2.08, wps=16074.6, ups=4.35, wpb=3693.5, bsz=141.1, num_updates=83900, lr=0.000109174, gnorm=1.719, loss_scale=4, train_wall=23, gb_free=10.9, wall=25644
2021-05-05 23:44:54 | INFO | train_inner | epoch 002:  23588 / 60421 loss=1.057, ppl=2.08, wps=16169.7, ups=4.3, wpb=3756.1, bsz=119.8, num_updates=84000, lr=0.000109109, gnorm=1.16, loss_scale=4, train_wall=23, gb_free=10.8, wall=25667
2021-05-05 23:44:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:45:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:45:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:45:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:45:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:45:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:45:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:45:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:45:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:45:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:45:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:45:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:45:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:45:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:45:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:45:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:45:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.565 | ppl 23.67 | bleu 28.75 | wps 2404.5 | wpb 2024.1 | bsz 97.5 | num_updates 84000 | best_bleu 29.03
2021-05-05 23:45:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 84000 updates
2021-05-05 23:45:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_84000.pt
2021-05-05 23:46:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_84000.pt
2021-05-05 23:46:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_84000.pt (epoch 2 @ 84000 updates, score 28.75) (writing took 8.514786441999604 seconds)
2021-05-05 23:46:31 | INFO | train_inner | epoch 002:  23688 / 60421 loss=1.165, ppl=2.24, wps=3849.5, ups=1.03, wpb=3749.8, bsz=128.9, num_updates=84100, lr=0.000109044, gnorm=1.418, loss_scale=4, train_wall=23, gb_free=10.9, wall=25765
2021-05-05 23:46:54 | INFO | train_inner | epoch 002:  23788 / 60421 loss=1.098, ppl=2.14, wps=16493.4, ups=4.41, wpb=3737.9, bsz=131.4, num_updates=84200, lr=0.000108979, gnorm=1.504, loss_scale=4, train_wall=22, gb_free=11.2, wall=25787
2021-05-05 23:47:16 | INFO | train_inner | epoch 002:  23888 / 60421 loss=1.062, ppl=2.09, wps=16485.9, ups=4.39, wpb=3759.4, bsz=133.4, num_updates=84300, lr=0.000108915, gnorm=1.29, loss_scale=4, train_wall=23, gb_free=10.7, wall=25810
2021-05-05 23:47:39 | INFO | train_inner | epoch 002:  23988 / 60421 loss=1.047, ppl=2.07, wps=16494.4, ups=4.4, wpb=3746.3, bsz=133.4, num_updates=84400, lr=0.00010885, gnorm=1.246, loss_scale=4, train_wall=23, gb_free=11, wall=25833
2021-05-05 23:48:02 | INFO | train_inner | epoch 002:  24088 / 60421 loss=1.024, ppl=2.03, wps=16447.4, ups=4.37, wpb=3762.7, bsz=139.6, num_updates=84500, lr=0.000108786, gnorm=1.267, loss_scale=4, train_wall=23, gb_free=11.3, wall=25856
2021-05-05 23:48:25 | INFO | train_inner | epoch 002:  24188 / 60421 loss=1.047, ppl=2.07, wps=16453.8, ups=4.33, wpb=3797.6, bsz=165, num_updates=84600, lr=0.000108721, gnorm=1.393, loss_scale=4, train_wall=23, gb_free=10.8, wall=25879
2021-05-05 23:48:48 | INFO | train_inner | epoch 002:  24288 / 60421 loss=1.07, ppl=2.1, wps=16259.5, ups=4.39, wpb=3705.7, bsz=147.3, num_updates=84700, lr=0.000108657, gnorm=1.433, loss_scale=4, train_wall=23, gb_free=10.8, wall=25902
2021-05-05 23:49:11 | INFO | train_inner | epoch 002:  24388 / 60421 loss=1.079, ppl=2.11, wps=16332.8, ups=4.3, wpb=3796.2, bsz=127.4, num_updates=84800, lr=0.000108593, gnorm=1.26, loss_scale=4, train_wall=23, gb_free=10.8, wall=25925
2021-05-05 23:49:34 | INFO | train_inner | epoch 002:  24488 / 60421 loss=1.026, ppl=2.04, wps=16006.2, ups=4.34, wpb=3688, bsz=146.8, num_updates=84900, lr=0.000108529, gnorm=1.329, loss_scale=4, train_wall=23, gb_free=10.8, wall=25948
2021-05-05 23:49:57 | INFO | train_inner | epoch 002:  24588 / 60421 loss=1.077, ppl=2.11, wps=16012.6, ups=4.29, wpb=3735.9, bsz=123.2, num_updates=85000, lr=0.000108465, gnorm=1.549, loss_scale=4, train_wall=23, gb_free=10.7, wall=25971
2021-05-05 23:49:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:50:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:50:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:50:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:51:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.594 | ppl 24.15 | bleu 28.68 | wps 2448.8 | wpb 2024.1 | bsz 97.5 | num_updates 85000 | best_bleu 29.03
2021-05-05 23:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 85000 updates
2021-05-05 23:51:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_85000.pt
2021-05-05 23:51:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_85000.pt
2021-05-05 23:51:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_85000.pt (epoch 2 @ 85000 updates, score 28.68) (writing took 9.024788504000753 seconds)
2021-05-05 23:51:34 | INFO | train_inner | epoch 002:  24688 / 60421 loss=1.043, ppl=2.06, wps=3827.9, ups=1.03, wpb=3699.3, bsz=140.5, num_updates=85100, lr=0.000108401, gnorm=1.531, loss_scale=4, train_wall=23, gb_free=10.9, wall=26068
2021-05-05 23:51:57 | INFO | train_inner | epoch 002:  24788 / 60421 loss=1.072, ppl=2.1, wps=16328.6, ups=4.41, wpb=3706.7, bsz=132.7, num_updates=85200, lr=0.000108338, gnorm=1.374, loss_scale=4, train_wall=23, gb_free=11.1, wall=26091
2021-05-05 23:52:20 | INFO | train_inner | epoch 002:  24888 / 60421 loss=1.14, ppl=2.2, wps=16515.3, ups=4.4, wpb=3749.9, bsz=141, num_updates=85300, lr=0.000108274, gnorm=1.553, loss_scale=4, train_wall=23, gb_free=10.9, wall=26113
2021-05-05 23:52:42 | INFO | train_inner | epoch 002:  24988 / 60421 loss=1.08, ppl=2.11, wps=16652.3, ups=4.35, wpb=3826.5, bsz=130, num_updates=85400, lr=0.000108211, gnorm=1.231, loss_scale=4, train_wall=23, gb_free=10.9, wall=26136
2021-05-05 23:53:05 | INFO | train_inner | epoch 002:  25088 / 60421 loss=1.087, ppl=2.12, wps=16344.9, ups=4.42, wpb=3697.5, bsz=125, num_updates=85500, lr=0.000108148, gnorm=1.357, loss_scale=4, train_wall=22, gb_free=11.1, wall=26159
2021-05-05 23:53:28 | INFO | train_inner | epoch 002:  25188 / 60421 loss=1.149, ppl=2.22, wps=16450.4, ups=4.36, wpb=3774.1, bsz=124.2, num_updates=85600, lr=0.000108084, gnorm=1.423, loss_scale=4, train_wall=23, gb_free=10.8, wall=26182
2021-05-05 23:53:51 | INFO | train_inner | epoch 002:  25288 / 60421 loss=1.164, ppl=2.24, wps=16218, ups=4.42, wpb=3671.8, bsz=111, num_updates=85700, lr=0.000108021, gnorm=1.785, loss_scale=4, train_wall=22, gb_free=10.7, wall=26205
2021-05-05 23:54:14 | INFO | train_inner | epoch 002:  25388 / 60421 loss=1.105, ppl=2.15, wps=16274.1, ups=4.36, wpb=3733.3, bsz=106.9, num_updates=85800, lr=0.000107958, gnorm=1.338, loss_scale=4, train_wall=23, gb_free=10.7, wall=26228
2021-05-05 23:54:37 | INFO | train_inner | epoch 002:  25488 / 60421 loss=1.106, ppl=2.15, wps=16174.5, ups=4.36, wpb=3711.4, bsz=118.9, num_updates=85900, lr=0.000107896, gnorm=1.614, loss_scale=4, train_wall=23, gb_free=10.5, wall=26250
2021-05-05 23:55:00 | INFO | train_inner | epoch 002:  25588 / 60421 loss=1.049, ppl=2.07, wps=16047.6, ups=4.29, wpb=3738.5, bsz=151.8, num_updates=86000, lr=0.000107833, gnorm=1.46, loss_scale=4, train_wall=23, gb_free=10.8, wall=26274
2021-05-05 23:55:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-05 23:55:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:55:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:55:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:55:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:55:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:55:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:55:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:55:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:55:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:55:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:55:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:55:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:55:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:55:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:55:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:55:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-05 23:55:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-05 23:55:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-05 23:56:07 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.593 | ppl 24.13 | bleu 28.77 | wps 2365 | wpb 2024.1 | bsz 97.5 | num_updates 86000 | best_bleu 29.03
2021-05-05 23:56:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 86000 updates
2021-05-05 23:56:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_86000.pt
2021-05-05 23:56:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_86000.pt
2021-05-05 23:56:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_86000.pt (epoch 2 @ 86000 updates, score 28.77) (writing took 8.452241254999535 seconds)
2021-05-05 23:56:38 | INFO | train_inner | epoch 002:  25688 / 60421 loss=1.123, ppl=2.18, wps=3818.4, ups=1.01, wpb=3763.1, bsz=133.5, num_updates=86100, lr=0.00010777, gnorm=1.471, loss_scale=4, train_wall=23, gb_free=10.8, wall=26372
2021-05-05 23:57:01 | INFO | train_inner | epoch 002:  25788 / 60421 loss=1.099, ppl=2.14, wps=16493.6, ups=4.38, wpb=3762.9, bsz=127, num_updates=86200, lr=0.000107708, gnorm=1.312, loss_scale=4, train_wall=23, gb_free=10.9, wall=26395
2021-05-05 23:57:24 | INFO | train_inner | epoch 002:  25888 / 60421 loss=1.117, ppl=2.17, wps=16436.8, ups=4.42, wpb=3715.5, bsz=122.3, num_updates=86300, lr=0.000107645, gnorm=1.502, loss_scale=4, train_wall=22, gb_free=11.1, wall=26418
2021-05-05 23:57:47 | INFO | train_inner | epoch 002:  25988 / 60421 loss=1.061, ppl=2.09, wps=16524.5, ups=4.38, wpb=3774.2, bsz=133.8, num_updates=86400, lr=0.000107583, gnorm=1.241, loss_scale=4, train_wall=23, gb_free=10.9, wall=26441
2021-05-05 23:58:09 | INFO | train_inner | epoch 002:  26088 / 60421 loss=1.098, ppl=2.14, wps=16353.9, ups=4.39, wpb=3726.2, bsz=137.6, num_updates=86500, lr=0.000107521, gnorm=1.472, loss_scale=4, train_wall=23, gb_free=10.9, wall=26463
2021-05-05 23:58:32 | INFO | train_inner | epoch 002:  26188 / 60421 loss=1.092, ppl=2.13, wps=16481.7, ups=4.36, wpb=3778.1, bsz=129, num_updates=86600, lr=0.000107459, gnorm=1.336, loss_scale=4, train_wall=23, gb_free=10.7, wall=26486
2021-05-05 23:58:55 | INFO | train_inner | epoch 002:  26288 / 60421 loss=1.107, ppl=2.15, wps=16163, ups=4.4, wpb=3675.9, bsz=130.6, num_updates=86700, lr=0.000107397, gnorm=1.955, loss_scale=4, train_wall=23, gb_free=10.9, wall=26509
2021-05-05 23:59:18 | INFO | train_inner | epoch 002:  26388 / 60421 loss=1.098, ppl=2.14, wps=16144.1, ups=4.36, wpb=3704.9, bsz=137.9, num_updates=86800, lr=0.000107335, gnorm=1.49, loss_scale=4, train_wall=23, gb_free=10.7, wall=26532
2021-05-05 23:59:41 | INFO | train_inner | epoch 002:  26488 / 60421 loss=1.133, ppl=2.19, wps=16082.1, ups=4.33, wpb=3715.6, bsz=125.8, num_updates=86900, lr=0.000107273, gnorm=1.683, loss_scale=4, train_wall=23, gb_free=11, wall=26555
2021-05-06 00:00:04 | INFO | train_inner | epoch 002:  26588 / 60421 loss=1.066, ppl=2.09, wps=16080.7, ups=4.3, wpb=3740.8, bsz=137.3, num_updates=87000, lr=0.000107211, gnorm=1.341, loss_scale=4, train_wall=23, gb_free=10.9, wall=26578
2021-05-06 00:00:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:00:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:00:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:00:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:00:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:00:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:00:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:00:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:00:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:00:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:00:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:00:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:00:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:00:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:00:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:00:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:01:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.602 | ppl 24.28 | bleu 28.91 | wps 2443.5 | wpb 2024.1 | bsz 97.5 | num_updates 87000 | best_bleu 29.03
2021-05-06 00:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 87000 updates
2021-05-06 00:01:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_87000.pt
2021-05-06 00:01:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_87000.pt
2021-05-06 00:01:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_87000.pt (epoch 2 @ 87000 updates, score 28.91) (writing took 8.183230744005414 seconds)
2021-05-06 00:01:40 | INFO | train_inner | epoch 002:  26688 / 60421 loss=1.109, ppl=2.16, wps=3906.9, ups=1.04, wpb=3745.5, bsz=119.8, num_updates=87100, lr=0.00010715, gnorm=1.624, loss_scale=4, train_wall=23, gb_free=10.8, wall=26674
2021-05-06 00:02:03 | INFO | train_inner | epoch 002:  26788 / 60421 loss=1.094, ppl=2.13, wps=16411.5, ups=4.4, wpb=3732.6, bsz=137.6, num_updates=87200, lr=0.000107088, gnorm=1.451, loss_scale=4, train_wall=23, gb_free=10.7, wall=26697
2021-05-06 00:02:26 | INFO | train_inner | epoch 002:  26888 / 60421 loss=1.084, ppl=2.12, wps=16469.7, ups=4.35, wpb=3783.2, bsz=132.6, num_updates=87300, lr=0.000107027, gnorm=1.222, loss_scale=4, train_wall=23, gb_free=10.8, wall=26720
2021-05-06 00:02:49 | INFO | train_inner | epoch 002:  26988 / 60421 loss=1.061, ppl=2.09, wps=16423.6, ups=4.4, wpb=3733.9, bsz=128, num_updates=87400, lr=0.000106966, gnorm=1.251, loss_scale=4, train_wall=23, gb_free=11, wall=26743
2021-05-06 00:03:12 | INFO | train_inner | epoch 002:  27088 / 60421 loss=1.054, ppl=2.08, wps=16368.7, ups=4.37, wpb=3749.6, bsz=140.2, num_updates=87500, lr=0.000106904, gnorm=1.326, loss_scale=4, train_wall=23, gb_free=10.7, wall=26766
2021-05-06 00:03:34 | INFO | train_inner | epoch 002:  27188 / 60421 loss=1.061, ppl=2.09, wps=16310.1, ups=4.42, wpb=3688.9, bsz=127.8, num_updates=87600, lr=0.000106843, gnorm=1.256, loss_scale=4, train_wall=22, gb_free=11.2, wall=26788
2021-05-06 00:03:57 | INFO | train_inner | epoch 002:  27288 / 60421 loss=1.015, ppl=2.02, wps=16330.3, ups=4.33, wpb=3767.4, bsz=165.7, num_updates=87700, lr=0.000106783, gnorm=1.358, loss_scale=4, train_wall=23, gb_free=10.8, wall=26811
2021-05-06 00:04:20 | INFO | train_inner | epoch 002:  27388 / 60421 loss=1.083, ppl=2.12, wps=16314.1, ups=4.33, wpb=3769.1, bsz=130, num_updates=87800, lr=0.000106722, gnorm=1.253, loss_scale=4, train_wall=23, gb_free=10.8, wall=26834
2021-05-06 00:04:43 | INFO | train_inner | epoch 002:  27488 / 60421 loss=1.085, ppl=2.12, wps=16036.9, ups=4.36, wpb=3674, bsz=129.4, num_updates=87900, lr=0.000106661, gnorm=1.86, loss_scale=4, train_wall=23, gb_free=10.8, wall=26857
2021-05-06 00:05:07 | INFO | train_inner | epoch 002:  27588 / 60421 loss=1.115, ppl=2.17, wps=16147.8, ups=4.26, wpb=3786.7, bsz=144.9, num_updates=88000, lr=0.0001066, gnorm=1.468, loss_scale=4, train_wall=23, gb_free=10.8, wall=26881
2021-05-06 00:05:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:05:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:05:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:05:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:05:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:05:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:05:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:05:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:05:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:05:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:05:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:05:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:05:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:05:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:05:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:05:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:05:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:05:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:05:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:06:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.658 | ppl 25.25 | bleu 28.93 | wps 2386.3 | wpb 2024.1 | bsz 97.5 | num_updates 88000 | best_bleu 29.03
2021-05-06 00:06:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 88000 updates
2021-05-06 00:06:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_88000.pt
2021-05-06 00:06:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_88000.pt
2021-05-06 00:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_88000.pt (epoch 2 @ 88000 updates, score 28.93) (writing took 8.446741626001312 seconds)
2021-05-06 00:06:45 | INFO | train_inner | epoch 002:  27688 / 60421 loss=1.14, ppl=2.2, wps=3857.1, ups=1.02, wpb=3782.8, bsz=124.1, num_updates=88100, lr=0.00010654, gnorm=1.348, loss_scale=4, train_wall=23, gb_free=10.7, wall=26979
2021-05-06 00:07:08 | INFO | train_inner | epoch 002:  27788 / 60421 loss=1.059, ppl=2.08, wps=16367.7, ups=4.43, wpb=3693.5, bsz=127.3, num_updates=88200, lr=0.000106479, gnorm=1.597, loss_scale=4, train_wall=22, gb_free=10.9, wall=27001
2021-05-06 00:07:30 | INFO | train_inner | epoch 002:  27888 / 60421 loss=1.077, ppl=2.11, wps=16611.9, ups=4.41, wpb=3766, bsz=123.4, num_updates=88300, lr=0.000106419, gnorm=1.197, loss_scale=4, train_wall=22, gb_free=10.9, wall=27024
2021-05-06 00:07:53 | INFO | train_inner | epoch 002:  27988 / 60421 loss=1.158, ppl=2.23, wps=16432.8, ups=4.43, wpb=3705.9, bsz=109.7, num_updates=88400, lr=0.000106359, gnorm=1.433, loss_scale=4, train_wall=22, gb_free=10.9, wall=27047
2021-05-06 00:08:16 | INFO | train_inner | epoch 002:  28088 / 60421 loss=1.073, ppl=2.1, wps=16433.6, ups=4.37, wpb=3758.8, bsz=140.1, num_updates=88500, lr=0.000106299, gnorm=1.469, loss_scale=4, train_wall=23, gb_free=10.9, wall=27069
2021-05-06 00:08:38 | INFO | train_inner | epoch 002:  28188 / 60421 loss=1.104, ppl=2.15, wps=16279.3, ups=4.4, wpb=3699, bsz=132.7, num_updates=88600, lr=0.000106239, gnorm=1.485, loss_scale=4, train_wall=23, gb_free=10.9, wall=27092
2021-05-06 00:09:01 | INFO | train_inner | epoch 002:  28288 / 60421 loss=1.096, ppl=2.14, wps=16410.9, ups=4.33, wpb=3788.4, bsz=134.8, num_updates=88700, lr=0.000106179, gnorm=1.221, loss_scale=4, train_wall=23, gb_free=10.9, wall=27115
2021-05-06 00:09:25 | INFO | train_inner | epoch 002:  28388 / 60421 loss=1.088, ppl=2.13, wps=16346.3, ups=4.33, wpb=3774.2, bsz=135.2, num_updates=88800, lr=0.000106119, gnorm=1.258, loss_scale=4, train_wall=23, gb_free=10.7, wall=27138
2021-05-06 00:09:48 | INFO | train_inner | epoch 002:  28488 / 60421 loss=0.996, ppl=1.99, wps=16097.4, ups=4.34, wpb=3709, bsz=141, num_updates=88900, lr=0.000106059, gnorm=1.194, loss_scale=4, train_wall=23, gb_free=10.7, wall=27161
2021-05-06 00:10:11 | INFO | train_inner | epoch 002:  28588 / 60421 loss=1.128, ppl=2.19, wps=16026.3, ups=4.28, wpb=3748.5, bsz=121.9, num_updates=89000, lr=0.000106, gnorm=1.534, loss_scale=4, train_wall=23, gb_free=11.2, wall=27185
2021-05-06 00:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:10:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:10:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:10:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:10:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:10:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:10:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:10:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:10:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:10:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:10:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:10:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:10:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:10:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:10:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:10:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:11:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.594 | ppl 24.14 | bleu 28.81 | wps 2419.8 | wpb 2024.1 | bsz 97.5 | num_updates 89000 | best_bleu 29.03
2021-05-06 00:11:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 89000 updates
2021-05-06 00:11:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_89000.pt
2021-05-06 00:11:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_89000.pt
2021-05-06 00:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_89000.pt (epoch 2 @ 89000 updates, score 28.81) (writing took 7.879206162993796 seconds)
2021-05-06 00:11:48 | INFO | train_inner | epoch 002:  28688 / 60421 loss=1.095, ppl=2.14, wps=3919.9, ups=1.04, wpb=3786, bsz=126.7, num_updates=89100, lr=0.00010594, gnorm=1.391, loss_scale=4, train_wall=23, gb_free=10.7, wall=27281
2021-05-06 00:12:10 | INFO | train_inner | epoch 002:  28788 / 60421 loss=1.098, ppl=2.14, wps=16385.6, ups=4.38, wpb=3742.6, bsz=132.2, num_updates=89200, lr=0.000105881, gnorm=1.277, loss_scale=4, train_wall=23, gb_free=10.8, wall=27304
2021-05-06 00:12:33 | INFO | train_inner | epoch 002:  28888 / 60421 loss=1.107, ppl=2.15, wps=16562.8, ups=4.38, wpb=3782, bsz=128.6, num_updates=89300, lr=0.000105822, gnorm=1.449, loss_scale=4, train_wall=23, gb_free=10.8, wall=27327
2021-05-06 00:12:56 | INFO | train_inner | epoch 002:  28988 / 60421 loss=1.059, ppl=2.08, wps=16541.7, ups=4.45, wpb=3717.9, bsz=130.2, num_updates=89400, lr=0.000105762, gnorm=1.413, loss_scale=4, train_wall=22, gb_free=10.8, wall=27350
2021-05-06 00:13:18 | INFO | train_inner | epoch 002:  29088 / 60421 loss=1.099, ppl=2.14, wps=16313, ups=4.47, wpb=3647.1, bsz=114.1, num_updates=89500, lr=0.000105703, gnorm=1.511, loss_scale=4, train_wall=22, gb_free=10.5, wall=27372
2021-05-06 00:13:41 | INFO | train_inner | epoch 002:  29188 / 60421 loss=1.045, ppl=2.06, wps=16255.1, ups=4.42, wpb=3678.6, bsz=137.5, num_updates=89600, lr=0.000105644, gnorm=1.485, loss_scale=4, train_wall=22, gb_free=11.2, wall=27395
2021-05-06 00:14:04 | INFO | train_inner | epoch 002:  29288 / 60421 loss=1.078, ppl=2.11, wps=16242.1, ups=4.37, wpb=3720.1, bsz=122.2, num_updates=89700, lr=0.000105585, gnorm=1.447, loss_scale=4, train_wall=23, gb_free=10.7, wall=27417
2021-05-06 00:14:27 | INFO | train_inner | epoch 002:  29388 / 60421 loss=1.105, ppl=2.15, wps=16459.7, ups=4.33, wpb=3797.2, bsz=125.8, num_updates=89800, lr=0.000105527, gnorm=1.209, loss_scale=4, train_wall=23, gb_free=10.8, wall=27441
2021-05-06 00:14:50 | INFO | train_inner | epoch 002:  29488 / 60421 loss=1.003, ppl=2, wps=16029.5, ups=4.28, wpb=3747.9, bsz=146.3, num_updates=89900, lr=0.000105468, gnorm=1.248, loss_scale=4, train_wall=23, gb_free=10.7, wall=27464
2021-05-06 00:15:14 | INFO | train_inner | epoch 002:  29588 / 60421 loss=1.049, ppl=2.07, wps=16115.9, ups=4.23, wpb=3812.7, bsz=131.2, num_updates=90000, lr=0.000105409, gnorm=1.242, loss_scale=4, train_wall=23, gb_free=10.8, wall=27488
2021-05-06 00:15:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:15:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:15:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:15:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:15:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:15:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:15:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:15:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:15:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:15:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:16:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.623 | ppl 24.64 | bleu 28.73 | wps 2400.2 | wpb 2024.1 | bsz 97.5 | num_updates 90000 | best_bleu 29.03
2021-05-06 00:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 90000 updates
2021-05-06 00:16:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_90000.pt
2021-05-06 00:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_90000.pt
2021-05-06 00:16:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_90000.pt (epoch 2 @ 90000 updates, score 28.73) (writing took 7.894957392010838 seconds)
2021-05-06 00:16:51 | INFO | train_inner | epoch 002:  29688 / 60421 loss=1.056, ppl=2.08, wps=3847.9, ups=1.03, wpb=3730.5, bsz=133, num_updates=90100, lr=0.000105351, gnorm=1.474, loss_scale=8, train_wall=23, gb_free=10.7, wall=27584
2021-05-06 00:17:13 | INFO | train_inner | epoch 002:  29788 / 60421 loss=1.111, ppl=2.16, wps=16558.6, ups=4.4, wpb=3759.5, bsz=114.6, num_updates=90200, lr=0.000105292, gnorm=1.306, loss_scale=8, train_wall=23, gb_free=11, wall=27607
2021-05-06 00:17:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-06 00:17:36 | INFO | train_inner | epoch 002:  29889 / 60421 loss=1.095, ppl=2.14, wps=16412.6, ups=4.41, wpb=3724.2, bsz=119.1, num_updates=90300, lr=0.000105234, gnorm=1.289, loss_scale=4, train_wall=22, gb_free=10.9, wall=27630
2021-05-06 00:17:59 | INFO | train_inner | epoch 002:  29989 / 60421 loss=1.103, ppl=2.15, wps=16591, ups=4.41, wpb=3763.4, bsz=117, num_updates=90400, lr=0.000105176, gnorm=1.157, loss_scale=4, train_wall=22, gb_free=10.9, wall=27653
2021-05-06 00:18:21 | INFO | train_inner | epoch 002:  30089 / 60421 loss=1.077, ppl=2.11, wps=16358.7, ups=4.48, wpb=3652.3, bsz=126.2, num_updates=90500, lr=0.000105118, gnorm=1.523, loss_scale=4, train_wall=22, gb_free=11.2, wall=27675
2021-05-06 00:18:44 | INFO | train_inner | epoch 002:  30189 / 60421 loss=1.072, ppl=2.1, wps=16783.7, ups=4.38, wpb=3828.7, bsz=136, num_updates=90600, lr=0.00010506, gnorm=1.103, loss_scale=4, train_wall=23, gb_free=11, wall=27698
2021-05-06 00:19:07 | INFO | train_inner | epoch 002:  30289 / 60421 loss=1.114, ppl=2.16, wps=16544.2, ups=4.41, wpb=3749.8, bsz=136.1, num_updates=90700, lr=0.000105002, gnorm=1.496, loss_scale=4, train_wall=22, gb_free=11, wall=27720
2021-05-06 00:19:29 | INFO | train_inner | epoch 002:  30389 / 60421 loss=1.048, ppl=2.07, wps=16587.2, ups=4.43, wpb=3743.7, bsz=129.3, num_updates=90800, lr=0.000104944, gnorm=1.266, loss_scale=4, train_wall=22, gb_free=10.8, wall=27743
2021-05-06 00:19:51 | INFO | train_inner | epoch 002:  30489 / 60421 loss=1.073, ppl=2.1, wps=16600.8, ups=4.47, wpb=3712.1, bsz=135.8, num_updates=90900, lr=0.000104886, gnorm=1.466, loss_scale=4, train_wall=22, gb_free=10.8, wall=27765
2021-05-06 00:20:14 | INFO | train_inner | epoch 002:  30589 / 60421 loss=1.072, ppl=2.1, wps=16553, ups=4.45, wpb=3718.8, bsz=123.6, num_updates=91000, lr=0.000104828, gnorm=1.415, loss_scale=4, train_wall=22, gb_free=10.8, wall=27788
2021-05-06 00:20:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:20:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:20:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:20:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:20:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:20:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:20:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:20:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:20:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:20:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:20:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:20:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:20:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:20:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:20:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:20:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:21:18 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.61 | ppl 24.42 | bleu 28.73 | wps 2457.7 | wpb 2024.1 | bsz 97.5 | num_updates 91000 | best_bleu 29.03
2021-05-06 00:21:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 91000 updates
2021-05-06 00:21:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_91000.pt
2021-05-06 00:21:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_91000.pt
2021-05-06 00:21:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_91000.pt (epoch 2 @ 91000 updates, score 28.73) (writing took 7.915970582980663 seconds)
2021-05-06 00:21:49 | INFO | train_inner | epoch 002:  30689 / 60421 loss=1.132, ppl=2.19, wps=3949.5, ups=1.05, wpb=3756.4, bsz=120.7, num_updates=91100, lr=0.000104771, gnorm=1.393, loss_scale=4, train_wall=22, gb_free=10.8, wall=27883
2021-05-06 00:22:11 | INFO | train_inner | epoch 002:  30789 / 60421 loss=1.097, ppl=2.14, wps=16401.9, ups=4.47, wpb=3673.2, bsz=127.1, num_updates=91200, lr=0.000104713, gnorm=1.557, loss_scale=4, train_wall=22, gb_free=10.7, wall=27905
2021-05-06 00:22:34 | INFO | train_inner | epoch 002:  30889 / 60421 loss=1.059, ppl=2.08, wps=16625.2, ups=4.44, wpb=3740.7, bsz=132.3, num_updates=91300, lr=0.000104656, gnorm=1.246, loss_scale=4, train_wall=22, gb_free=10.7, wall=27928
2021-05-06 00:22:57 | INFO | train_inner | epoch 002:  30989 / 60421 loss=1.035, ppl=2.05, wps=16354, ups=4.41, wpb=3709, bsz=128.2, num_updates=91400, lr=0.000104599, gnorm=1.232, loss_scale=4, train_wall=22, gb_free=10.6, wall=27950
2021-05-06 00:23:19 | INFO | train_inner | epoch 002:  31089 / 60421 loss=1.064, ppl=2.09, wps=16488.4, ups=4.39, wpb=3753.4, bsz=130.6, num_updates=91500, lr=0.000104542, gnorm=1.28, loss_scale=4, train_wall=23, gb_free=10.8, wall=27973
2021-05-06 00:23:42 | INFO | train_inner | epoch 002:  31189 / 60421 loss=1.086, ppl=2.12, wps=16377, ups=4.35, wpb=3762.7, bsz=139.1, num_updates=91600, lr=0.000104485, gnorm=1.489, loss_scale=4, train_wall=23, gb_free=10.9, wall=27996
2021-05-06 00:24:06 | INFO | train_inner | epoch 002:  31289 / 60421 loss=1.07, ppl=2.1, wps=16283.9, ups=4.32, wpb=3770.1, bsz=127, num_updates=91700, lr=0.000104428, gnorm=1.38, loss_scale=4, train_wall=23, gb_free=10.9, wall=28019
2021-05-06 00:24:28 | INFO | train_inner | epoch 002:  31389 / 60421 loss=1.075, ppl=2.11, wps=15991, ups=4.4, wpb=3634.4, bsz=123.5, num_updates=91800, lr=0.000104371, gnorm=1.569, loss_scale=4, train_wall=23, gb_free=10.7, wall=28042
2021-05-06 00:24:52 | INFO | train_inner | epoch 002:  31489 / 60421 loss=1.12, ppl=2.17, wps=16156.9, ups=4.29, wpb=3766, bsz=121, num_updates=91900, lr=0.000104314, gnorm=1.276, loss_scale=4, train_wall=23, gb_free=10.8, wall=28065
2021-05-06 00:25:15 | INFO | train_inner | epoch 002:  31589 / 60421 loss=1.098, ppl=2.14, wps=15992.5, ups=4.33, wpb=3697, bsz=112.5, num_updates=92000, lr=0.000104257, gnorm=1.446, loss_scale=4, train_wall=23, gb_free=10.8, wall=28089
2021-05-06 00:25:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:25:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:25:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:25:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:25:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:25:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:25:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:25:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:25:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:25:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:26:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.713 | ppl 26.23 | bleu 28.43 | wps 2422 | wpb 2024.1 | bsz 97.5 | num_updates 92000 | best_bleu 29.03
2021-05-06 00:26:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 92000 updates
2021-05-06 00:26:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_92000.pt
2021-05-06 00:26:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_92000.pt
2021-05-06 00:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_92000.pt (epoch 2 @ 92000 updates, score 28.43) (writing took 7.921803890989395 seconds)
2021-05-06 00:26:51 | INFO | train_inner | epoch 002:  31689 / 60421 loss=1.056, ppl=2.08, wps=3857.9, ups=1.04, wpb=3716.8, bsz=127.2, num_updates=92100, lr=0.000104201, gnorm=1.346, loss_scale=4, train_wall=23, gb_free=10.9, wall=28185
2021-05-06 00:27:13 | INFO | train_inner | epoch 002:  31789 / 60421 loss=1.053, ppl=2.08, wps=16054.8, ups=4.51, wpb=3558, bsz=129.4, num_updates=92200, lr=0.000104144, gnorm=2.055, loss_scale=4, train_wall=22, gb_free=10.9, wall=28207
2021-05-06 00:27:36 | INFO | train_inner | epoch 002:  31889 / 60421 loss=1.039, ppl=2.06, wps=16448.4, ups=4.42, wpb=3720.8, bsz=131.1, num_updates=92300, lr=0.000104088, gnorm=1.407, loss_scale=4, train_wall=22, gb_free=10.6, wall=28230
2021-05-06 00:27:59 | INFO | train_inner | epoch 002:  31989 / 60421 loss=1.111, ppl=2.16, wps=16513.7, ups=4.4, wpb=3750.1, bsz=126.6, num_updates=92400, lr=0.000104031, gnorm=1.3, loss_scale=4, train_wall=23, gb_free=10.9, wall=28252
2021-05-06 00:28:21 | INFO | train_inner | epoch 002:  32089 / 60421 loss=1.066, ppl=2.09, wps=16363.2, ups=4.4, wpb=3717.3, bsz=123.5, num_updates=92500, lr=0.000103975, gnorm=1.442, loss_scale=4, train_wall=23, gb_free=11.3, wall=28275
2021-05-06 00:28:44 | INFO | train_inner | epoch 002:  32189 / 60421 loss=1.156, ppl=2.23, wps=16240.9, ups=4.39, wpb=3699.1, bsz=126.5, num_updates=92600, lr=0.000103919, gnorm=1.696, loss_scale=4, train_wall=23, gb_free=10.8, wall=28298
2021-05-06 00:29:07 | INFO | train_inner | epoch 002:  32289 / 60421 loss=1.019, ppl=2.03, wps=16161.7, ups=4.36, wpb=3706.2, bsz=140.5, num_updates=92700, lr=0.000103863, gnorm=1.428, loss_scale=4, train_wall=23, gb_free=10.9, wall=28321
2021-05-06 00:29:30 | INFO | train_inner | epoch 002:  32389 / 60421 loss=1.086, ppl=2.12, wps=16184.4, ups=4.32, wpb=3749.2, bsz=133, num_updates=92800, lr=0.000103807, gnorm=1.156, loss_scale=4, train_wall=23, gb_free=10.7, wall=28344
2021-05-06 00:29:53 | INFO | train_inner | epoch 002:  32489 / 60421 loss=1.044, ppl=2.06, wps=16141, ups=4.34, wpb=3715.4, bsz=134.8, num_updates=92900, lr=0.000103751, gnorm=1.302, loss_scale=4, train_wall=23, gb_free=10.7, wall=28367
2021-05-06 00:30:17 | INFO | train_inner | epoch 002:  32589 / 60421 loss=1.034, ppl=2.05, wps=15995.3, ups=4.25, wpb=3763.5, bsz=147.4, num_updates=93000, lr=0.000103695, gnorm=1.19, loss_scale=4, train_wall=23, gb_free=11.4, wall=28391
2021-05-06 00:30:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:30:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:30:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:30:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:30:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:30:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:30:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:30:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:30:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:30:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:31:22 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.682 | ppl 25.67 | bleu 28.97 | wps 2435.9 | wpb 2024.1 | bsz 97.5 | num_updates 93000 | best_bleu 29.03
2021-05-06 00:31:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 93000 updates
2021-05-06 00:31:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_93000.pt
2021-05-06 00:31:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_93000.pt
2021-05-06 00:31:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_93000.pt (epoch 2 @ 93000 updates, score 28.97) (writing took 7.904864002979593 seconds)
2021-05-06 00:31:53 | INFO | train_inner | epoch 002:  32689 / 60421 loss=1.068, ppl=2.1, wps=3951.4, ups=1.04, wpb=3796.9, bsz=151.7, num_updates=93100, lr=0.000103639, gnorm=1.183, loss_scale=4, train_wall=23, gb_free=10.9, wall=28487
2021-05-06 00:32:15 | INFO | train_inner | epoch 002:  32789 / 60421 loss=1.018, ppl=2.03, wps=16415.7, ups=4.41, wpb=3722.6, bsz=141.1, num_updates=93200, lr=0.000103584, gnorm=1.493, loss_scale=4, train_wall=22, gb_free=12, wall=28509
2021-05-06 00:32:38 | INFO | train_inner | epoch 002:  32889 / 60421 loss=1.105, ppl=2.15, wps=16388.9, ups=4.48, wpb=3656.5, bsz=123.8, num_updates=93300, lr=0.000103528, gnorm=1.294, loss_scale=4, train_wall=22, gb_free=11.2, wall=28532
2021-05-06 00:33:00 | INFO | train_inner | epoch 002:  32989 / 60421 loss=1.16, ppl=2.23, wps=16489.4, ups=4.4, wpb=3747.3, bsz=132.7, num_updates=93400, lr=0.000103473, gnorm=1.635, loss_scale=4, train_wall=23, gb_free=10.8, wall=28554
2021-05-06 00:33:23 | INFO | train_inner | epoch 002:  33089 / 60421 loss=1.066, ppl=2.09, wps=16258.8, ups=4.43, wpb=3671.1, bsz=116.2, num_updates=93500, lr=0.000103418, gnorm=1.489, loss_scale=4, train_wall=22, gb_free=11, wall=28577
2021-05-06 00:33:46 | INFO | train_inner | epoch 002:  33189 / 60421 loss=1.067, ppl=2.1, wps=16171.3, ups=4.4, wpb=3678.9, bsz=159.2, num_updates=93600, lr=0.000103362, gnorm=1.671, loss_scale=4, train_wall=23, gb_free=10.8, wall=28600
2021-05-06 00:34:09 | INFO | train_inner | epoch 002:  33289 / 60421 loss=1.073, ppl=2.1, wps=16295.5, ups=4.33, wpb=3763.1, bsz=150.8, num_updates=93700, lr=0.000103307, gnorm=1.336, loss_scale=4, train_wall=23, gb_free=10.7, wall=28623
2021-05-06 00:34:32 | INFO | train_inner | epoch 002:  33389 / 60421 loss=0.967, ppl=1.95, wps=15994.6, ups=4.3, wpb=3721.4, bsz=157, num_updates=93800, lr=0.000103252, gnorm=1.484, loss_scale=4, train_wall=23, gb_free=10.8, wall=28646
2021-05-06 00:34:55 | INFO | train_inner | epoch 002:  33489 / 60421 loss=1.073, ppl=2.1, wps=16069.5, ups=4.34, wpb=3706.7, bsz=130.2, num_updates=93900, lr=0.000103197, gnorm=1.662, loss_scale=4, train_wall=23, gb_free=10.8, wall=28669
2021-05-06 00:35:19 | INFO | train_inner | epoch 002:  33589 / 60421 loss=1.117, ppl=2.17, wps=15941.9, ups=4.28, wpb=3722.7, bsz=120.6, num_updates=94000, lr=0.000103142, gnorm=1.493, loss_scale=4, train_wall=23, gb_free=11, wall=28692
2021-05-06 00:35:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:35:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:35:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:35:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:35:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:35:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:35:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:35:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:35:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:35:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:35:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:35:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:35:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:35:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:35:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:35:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:36:23 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.718 | ppl 26.32 | bleu 27.36 | wps 2460.8 | wpb 2024.1 | bsz 97.5 | num_updates 94000 | best_bleu 29.03
2021-05-06 00:36:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 94000 updates
2021-05-06 00:36:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_94000.pt
2021-05-06 00:36:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_94000.pt
2021-05-06 00:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_94000.pt (epoch 2 @ 94000 updates, score 27.36) (writing took 7.921018171007745 seconds)
2021-05-06 00:36:54 | INFO | train_inner | epoch 002:  33689 / 60421 loss=1.051, ppl=2.07, wps=3918.4, ups=1.05, wpb=3733.2, bsz=139.6, num_updates=94100, lr=0.000103087, gnorm=1.242, loss_scale=4, train_wall=23, gb_free=10.9, wall=28788
2021-05-06 00:37:16 | INFO | train_inner | epoch 002:  33789 / 60421 loss=1.111, ppl=2.16, wps=16599.1, ups=4.42, wpb=3756.7, bsz=130.2, num_updates=94200, lr=0.000103033, gnorm=1.343, loss_scale=4, train_wall=22, gb_free=10.8, wall=28810
2021-05-06 00:37:39 | INFO | train_inner | epoch 002:  33889 / 60421 loss=1.055, ppl=2.08, wps=16480.5, ups=4.39, wpb=3749.9, bsz=131.1, num_updates=94300, lr=0.000102978, gnorm=1.302, loss_scale=4, train_wall=23, gb_free=10.9, wall=28833
2021-05-06 00:38:02 | INFO | train_inner | epoch 002:  33989 / 60421 loss=1.089, ppl=2.13, wps=16503, ups=4.42, wpb=3734.1, bsz=116.6, num_updates=94400, lr=0.000102923, gnorm=1.337, loss_scale=4, train_wall=22, gb_free=10.7, wall=28856
2021-05-06 00:38:24 | INFO | train_inner | epoch 002:  34089 / 60421 loss=1.034, ppl=2.05, wps=16229.3, ups=4.44, wpb=3656.2, bsz=120.2, num_updates=94500, lr=0.000102869, gnorm=1.331, loss_scale=4, train_wall=22, gb_free=10.7, wall=28878
2021-05-06 00:38:47 | INFO | train_inner | epoch 002:  34189 / 60421 loss=1.02, ppl=2.03, wps=16266.8, ups=4.39, wpb=3703.9, bsz=139.2, num_updates=94600, lr=0.000102815, gnorm=1.449, loss_scale=4, train_wall=23, gb_free=10.7, wall=28901
2021-05-06 00:39:10 | INFO | train_inner | epoch 002:  34289 / 60421 loss=0.958, ppl=1.94, wps=16125.1, ups=4.39, wpb=3672.1, bsz=147.2, num_updates=94700, lr=0.00010276, gnorm=1.263, loss_scale=4, train_wall=23, gb_free=10.8, wall=28924
2021-05-06 00:39:33 | INFO | train_inner | epoch 002:  34389 / 60421 loss=1.067, ppl=2.09, wps=16194.5, ups=4.35, wpb=3725.8, bsz=132.3, num_updates=94800, lr=0.000102706, gnorm=1.342, loss_scale=4, train_wall=23, gb_free=10.7, wall=28947
2021-05-06 00:39:56 | INFO | train_inner | epoch 002:  34489 / 60421 loss=1.061, ppl=2.09, wps=16052, ups=4.31, wpb=3727.3, bsz=131.1, num_updates=94900, lr=0.000102652, gnorm=1.468, loss_scale=4, train_wall=23, gb_free=11, wall=28970
2021-05-06 00:40:20 | INFO | train_inner | epoch 002:  34589 / 60421 loss=1.025, ppl=2.04, wps=15874.1, ups=4.27, wpb=3721.5, bsz=135, num_updates=95000, lr=0.000102598, gnorm=1.619, loss_scale=4, train_wall=23, gb_free=10.7, wall=28993
2021-05-06 00:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:40:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:40:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:40:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:40:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:40:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:40:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:41:24 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.682 | ppl 25.67 | bleu 28.6 | wps 2443.5 | wpb 2024.1 | bsz 97.5 | num_updates 95000 | best_bleu 29.03
2021-05-06 00:41:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 95000 updates
2021-05-06 00:41:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_95000.pt
2021-05-06 00:41:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_95000.pt
2021-05-06 00:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_95000.pt (epoch 2 @ 95000 updates, score 28.6) (writing took 7.910758946993155 seconds)
2021-05-06 00:41:55 | INFO | train_inner | epoch 002:  34689 / 60421 loss=1.056, ppl=2.08, wps=3924.1, ups=1.04, wpb=3761.1, bsz=135.5, num_updates=95100, lr=0.000102544, gnorm=1.504, loss_scale=4, train_wall=23, gb_free=11, wall=29089
2021-05-06 00:42:18 | INFO | train_inner | epoch 002:  34789 / 60421 loss=1.09, ppl=2.13, wps=16362.8, ups=4.47, wpb=3660, bsz=119.6, num_updates=95200, lr=0.00010249, gnorm=1.636, loss_scale=4, train_wall=22, gb_free=10.9, wall=29112
2021-05-06 00:42:40 | INFO | train_inner | epoch 002:  34889 / 60421 loss=1.078, ppl=2.11, wps=16443.3, ups=4.42, wpb=3721, bsz=127.6, num_updates=95300, lr=0.000102436, gnorm=1.318, loss_scale=4, train_wall=22, gb_free=10.6, wall=29134
2021-05-06 00:43:03 | INFO | train_inner | epoch 002:  34989 / 60421 loss=1.14, ppl=2.2, wps=16413.2, ups=4.43, wpb=3704, bsz=126.7, num_updates=95400, lr=0.000102383, gnorm=1.855, loss_scale=4, train_wall=22, gb_free=10.7, wall=29157
2021-05-06 00:43:26 | INFO | train_inner | epoch 002:  35089 / 60421 loss=1.1, ppl=2.14, wps=16423.1, ups=4.33, wpb=3793.5, bsz=138.9, num_updates=95500, lr=0.000102329, gnorm=1.29, loss_scale=4, train_wall=23, gb_free=10.8, wall=29180
2021-05-06 00:43:49 | INFO | train_inner | epoch 002:  35189 / 60421 loss=1.113, ppl=2.16, wps=16189.4, ups=4.39, wpb=3688.5, bsz=110.6, num_updates=95600, lr=0.000102275, gnorm=1.552, loss_scale=4, train_wall=23, gb_free=10.8, wall=29203
2021-05-06 00:44:12 | INFO | train_inner | epoch 002:  35289 / 60421 loss=1.069, ppl=2.1, wps=16306, ups=4.33, wpb=3769.6, bsz=140.1, num_updates=95700, lr=0.000102222, gnorm=1.136, loss_scale=4, train_wall=23, gb_free=10.7, wall=29226
2021-05-06 00:44:35 | INFO | train_inner | epoch 002:  35389 / 60421 loss=1.052, ppl=2.07, wps=16234.6, ups=4.38, wpb=3708, bsz=131.4, num_updates=95800, lr=0.000102169, gnorm=1.35, loss_scale=4, train_wall=23, gb_free=11.3, wall=29249
2021-05-06 00:44:58 | INFO | train_inner | epoch 002:  35489 / 60421 loss=1.065, ppl=2.09, wps=16172, ups=4.3, wpb=3760.7, bsz=137.2, num_updates=95900, lr=0.000102115, gnorm=1.206, loss_scale=4, train_wall=23, gb_free=10.7, wall=29272
2021-05-06 00:45:22 | INFO | train_inner | epoch 002:  35589 / 60421 loss=1.01, ppl=2.01, wps=15923.1, ups=4.26, wpb=3740.4, bsz=125, num_updates=96000, lr=0.000102062, gnorm=1.327, loss_scale=4, train_wall=23, gb_free=11.4, wall=29295
2021-05-06 00:45:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:45:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:45:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:45:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:45:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:45:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:45:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:45:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:45:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:45:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:45:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:45:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:45:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:46:26 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.698 | ppl 25.95 | bleu 28.74 | wps 2447.3 | wpb 2024.1 | bsz 97.5 | num_updates 96000 | best_bleu 29.03
2021-05-06 00:46:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 96000 updates
2021-05-06 00:46:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_96000.pt
2021-05-06 00:46:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_96000.pt
2021-05-06 00:46:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_96000.pt (epoch 2 @ 96000 updates, score 28.74) (writing took 7.894975736999186 seconds)
2021-05-06 00:46:57 | INFO | train_inner | epoch 002:  35689 / 60421 loss=1.119, ppl=2.17, wps=3910.1, ups=1.05, wpb=3729.5, bsz=132.8, num_updates=96100, lr=0.000102009, gnorm=1.514, loss_scale=4, train_wall=22, gb_free=10.7, wall=29391
2021-05-06 00:47:20 | INFO | train_inner | epoch 002:  35789 / 60421 loss=1.062, ppl=2.09, wps=16308.1, ups=4.42, wpb=3693.2, bsz=135.5, num_updates=96200, lr=0.000101956, gnorm=1.538, loss_scale=4, train_wall=22, gb_free=10.8, wall=29414
2021-05-06 00:47:42 | INFO | train_inner | epoch 002:  35889 / 60421 loss=1.048, ppl=2.07, wps=16378.5, ups=4.42, wpb=3706, bsz=132.6, num_updates=96300, lr=0.000101903, gnorm=1.276, loss_scale=4, train_wall=22, gb_free=10.6, wall=29436
2021-05-06 00:48:05 | INFO | train_inner | epoch 002:  35989 / 60421 loss=1.082, ppl=2.12, wps=16311.9, ups=4.4, wpb=3705.1, bsz=124, num_updates=96400, lr=0.00010185, gnorm=1.667, loss_scale=4, train_wall=23, gb_free=10.8, wall=29459
2021-05-06 00:48:28 | INFO | train_inner | epoch 002:  36089 / 60421 loss=1.07, ppl=2.1, wps=16498.6, ups=4.34, wpb=3800.3, bsz=126.4, num_updates=96500, lr=0.000101797, gnorm=1.316, loss_scale=4, train_wall=23, gb_free=11, wall=29482
2021-05-06 00:48:51 | INFO | train_inner | epoch 002:  36189 / 60421 loss=0.998, ppl=2, wps=16325.4, ups=4.29, wpb=3801.3, bsz=149.6, num_updates=96600, lr=0.000101745, gnorm=1.096, loss_scale=4, train_wall=23, gb_free=10.8, wall=29505
2021-05-06 00:49:14 | INFO | train_inner | epoch 002:  36289 / 60421 loss=1.067, ppl=2.09, wps=16162, ups=4.36, wpb=3706, bsz=125.1, num_updates=96700, lr=0.000101692, gnorm=1.479, loss_scale=4, train_wall=23, gb_free=10.9, wall=29528
2021-05-06 00:49:37 | INFO | train_inner | epoch 002:  36389 / 60421 loss=1.046, ppl=2.06, wps=16276.6, ups=4.32, wpb=3770.7, bsz=141.3, num_updates=96800, lr=0.000101639, gnorm=1.451, loss_scale=4, train_wall=23, gb_free=10.7, wall=29551
2021-05-06 00:50:01 | INFO | train_inner | epoch 002:  36489 / 60421 loss=1.094, ppl=2.14, wps=16245, ups=4.32, wpb=3759.1, bsz=125.3, num_updates=96900, lr=0.000101587, gnorm=1.408, loss_scale=4, train_wall=23, gb_free=10.7, wall=29574
2021-05-06 00:50:24 | INFO | train_inner | epoch 002:  36589 / 60421 loss=1.066, ppl=2.09, wps=15934.7, ups=4.26, wpb=3741.3, bsz=125.5, num_updates=97000, lr=0.000101535, gnorm=1.359, loss_scale=4, train_wall=23, gb_free=10.8, wall=29598
2021-05-06 00:50:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:50:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:50:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:50:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:50:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:50:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:50:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:50:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:50:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:50:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:51:30 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.691 | ppl 25.83 | bleu 29.02 | wps 2408.1 | wpb 2024.1 | bsz 97.5 | num_updates 97000 | best_bleu 29.03
2021-05-06 00:51:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 97000 updates
2021-05-06 00:51:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_97000.pt
2021-05-06 00:51:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_97000.pt
2021-05-06 00:51:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_97000.pt (epoch 2 @ 97000 updates, score 29.02) (writing took 10.237912642012816 seconds)
2021-05-06 00:52:03 | INFO | train_inner | epoch 002:  36689 / 60421 loss=1.04, ppl=2.06, wps=3794.8, ups=1.01, wpb=3756.3, bsz=134.6, num_updates=97100, lr=0.000101482, gnorm=1.179, loss_scale=4, train_wall=23, gb_free=11, wall=29697
2021-05-06 00:52:26 | INFO | train_inner | epoch 002:  36789 / 60421 loss=1.057, ppl=2.08, wps=16557.1, ups=4.37, wpb=3790.2, bsz=119.1, num_updates=97200, lr=0.00010143, gnorm=1.216, loss_scale=4, train_wall=23, gb_free=10.6, wall=29720
2021-05-06 00:52:48 | INFO | train_inner | epoch 002:  36889 / 60421 loss=1.142, ppl=2.21, wps=16412, ups=4.44, wpb=3693.3, bsz=120.5, num_updates=97300, lr=0.000101378, gnorm=1.65, loss_scale=4, train_wall=22, gb_free=10.8, wall=29742
2021-05-06 00:53:11 | INFO | train_inner | epoch 002:  36989 / 60421 loss=1.04, ppl=2.06, wps=16432.6, ups=4.39, wpb=3739.6, bsz=124.1, num_updates=97400, lr=0.000101326, gnorm=1.263, loss_scale=4, train_wall=23, gb_free=10.7, wall=29765
2021-05-06 00:53:34 | INFO | train_inner | epoch 002:  37089 / 60421 loss=1.103, ppl=2.15, wps=16309.2, ups=4.44, wpb=3676.2, bsz=118.3, num_updates=97500, lr=0.000101274, gnorm=1.668, loss_scale=4, train_wall=22, gb_free=10.7, wall=29788
2021-05-06 00:53:57 | INFO | train_inner | epoch 002:  37189 / 60421 loss=1.093, ppl=2.13, wps=16407.8, ups=4.35, wpb=3767.6, bsz=138.5, num_updates=97600, lr=0.000101222, gnorm=1.445, loss_scale=4, train_wall=23, gb_free=10.7, wall=29811
2021-05-06 00:54:20 | INFO | train_inner | epoch 002:  37289 / 60421 loss=1.02, ppl=2.03, wps=16275.6, ups=4.31, wpb=3773.3, bsz=163.8, num_updates=97700, lr=0.00010117, gnorm=1.323, loss_scale=4, train_wall=23, gb_free=10.7, wall=29834
2021-05-06 00:54:43 | INFO | train_inner | epoch 002:  37389 / 60421 loss=1.079, ppl=2.11, wps=15934.7, ups=4.37, wpb=3650.5, bsz=117.6, num_updates=97800, lr=0.000101118, gnorm=1.787, loss_scale=4, train_wall=23, gb_free=11, wall=29857
2021-05-06 00:55:06 | INFO | train_inner | epoch 002:  37489 / 60421 loss=1.02, ppl=2.03, wps=15954.7, ups=4.27, wpb=3737.7, bsz=139.9, num_updates=97900, lr=0.000101067, gnorm=1.382, loss_scale=4, train_wall=23, gb_free=10.7, wall=29880
2021-05-06 00:55:30 | INFO | train_inner | epoch 002:  37589 / 60421 loss=1.106, ppl=2.15, wps=15964.8, ups=4.25, wpb=3754.6, bsz=125.8, num_updates=98000, lr=0.000101015, gnorm=1.426, loss_scale=4, train_wall=23, gb_free=10.8, wall=29904
2021-05-06 00:55:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 00:55:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:55:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:55:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:55:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:55:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:55:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:55:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:55:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:55:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:55:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 00:55:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 00:55:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 00:56:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.718 | ppl 26.31 | bleu 29.05 | wps 2411.8 | wpb 2024.1 | bsz 97.5 | num_updates 98000 | best_bleu 29.05
2021-05-06 00:56:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 98000 updates
2021-05-06 00:56:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_98000.pt
2021-05-06 00:56:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_98000.pt
2021-05-06 00:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_98000.pt (epoch 2 @ 98000 updates, score 29.05) (writing took 14.603625705000013 seconds)
2021-05-06 00:57:13 | INFO | train_inner | epoch 002:  37689 / 60421 loss=1.025, ppl=2.04, wps=3586.5, ups=0.97, wpb=3693.4, bsz=142.2, num_updates=98100, lr=0.000100964, gnorm=1.753, loss_scale=4, train_wall=22, gb_free=10.9, wall=30007
2021-05-06 00:57:36 | INFO | train_inner | epoch 002:  37789 / 60421 loss=1.054, ppl=2.08, wps=16419.5, ups=4.38, wpb=3745.4, bsz=139.8, num_updates=98200, lr=0.000100912, gnorm=1.401, loss_scale=4, train_wall=23, gb_free=10.7, wall=30029
2021-05-06 00:57:58 | INFO | train_inner | epoch 002:  37889 / 60421 loss=1.04, ppl=2.06, wps=16601.9, ups=4.4, wpb=3770.9, bsz=128.3, num_updates=98300, lr=0.000100861, gnorm=1.135, loss_scale=4, train_wall=23, gb_free=11, wall=30052
2021-05-06 00:58:21 | INFO | train_inner | epoch 002:  37989 / 60421 loss=0.994, ppl=1.99, wps=16254.8, ups=4.38, wpb=3713.1, bsz=158.8, num_updates=98400, lr=0.00010081, gnorm=1.606, loss_scale=4, train_wall=23, gb_free=10.9, wall=30075
2021-05-06 00:58:44 | INFO | train_inner | epoch 002:  38089 / 60421 loss=1.058, ppl=2.08, wps=16434.9, ups=4.36, wpb=3773.2, bsz=133.3, num_updates=98500, lr=0.000100759, gnorm=1.377, loss_scale=4, train_wall=23, gb_free=11.1, wall=30098
2021-05-06 00:59:07 | INFO | train_inner | epoch 002:  38189 / 60421 loss=1.023, ppl=2.03, wps=16298.7, ups=4.35, wpb=3746.9, bsz=137.4, num_updates=98600, lr=0.000100707, gnorm=1.301, loss_scale=4, train_wall=23, gb_free=10.8, wall=30121
2021-05-06 00:59:30 | INFO | train_inner | epoch 002:  38289 / 60421 loss=1.084, ppl=2.12, wps=16423.6, ups=4.31, wpb=3809.1, bsz=142.6, num_updates=98700, lr=0.000100656, gnorm=1.213, loss_scale=4, train_wall=23, gb_free=10.7, wall=30144
2021-05-06 00:59:54 | INFO | train_inner | epoch 002:  38389 / 60421 loss=1.052, ppl=2.07, wps=16064, ups=4.28, wpb=3756.7, bsz=161.1, num_updates=98800, lr=0.000100605, gnorm=1.347, loss_scale=4, train_wall=23, gb_free=10.7, wall=30167
2021-05-06 01:00:17 | INFO | train_inner | epoch 002:  38489 / 60421 loss=1.05, ppl=2.07, wps=15971.2, ups=4.28, wpb=3732.4, bsz=135.1, num_updates=98900, lr=0.000100555, gnorm=1.333, loss_scale=4, train_wall=23, gb_free=10.8, wall=30191
2021-05-06 01:00:41 | INFO | train_inner | epoch 002:  38589 / 60421 loss=1.108, ppl=2.16, wps=15912.4, ups=4.19, wpb=3793.7, bsz=137.8, num_updates=99000, lr=0.000100504, gnorm=1.48, loss_scale=4, train_wall=24, gb_free=10.7, wall=30215
2021-05-06 01:00:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:00:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:00:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:00:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:00:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:00:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:00:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:01:46 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.703 | ppl 26.05 | bleu 28.86 | wps 2443.9 | wpb 2024.1 | bsz 97.5 | num_updates 99000 | best_bleu 29.05
2021-05-06 01:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 99000 updates
2021-05-06 01:01:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_99000.pt
2021-05-06 01:01:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_99000.pt
2021-05-06 01:01:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_99000.pt (epoch 2 @ 99000 updates, score 28.86) (writing took 9.038096684991615 seconds)
2021-05-06 01:02:18 | INFO | train_inner | epoch 002:  38689 / 60421 loss=1.042, ppl=2.06, wps=3897.2, ups=1.03, wpb=3775.8, bsz=142.7, num_updates=99100, lr=0.000100453, gnorm=1.523, loss_scale=4, train_wall=23, gb_free=10.6, wall=30312
2021-05-06 01:02:40 | INFO | train_inner | epoch 002:  38789 / 60421 loss=1.098, ppl=2.14, wps=16426.5, ups=4.43, wpb=3708.7, bsz=121.8, num_updates=99200, lr=0.000100402, gnorm=1.713, loss_scale=4, train_wall=22, gb_free=10.8, wall=30334
2021-05-06 01:03:03 | INFO | train_inner | epoch 002:  38889 / 60421 loss=1.04, ppl=2.06, wps=16504.6, ups=4.39, wpb=3759.3, bsz=128.1, num_updates=99300, lr=0.000100352, gnorm=1.196, loss_scale=4, train_wall=23, gb_free=10.9, wall=30357
2021-05-06 01:03:26 | INFO | train_inner | epoch 002:  38989 / 60421 loss=1.045, ppl=2.06, wps=16348.9, ups=4.42, wpb=3696.3, bsz=129, num_updates=99400, lr=0.000100301, gnorm=1.614, loss_scale=4, train_wall=22, gb_free=10.8, wall=30380
2021-05-06 01:03:48 | INFO | train_inner | epoch 002:  39089 / 60421 loss=1.065, ppl=2.09, wps=16238.2, ups=4.4, wpb=3691.5, bsz=111.3, num_updates=99500, lr=0.000100251, gnorm=1.41, loss_scale=4, train_wall=23, gb_free=10.9, wall=30402
2021-05-06 01:04:12 | INFO | train_inner | epoch 002:  39189 / 60421 loss=1.023, ppl=2.03, wps=16272.2, ups=4.31, wpb=3779, bsz=135.7, num_updates=99600, lr=0.000100201, gnorm=1.254, loss_scale=4, train_wall=23, gb_free=10.8, wall=30425
2021-05-06 01:04:35 | INFO | train_inner | epoch 002:  39289 / 60421 loss=1.038, ppl=2.05, wps=16339.4, ups=4.33, wpb=3769.9, bsz=134.2, num_updates=99700, lr=0.00010015, gnorm=1.122, loss_scale=4, train_wall=23, gb_free=10.8, wall=30449
2021-05-06 01:04:58 | INFO | train_inner | epoch 002:  39389 / 60421 loss=0.979, ppl=1.97, wps=16069.7, ups=4.29, wpb=3745.5, bsz=144.6, num_updates=99800, lr=0.0001001, gnorm=1.163, loss_scale=4, train_wall=23, gb_free=10.7, wall=30472
2021-05-06 01:05:22 | INFO | train_inner | epoch 002:  39489 / 60421 loss=1.006, ppl=2.01, wps=15985.2, ups=4.23, wpb=3774.8, bsz=136, num_updates=99900, lr=0.00010005, gnorm=1.117, loss_scale=4, train_wall=23, gb_free=10.8, wall=30495
2021-05-06 01:05:46 | INFO | train_inner | epoch 002:  39589 / 60421 loss=0.983, ppl=1.98, wps=15900.7, ups=4.19, wpb=3796.7, bsz=155.1, num_updates=100000, lr=0.0001, gnorm=1.078, loss_scale=4, train_wall=24, gb_free=10.7, wall=30519
2021-05-06 01:05:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:05:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:05:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:05:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:05:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:05:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:05:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:06:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:06:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:06:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:06:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:06:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:06:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:06:50 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.714 | ppl 26.25 | bleu 29.12 | wps 2442.2 | wpb 2024.1 | bsz 97.5 | num_updates 100000 | best_bleu 29.12
2021-05-06 01:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 100000 updates
2021-05-06 01:06:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_100000.pt
2021-05-06 01:06:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_100000.pt
2021-05-06 01:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_100000.pt (epoch 2 @ 100000 updates, score 29.12) (writing took 14.635507939005038 seconds)
2021-05-06 01:07:28 | INFO | train_inner | epoch 002:  39689 / 60421 loss=1.064, ppl=2.09, wps=3622.7, ups=0.98, wpb=3700.9, bsz=110.9, num_updates=100100, lr=9.995e-05, gnorm=1.387, loss_scale=4, train_wall=22, gb_free=10.8, wall=30622
2021-05-06 01:07:50 | INFO | train_inner | epoch 002:  39789 / 60421 loss=1.045, ppl=2.06, wps=16515.7, ups=4.44, wpb=3722.6, bsz=137.7, num_updates=100200, lr=9.99001e-05, gnorm=1.418, loss_scale=4, train_wall=22, gb_free=10.7, wall=30644
2021-05-06 01:08:13 | INFO | train_inner | epoch 002:  39889 / 60421 loss=0.997, ppl=2, wps=16322.5, ups=4.41, wpb=3702.2, bsz=152.2, num_updates=100300, lr=9.98503e-05, gnorm=1.348, loss_scale=4, train_wall=22, gb_free=10.8, wall=30667
2021-05-06 01:08:36 | INFO | train_inner | epoch 002:  39989 / 60421 loss=0.969, ppl=1.96, wps=16246.9, ups=4.41, wpb=3686, bsz=150.2, num_updates=100400, lr=9.98006e-05, gnorm=1.369, loss_scale=4, train_wall=22, gb_free=11.1, wall=30689
2021-05-06 01:08:59 | INFO | train_inner | epoch 002:  40089 / 60421 loss=1.059, ppl=2.08, wps=16554.1, ups=4.34, wpb=3810.8, bsz=132.4, num_updates=100500, lr=9.97509e-05, gnorm=1.139, loss_scale=4, train_wall=23, gb_free=10.8, wall=30712
2021-05-06 01:09:21 | INFO | train_inner | epoch 002:  40189 / 60421 loss=1.088, ppl=2.13, wps=16220, ups=4.39, wpb=3692.5, bsz=131.4, num_updates=100600, lr=9.97013e-05, gnorm=1.633, loss_scale=4, train_wall=23, gb_free=10.7, wall=30735
2021-05-06 01:09:45 | INFO | train_inner | epoch 002:  40289 / 60421 loss=1.049, ppl=2.07, wps=16256, ups=4.31, wpb=3769.2, bsz=146.5, num_updates=100700, lr=9.96518e-05, gnorm=1.316, loss_scale=4, train_wall=23, gb_free=10.7, wall=30758
2021-05-06 01:10:08 | INFO | train_inner | epoch 002:  40389 / 60421 loss=1.026, ppl=2.04, wps=15975.4, ups=4.27, wpb=3741.8, bsz=148.4, num_updates=100800, lr=9.96024e-05, gnorm=1.67, loss_scale=4, train_wall=23, gb_free=10.9, wall=30782
2021-05-06 01:10:31 | INFO | train_inner | epoch 002:  40489 / 60421 loss=1.066, ppl=2.09, wps=15861.7, ups=4.27, wpb=3713.7, bsz=119.1, num_updates=100900, lr=9.9553e-05, gnorm=1.357, loss_scale=4, train_wall=23, gb_free=11, wall=30805
2021-05-06 01:10:55 | INFO | train_inner | epoch 002:  40589 / 60421 loss=1.104, ppl=2.15, wps=15680.7, ups=4.24, wpb=3699, bsz=121.5, num_updates=101000, lr=9.95037e-05, gnorm=1.362, loss_scale=4, train_wall=23, gb_free=10.8, wall=30829
2021-05-06 01:10:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:11:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:11:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:11:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:11:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:11:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:11:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:11:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:11:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:11:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:11:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:11:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:11:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:11:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:11:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:11:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:11:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.719 | ppl 26.33 | bleu 28.76 | wps 2505.1 | wpb 2024.1 | bsz 97.5 | num_updates 101000 | best_bleu 29.12
2021-05-06 01:11:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 101000 updates
2021-05-06 01:11:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_101000.pt
2021-05-06 01:12:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_101000.pt
2021-05-06 01:12:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_101000.pt (epoch 2 @ 101000 updates, score 28.76) (writing took 8.211349052988226 seconds)
2021-05-06 01:12:29 | INFO | train_inner | epoch 002:  40689 / 60421 loss=0.992, ppl=1.99, wps=3953.6, ups=1.06, wpb=3731, bsz=150.8, num_updates=101100, lr=9.94545e-05, gnorm=1.643, loss_scale=4, train_wall=23, gb_free=10.9, wall=30923
2021-05-06 01:12:52 | INFO | train_inner | epoch 002:  40789 / 60421 loss=1.101, ppl=2.14, wps=16477.8, ups=4.4, wpb=3748.8, bsz=119.1, num_updates=101200, lr=9.94053e-05, gnorm=1.476, loss_scale=4, train_wall=23, gb_free=10.8, wall=30946
2021-05-06 01:13:15 | INFO | train_inner | epoch 002:  40889 / 60421 loss=1.039, ppl=2.05, wps=16609.3, ups=4.38, wpb=3791.9, bsz=130.6, num_updates=101300, lr=9.93563e-05, gnorm=1.145, loss_scale=4, train_wall=23, gb_free=10.7, wall=30969
2021-05-06 01:13:37 | INFO | train_inner | epoch 002:  40989 / 60421 loss=1.031, ppl=2.04, wps=16285.6, ups=4.43, wpb=3673.3, bsz=127.3, num_updates=101400, lr=9.93073e-05, gnorm=1.744, loss_scale=4, train_wall=22, gb_free=11.1, wall=30991
2021-05-06 01:14:00 | INFO | train_inner | epoch 002:  41089 / 60421 loss=1.101, ppl=2.15, wps=16403, ups=4.4, wpb=3727.9, bsz=116.5, num_updates=101500, lr=9.92583e-05, gnorm=1.313, loss_scale=4, train_wall=23, gb_free=10.7, wall=31014
2021-05-06 01:14:23 | INFO | train_inner | epoch 002:  41189 / 60421 loss=1.007, ppl=2.01, wps=16104.8, ups=4.38, wpb=3679, bsz=141.4, num_updates=101600, lr=9.92095e-05, gnorm=1.442, loss_scale=4, train_wall=23, gb_free=11, wall=31037
2021-05-06 01:14:46 | INFO | train_inner | epoch 002:  41289 / 60421 loss=1.038, ppl=2.05, wps=16245.2, ups=4.31, wpb=3770.7, bsz=142.6, num_updates=101700, lr=9.91607e-05, gnorm=1.31, loss_scale=4, train_wall=23, gb_free=10.8, wall=31060
2021-05-06 01:15:10 | INFO | train_inner | epoch 002:  41389 / 60421 loss=1.032, ppl=2.05, wps=16086.8, ups=4.29, wpb=3752.9, bsz=134.3, num_updates=101800, lr=9.9112e-05, gnorm=1.179, loss_scale=4, train_wall=23, gb_free=10.8, wall=31083
2021-05-06 01:15:33 | INFO | train_inner | epoch 002:  41489 / 60421 loss=1.064, ppl=2.09, wps=15956.7, ups=4.23, wpb=3771.3, bsz=119.9, num_updates=101900, lr=9.90633e-05, gnorm=1.229, loss_scale=4, train_wall=23, gb_free=10.8, wall=31107
2021-05-06 01:15:57 | INFO | train_inner | epoch 002:  41589 / 60421 loss=1.056, ppl=2.08, wps=15702.3, ups=4.16, wpb=3773.4, bsz=128.8, num_updates=102000, lr=9.90148e-05, gnorm=1.391, loss_scale=4, train_wall=24, gb_free=11.3, wall=31131
2021-05-06 01:15:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:16:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:16:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:16:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:16:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:16:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:16:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:16:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:16:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:16:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:16:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:16:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:16:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:17:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.725 | ppl 26.45 | bleu 28.96 | wps 2468.3 | wpb 2024.1 | bsz 97.5 | num_updates 102000 | best_bleu 29.12
2021-05-06 01:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 102000 updates
2021-05-06 01:17:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_102000.pt
2021-05-06 01:17:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_102000.pt
2021-05-06 01:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_102000.pt (epoch 2 @ 102000 updates, score 28.96) (writing took 10.1559414640069 seconds)
2021-05-06 01:17:34 | INFO | train_inner | epoch 002:  41689 / 60421 loss=1.129, ppl=2.19, wps=3850.1, ups=1.03, wpb=3733.7, bsz=137, num_updates=102100, lr=9.89663e-05, gnorm=1.591, loss_scale=4, train_wall=22, gb_free=10.8, wall=31228
2021-05-06 01:17:57 | INFO | train_inner | epoch 002:  41789 / 60421 loss=1.095, ppl=2.14, wps=16401.5, ups=4.48, wpb=3660.6, bsz=118.4, num_updates=102200, lr=9.89178e-05, gnorm=1.913, loss_scale=4, train_wall=22, gb_free=10.7, wall=31250
2021-05-06 01:18:19 | INFO | train_inner | epoch 002:  41889 / 60421 loss=1.043, ppl=2.06, wps=16526.5, ups=4.42, wpb=3739.5, bsz=132.2, num_updates=102300, lr=9.88695e-05, gnorm=1.407, loss_scale=4, train_wall=22, gb_free=10.8, wall=31273
2021-05-06 01:18:42 | INFO | train_inner | epoch 002:  41989 / 60421 loss=1.046, ppl=2.06, wps=16637.1, ups=4.39, wpb=3788.2, bsz=135.1, num_updates=102400, lr=9.88212e-05, gnorm=1.117, loss_scale=4, train_wall=23, gb_free=10.8, wall=31296
2021-05-06 01:19:05 | INFO | train_inner | epoch 002:  42089 / 60421 loss=1.075, ppl=2.11, wps=16637.1, ups=4.4, wpb=3777.8, bsz=134.9, num_updates=102500, lr=9.8773e-05, gnorm=1.284, loss_scale=4, train_wall=23, gb_free=10.7, wall=31319
2021-05-06 01:19:27 | INFO | train_inner | epoch 002:  42189 / 60421 loss=1.043, ppl=2.06, wps=16596.5, ups=4.41, wpb=3761.8, bsz=139, num_updates=102600, lr=9.87248e-05, gnorm=1.171, loss_scale=4, train_wall=22, gb_free=10.7, wall=31341
2021-05-06 01:19:50 | INFO | train_inner | epoch 002:  42289 / 60421 loss=1.028, ppl=2.04, wps=16649.3, ups=4.37, wpb=3809.1, bsz=131, num_updates=102700, lr=9.86767e-05, gnorm=1.177, loss_scale=4, train_wall=23, gb_free=10.9, wall=31364
2021-05-06 01:20:13 | INFO | train_inner | epoch 002:  42389 / 60421 loss=1.025, ppl=2.04, wps=16562.6, ups=4.38, wpb=3783.2, bsz=124.5, num_updates=102800, lr=9.86287e-05, gnorm=1.169, loss_scale=4, train_wall=23, gb_free=10.7, wall=31387
2021-05-06 01:20:36 | INFO | train_inner | epoch 002:  42489 / 60421 loss=1.02, ppl=2.03, wps=16612.9, ups=4.4, wpb=3773.8, bsz=137, num_updates=102900, lr=9.85808e-05, gnorm=1.266, loss_scale=4, train_wall=23, gb_free=10.7, wall=31410
2021-05-06 01:20:58 | INFO | train_inner | epoch 002:  42589 / 60421 loss=1.006, ppl=2.01, wps=16573.2, ups=4.42, wpb=3750.5, bsz=146.9, num_updates=103000, lr=9.85329e-05, gnorm=1.232, loss_scale=4, train_wall=22, gb_free=11, wall=31432
2021-05-06 01:20:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:21:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:21:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:21:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:21:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:21:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:21:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:21:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:21:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:21:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:21:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:21:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:21:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:21:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:21:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:21:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:22:02 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.746 | ppl 26.83 | bleu 28.92 | wps 2492.7 | wpb 2024.1 | bsz 97.5 | num_updates 103000 | best_bleu 29.12
2021-05-06 01:22:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 103000 updates
2021-05-06 01:22:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_103000.pt
2021-05-06 01:22:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_103000.pt
2021-05-06 01:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_103000.pt (epoch 2 @ 103000 updates, score 28.92) (writing took 8.30338037700858 seconds)
2021-05-06 01:22:33 | INFO | train_inner | epoch 002:  42689 / 60421 loss=1.065, ppl=2.09, wps=3875.8, ups=1.06, wpb=3657.8, bsz=118.6, num_updates=103100, lr=9.84851e-05, gnorm=1.711, loss_scale=4, train_wall=22, gb_free=10.8, wall=31527
2021-05-06 01:22:55 | INFO | train_inner | epoch 002:  42789 / 60421 loss=1.007, ppl=2.01, wps=16471.9, ups=4.43, wpb=3714.7, bsz=147.6, num_updates=103200, lr=9.84374e-05, gnorm=1.569, loss_scale=4, train_wall=22, gb_free=10.8, wall=31549
2021-05-06 01:23:18 | INFO | train_inner | epoch 002:  42889 / 60421 loss=1, ppl=2, wps=16404.8, ups=4.43, wpb=3705.8, bsz=127.4, num_updates=103300, lr=9.83897e-05, gnorm=1.275, loss_scale=4, train_wall=22, gb_free=10.7, wall=31572
2021-05-06 01:23:41 | INFO | train_inner | epoch 002:  42989 / 60421 loss=1.057, ppl=2.08, wps=16250.7, ups=4.39, wpb=3698, bsz=139.8, num_updates=103400, lr=9.83422e-05, gnorm=1.407, loss_scale=4, train_wall=23, gb_free=10.7, wall=31595
2021-05-06 01:24:04 | INFO | train_inner | epoch 002:  43089 / 60421 loss=1.028, ppl=2.04, wps=16377.8, ups=4.33, wpb=3778.8, bsz=132.5, num_updates=103500, lr=9.82946e-05, gnorm=1.182, loss_scale=4, train_wall=23, gb_free=11.1, wall=31618
2021-05-06 01:24:27 | INFO | train_inner | epoch 002:  43189 / 60421 loss=1.025, ppl=2.03, wps=16096.3, ups=4.32, wpb=3728, bsz=144.3, num_updates=103600, lr=9.82472e-05, gnorm=1.429, loss_scale=4, train_wall=23, gb_free=10.8, wall=31641
2021-05-06 01:24:50 | INFO | train_inner | epoch 002:  43289 / 60421 loss=0.987, ppl=1.98, wps=16198.2, ups=4.26, wpb=3800.9, bsz=138.6, num_updates=103700, lr=9.81998e-05, gnorm=1.202, loss_scale=4, train_wall=23, gb_free=10.9, wall=31664
2021-05-06 01:25:14 | INFO | train_inner | epoch 002:  43389 / 60421 loss=1.012, ppl=2.02, wps=15986.9, ups=4.28, wpb=3731.2, bsz=129, num_updates=103800, lr=9.81525e-05, gnorm=1.322, loss_scale=4, train_wall=23, gb_free=10.7, wall=31688
2021-05-06 01:25:37 | INFO | train_inner | epoch 002:  43489 / 60421 loss=1.007, ppl=2.01, wps=15774.7, ups=4.23, wpb=3733.3, bsz=131.4, num_updates=103900, lr=9.81052e-05, gnorm=1.247, loss_scale=4, train_wall=23, gb_free=11.3, wall=31711
2021-05-06 01:26:01 | INFO | train_inner | epoch 002:  43589 / 60421 loss=1.105, ppl=2.15, wps=15613, ups=4.29, wpb=3638.9, bsz=116.8, num_updates=104000, lr=9.80581e-05, gnorm=1.794, loss_scale=4, train_wall=23, gb_free=10.8, wall=31735
2021-05-06 01:26:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:26:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:26:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:26:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:26:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:26:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:26:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:26:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:26:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:26:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:26:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:26:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:26:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:26:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:26:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:26:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:27:05 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.749 | ppl 26.88 | bleu 28.35 | wps 2454.8 | wpb 2024.1 | bsz 97.5 | num_updates 104000 | best_bleu 29.12
2021-05-06 01:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 104000 updates
2021-05-06 01:27:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_104000.pt
2021-05-06 01:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_104000.pt
2021-05-06 01:27:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_104000.pt (epoch 2 @ 104000 updates, score 28.35) (writing took 8.393209169997135 seconds)
2021-05-06 01:27:37 | INFO | train_inner | epoch 002:  43689 / 60421 loss=1.098, ppl=2.14, wps=3919.4, ups=1.04, wpb=3767.4, bsz=144.1, num_updates=104100, lr=9.8011e-05, gnorm=1.648, loss_scale=4, train_wall=23, gb_free=10.9, wall=31831
2021-05-06 01:27:59 | INFO | train_inner | epoch 002:  43789 / 60421 loss=1.002, ppl=2, wps=16220.3, ups=4.45, wpb=3642, bsz=141.8, num_updates=104200, lr=9.79639e-05, gnorm=1.473, loss_scale=4, train_wall=22, gb_free=11.3, wall=31853
2021-05-06 01:28:22 | INFO | train_inner | epoch 002:  43889 / 60421 loss=1.027, ppl=2.04, wps=16470.6, ups=4.38, wpb=3761.8, bsz=139.8, num_updates=104300, lr=9.79169e-05, gnorm=1.301, loss_scale=4, train_wall=23, gb_free=10.9, wall=31876
2021-05-06 01:28:45 | INFO | train_inner | epoch 002:  43989 / 60421 loss=1.055, ppl=2.08, wps=16247, ups=4.47, wpb=3638, bsz=121.1, num_updates=104400, lr=9.787e-05, gnorm=1.445, loss_scale=4, train_wall=22, gb_free=11, wall=31898
2021-05-06 01:29:08 | INFO | train_inner | epoch 002:  44089 / 60421 loss=1.15, ppl=2.22, wps=16390.4, ups=4.35, wpb=3764.8, bsz=117.9, num_updates=104500, lr=9.78232e-05, gnorm=1.414, loss_scale=4, train_wall=23, gb_free=10.8, wall=31921
2021-05-06 01:29:31 | INFO | train_inner | epoch 002:  44189 / 60421 loss=1.082, ppl=2.12, wps=16260.8, ups=4.34, wpb=3748.5, bsz=128.9, num_updates=104600, lr=9.77764e-05, gnorm=1.332, loss_scale=4, train_wall=23, gb_free=10.7, wall=31944
2021-05-06 01:29:54 | INFO | train_inner | epoch 002:  44289 / 60421 loss=1.043, ppl=2.06, wps=16218.3, ups=4.33, wpb=3742.2, bsz=119.6, num_updates=104700, lr=9.77297e-05, gnorm=1.052, loss_scale=4, train_wall=23, gb_free=11.1, wall=31967
2021-05-06 01:30:17 | INFO | train_inner | epoch 002:  44389 / 60421 loss=1.056, ppl=2.08, wps=16204.8, ups=4.28, wpb=3786.9, bsz=134.4, num_updates=104800, lr=9.76831e-05, gnorm=1.28, loss_scale=4, train_wall=23, gb_free=10.9, wall=31991
2021-05-06 01:30:41 | INFO | train_inner | epoch 002:  44489 / 60421 loss=1.073, ppl=2.1, wps=15708.6, ups=4.26, wpb=3691.3, bsz=122, num_updates=104900, lr=9.76365e-05, gnorm=1.512, loss_scale=4, train_wall=23, gb_free=11.3, wall=32014
2021-05-06 01:31:04 | INFO | train_inner | epoch 002:  44589 / 60421 loss=1.025, ppl=2.04, wps=15670.2, ups=4.24, wpb=3699.4, bsz=144.4, num_updates=105000, lr=9.759e-05, gnorm=1.5, loss_scale=4, train_wall=23, gb_free=11.3, wall=32038
2021-05-06 01:31:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:31:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:31:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:31:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:31:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:31:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:31:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:31:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:31:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:31:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:32:08 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.616 | ppl 24.53 | bleu 28.65 | wps 2461.6 | wpb 2024.1 | bsz 97.5 | num_updates 105000 | best_bleu 29.12
2021-05-06 01:32:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 105000 updates
2021-05-06 01:32:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_105000.pt
2021-05-06 01:32:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_105000.pt
2021-05-06 01:32:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_105000.pt (epoch 2 @ 105000 updates, score 28.65) (writing took 8.502140958997188 seconds)
2021-05-06 01:32:40 | INFO | train_inner | epoch 002:  44689 / 60421 loss=1.033, ppl=2.05, wps=3867.3, ups=1.05, wpb=3694.9, bsz=137.8, num_updates=105100, lr=9.75436e-05, gnorm=1.446, loss_scale=4, train_wall=22, gb_free=10.8, wall=32134
2021-05-06 01:33:02 | INFO | train_inner | epoch 002:  44789 / 60421 loss=0.999, ppl=2, wps=16489.9, ups=4.41, wpb=3738.9, bsz=139.8, num_updates=105200, lr=9.74972e-05, gnorm=1.149, loss_scale=4, train_wall=22, gb_free=10.7, wall=32156
2021-05-06 01:33:25 | INFO | train_inner | epoch 002:  44889 / 60421 loss=1.066, ppl=2.09, wps=16505.7, ups=4.39, wpb=3757.3, bsz=123.4, num_updates=105300, lr=9.74509e-05, gnorm=1.299, loss_scale=4, train_wall=23, gb_free=10.8, wall=32179
2021-05-06 01:33:48 | INFO | train_inner | epoch 002:  44989 / 60421 loss=1.009, ppl=2.01, wps=16264.1, ups=4.4, wpb=3693.5, bsz=132.2, num_updates=105400, lr=9.74047e-05, gnorm=1.315, loss_scale=4, train_wall=23, gb_free=11, wall=32202
2021-05-06 01:34:11 | INFO | train_inner | epoch 002:  45089 / 60421 loss=1, ppl=2, wps=16285.3, ups=4.4, wpb=3704, bsz=140.6, num_updates=105500, lr=9.73585e-05, gnorm=1.079, loss_scale=4, train_wall=23, gb_free=10.8, wall=32224
2021-05-06 01:34:34 | INFO | train_inner | epoch 002:  45189 / 60421 loss=1.05, ppl=2.07, wps=16207.2, ups=4.33, wpb=3739.8, bsz=125.4, num_updates=105600, lr=9.73124e-05, gnorm=1.332, loss_scale=4, train_wall=23, gb_free=11.1, wall=32247
2021-05-06 01:34:57 | INFO | train_inner | epoch 002:  45289 / 60421 loss=1.015, ppl=2.02, wps=16170.1, ups=4.28, wpb=3775.2, bsz=139.8, num_updates=105700, lr=9.72663e-05, gnorm=1.235, loss_scale=4, train_wall=23, gb_free=10.9, wall=32271
2021-05-06 01:35:21 | INFO | train_inner | epoch 002:  45389 / 60421 loss=1.032, ppl=2.04, wps=16055.3, ups=4.25, wpb=3782, bsz=113.4, num_updates=105800, lr=9.72203e-05, gnorm=1.135, loss_scale=4, train_wall=23, gb_free=10.9, wall=32294
2021-05-06 01:35:44 | INFO | train_inner | epoch 002:  45489 / 60421 loss=1.02, ppl=2.03, wps=15886, ups=4.2, wpb=3779.1, bsz=148.6, num_updates=105900, lr=9.71744e-05, gnorm=1.268, loss_scale=4, train_wall=24, gb_free=10.7, wall=32318
2021-05-06 01:36:08 | INFO | train_inner | epoch 002:  45589 / 60421 loss=0.972, ppl=1.96, wps=15562.9, ups=4.23, wpb=3679.6, bsz=145.9, num_updates=106000, lr=9.71286e-05, gnorm=1.608, loss_scale=4, train_wall=23, gb_free=10.8, wall=32342
2021-05-06 01:36:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:36:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:36:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:36:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:36:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:36:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:36:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:36:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:36:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:36:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:37:12 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.798 | ppl 27.81 | bleu 28.51 | wps 2456.5 | wpb 2024.1 | bsz 97.5 | num_updates 106000 | best_bleu 29.12
2021-05-06 01:37:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 106000 updates
2021-05-06 01:37:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_106000.pt
2021-05-06 01:37:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_106000.pt
2021-05-06 01:37:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_106000.pt (epoch 2 @ 106000 updates, score 28.51) (writing took 8.500553649995709 seconds)
2021-05-06 01:37:44 | INFO | train_inner | epoch 002:  45689 / 60421 loss=1.019, ppl=2.03, wps=3874.8, ups=1.05, wpb=3707.6, bsz=125, num_updates=106100, lr=9.70828e-05, gnorm=1.235, loss_scale=4, train_wall=22, gb_free=10.8, wall=32438
2021-05-06 01:38:06 | INFO | train_inner | epoch 002:  45789 / 60421 loss=1.056, ppl=2.08, wps=16273.5, ups=4.39, wpb=3703.6, bsz=137.9, num_updates=106200, lr=9.70371e-05, gnorm=1.551, loss_scale=4, train_wall=23, gb_free=10.8, wall=32460
2021-05-06 01:38:29 | INFO | train_inner | epoch 002:  45889 / 60421 loss=1.049, ppl=2.07, wps=16285.1, ups=4.38, wpb=3718.7, bsz=122.2, num_updates=106300, lr=9.69914e-05, gnorm=1.424, loss_scale=4, train_wall=23, gb_free=10.9, wall=32483
2021-05-06 01:38:52 | INFO | train_inner | epoch 002:  45989 / 60421 loss=1.088, ppl=2.13, wps=16206.8, ups=4.4, wpb=3683.6, bsz=129, num_updates=106400, lr=9.69458e-05, gnorm=1.5, loss_scale=4, train_wall=23, gb_free=11.2, wall=32506
2021-05-06 01:39:15 | INFO | train_inner | epoch 002:  46089 / 60421 loss=1.041, ppl=2.06, wps=16274, ups=4.36, wpb=3731.2, bsz=142.5, num_updates=106500, lr=9.69003e-05, gnorm=1.411, loss_scale=4, train_wall=23, gb_free=10.8, wall=32529
2021-05-06 01:39:38 | INFO | train_inner | epoch 002:  46189 / 60421 loss=1.103, ppl=2.15, wps=16160.6, ups=4.36, wpb=3706.7, bsz=107.7, num_updates=106600, lr=9.68549e-05, gnorm=1.531, loss_scale=4, train_wall=23, gb_free=10.9, wall=32552
2021-05-06 01:40:01 | INFO | train_inner | epoch 002:  46289 / 60421 loss=1.036, ppl=2.05, wps=15841.1, ups=4.32, wpb=3665.5, bsz=123.4, num_updates=106700, lr=9.68095e-05, gnorm=1.383, loss_scale=8, train_wall=23, gb_free=10.9, wall=32575
2021-05-06 01:40:24 | INFO | train_inner | epoch 002:  46389 / 60421 loss=1.015, ppl=2.02, wps=15906.6, ups=4.27, wpb=3729.3, bsz=124.7, num_updates=106800, lr=9.67641e-05, gnorm=1.104, loss_scale=8, train_wall=23, gb_free=10.9, wall=32598
2021-05-06 01:40:48 | INFO | train_inner | epoch 002:  46489 / 60421 loss=1.036, ppl=2.05, wps=15560.2, ups=4.29, wpb=3624.3, bsz=136, num_updates=106900, lr=9.67189e-05, gnorm=1.773, loss_scale=8, train_wall=23, gb_free=10.7, wall=32622
2021-05-06 01:41:12 | INFO | train_inner | epoch 002:  46589 / 60421 loss=1.126, ppl=2.18, wps=15964.6, ups=4.19, wpb=3811.2, bsz=115.4, num_updates=107000, lr=9.66736e-05, gnorm=1.414, loss_scale=8, train_wall=24, gb_free=10.7, wall=32645
2021-05-06 01:41:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:41:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:41:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:41:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:41:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:41:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:41:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:41:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:41:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:41:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:41:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:41:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:41:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:41:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:41:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:41:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:41:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:41:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:41:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:42:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.776 | ppl 27.39 | bleu 28.91 | wps 2472.1 | wpb 2024.1 | bsz 97.5 | num_updates 107000 | best_bleu 29.12
2021-05-06 01:42:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 107000 updates
2021-05-06 01:42:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_107000.pt
2021-05-06 01:42:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_107000.pt
2021-05-06 01:42:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_107000.pt (epoch 2 @ 107000 updates, score 28.91) (writing took 8.53238820200204 seconds)
2021-05-06 01:42:47 | INFO | train_inner | epoch 002:  46689 / 60421 loss=1.023, ppl=2.03, wps=3949.1, ups=1.04, wpb=3779.1, bsz=140, num_updates=107100, lr=9.66285e-05, gnorm=1.268, loss_scale=8, train_wall=23, gb_free=10.8, wall=32741
2021-05-06 01:43:10 | INFO | train_inner | epoch 002:  46789 / 60421 loss=1.024, ppl=2.03, wps=16272.4, ups=4.43, wpb=3675.2, bsz=130.6, num_updates=107200, lr=9.65834e-05, gnorm=1.474, loss_scale=8, train_wall=22, gb_free=11, wall=32764
2021-05-06 01:43:33 | INFO | train_inner | epoch 002:  46889 / 60421 loss=0.978, ppl=1.97, wps=16118.7, ups=4.41, wpb=3655.4, bsz=144.8, num_updates=107300, lr=9.65384e-05, gnorm=1.426, loss_scale=8, train_wall=22, gb_free=11.1, wall=32786
2021-05-06 01:43:56 | INFO | train_inner | epoch 002:  46989 / 60421 loss=1.062, ppl=2.09, wps=16423.9, ups=4.34, wpb=3783.6, bsz=130, num_updates=107400, lr=9.64935e-05, gnorm=1.343, loss_scale=8, train_wall=23, gb_free=10.8, wall=32809
2021-05-06 01:44:19 | INFO | train_inner | epoch 002:  47089 / 60421 loss=1.074, ppl=2.11, wps=16240.2, ups=4.36, wpb=3726.4, bsz=120.2, num_updates=107500, lr=9.64486e-05, gnorm=1.521, loss_scale=8, train_wall=23, gb_free=10.9, wall=32832
2021-05-06 01:44:42 | INFO | train_inner | epoch 002:  47189 / 60421 loss=0.995, ppl=1.99, wps=16335.6, ups=4.34, wpb=3766.8, bsz=136.3, num_updates=107600, lr=9.64037e-05, gnorm=1.208, loss_scale=8, train_wall=23, gb_free=10.8, wall=32855
2021-05-06 01:45:05 | INFO | train_inner | epoch 002:  47289 / 60421 loss=1.022, ppl=2.03, wps=16079.2, ups=4.3, wpb=3735.2, bsz=124.7, num_updates=107700, lr=9.6359e-05, gnorm=1.31, loss_scale=8, train_wall=23, gb_free=11.3, wall=32879
2021-05-06 01:45:28 | INFO | train_inner | epoch 002:  47389 / 60421 loss=1.071, ppl=2.1, wps=16037.5, ups=4.25, wpb=3772.9, bsz=119.8, num_updates=107800, lr=9.63143e-05, gnorm=1.213, loss_scale=8, train_wall=23, gb_free=11.5, wall=32902
2021-05-06 01:45:52 | INFO | train_inner | epoch 002:  47489 / 60421 loss=1.01, ppl=2.01, wps=15677.1, ups=4.2, wpb=3733.6, bsz=155.3, num_updates=107900, lr=9.62696e-05, gnorm=1.426, loss_scale=8, train_wall=24, gb_free=10.8, wall=32926
2021-05-06 01:46:16 | INFO | train_inner | epoch 002:  47589 / 60421 loss=0.994, ppl=1.99, wps=15915.8, ups=4.25, wpb=3748, bsz=128.1, num_updates=108000, lr=9.6225e-05, gnorm=1.166, loss_scale=8, train_wall=23, gb_free=10.8, wall=32950
2021-05-06 01:46:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:46:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:46:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:46:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:47:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.792 | ppl 27.7 | bleu 29.19 | wps 2429 | wpb 2024.1 | bsz 97.5 | num_updates 108000 | best_bleu 29.19
2021-05-06 01:47:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 108000 updates
2021-05-06 01:47:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_108000.pt
2021-05-06 01:47:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_108000.pt
2021-05-06 01:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_108000.pt (epoch 2 @ 108000 updates, score 29.19) (writing took 14.62871218501823 seconds)
2021-05-06 01:47:58 | INFO | train_inner | epoch 002:  47689 / 60421 loss=1.06, ppl=2.09, wps=3666.6, ups=0.97, wpb=3764.4, bsz=131.6, num_updates=108100, lr=9.61805e-05, gnorm=1.258, loss_scale=8, train_wall=22, gb_free=10.8, wall=33052
2021-05-06 01:48:21 | INFO | train_inner | epoch 002:  47789 / 60421 loss=0.988, ppl=1.98, wps=16395.2, ups=4.44, wpb=3693.8, bsz=152.8, num_updates=108200, lr=9.61361e-05, gnorm=1.381, loss_scale=8, train_wall=22, gb_free=10.8, wall=33075
2021-05-06 01:48:44 | INFO | train_inner | epoch 002:  47889 / 60421 loss=1.048, ppl=2.07, wps=16250.7, ups=4.4, wpb=3690.7, bsz=137.9, num_updates=108300, lr=9.60917e-05, gnorm=1.499, loss_scale=8, train_wall=23, gb_free=10.8, wall=33098
2021-05-06 01:49:06 | INFO | train_inner | epoch 002:  47989 / 60421 loss=1.077, ppl=2.11, wps=16347.2, ups=4.38, wpb=3730.8, bsz=132.1, num_updates=108400, lr=9.60473e-05, gnorm=1.511, loss_scale=8, train_wall=23, gb_free=11.3, wall=33120
2021-05-06 01:49:30 | INFO | train_inner | epoch 002:  48089 / 60421 loss=0.997, ppl=2, wps=16310.4, ups=4.32, wpb=3776, bsz=146.1, num_updates=108500, lr=9.60031e-05, gnorm=1.24, loss_scale=8, train_wall=23, gb_free=11, wall=33143
2021-05-06 01:49:53 | INFO | train_inner | epoch 002:  48189 / 60421 loss=1.008, ppl=2.01, wps=16109.2, ups=4.32, wpb=3731.1, bsz=129, num_updates=108600, lr=9.59589e-05, gnorm=1.085, loss_scale=8, train_wall=23, gb_free=10.9, wall=33167
2021-05-06 01:50:16 | INFO | train_inner | epoch 002:  48289 / 60421 loss=1.04, ppl=2.06, wps=15912.9, ups=4.24, wpb=3752.9, bsz=131, num_updates=108700, lr=9.59147e-05, gnorm=1.32, loss_scale=8, train_wall=23, gb_free=10.8, wall=33190
2021-05-06 01:50:40 | INFO | train_inner | epoch 002:  48389 / 60421 loss=0.97, ppl=1.96, wps=15714.5, ups=4.24, wpb=3708.4, bsz=134.7, num_updates=108800, lr=9.58706e-05, gnorm=1.32, loss_scale=8, train_wall=23, gb_free=10.9, wall=33214
2021-05-06 01:51:04 | INFO | train_inner | epoch 002:  48489 / 60421 loss=1.06, ppl=2.08, wps=15622.6, ups=4.2, wpb=3723.3, bsz=123.4, num_updates=108900, lr=9.58266e-05, gnorm=1.329, loss_scale=8, train_wall=24, gb_free=11, wall=33238
2021-05-06 01:51:27 | INFO | train_inner | epoch 002:  48589 / 60421 loss=0.924, ppl=1.9, wps=16057.5, ups=4.31, wpb=3729.1, bsz=147.3, num_updates=109000, lr=9.57826e-05, gnorm=1.221, loss_scale=8, train_wall=23, gb_free=10.4, wall=33261
2021-05-06 01:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:51:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:51:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:51:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:51:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:51:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:51:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:51:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:51:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:51:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:51:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:51:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:51:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:51:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:51:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:51:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:52:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.786 | ppl 27.6 | bleu 29.04 | wps 2475.3 | wpb 2024.1 | bsz 97.5 | num_updates 109000 | best_bleu 29.19
2021-05-06 01:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 109000 updates
2021-05-06 01:52:31 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_109000.pt
2021-05-06 01:52:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_109000.pt
2021-05-06 01:52:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_109000.pt (epoch 2 @ 109000 updates, score 29.04) (writing took 7.8989501369942445 seconds)
2021-05-06 01:53:02 | INFO | train_inner | epoch 002:  48689 / 60421 loss=1.019, ppl=2.03, wps=3951.1, ups=1.05, wpb=3748.6, bsz=140.2, num_updates=109100, lr=9.57387e-05, gnorm=1.249, loss_scale=8, train_wall=23, gb_free=10.8, wall=33356
2021-05-06 01:53:25 | INFO | train_inner | epoch 002:  48789 / 60421 loss=0.99, ppl=1.99, wps=16319, ups=4.39, wpb=3715.3, bsz=145, num_updates=109200, lr=9.56949e-05, gnorm=1.295, loss_scale=8, train_wall=23, gb_free=11.3, wall=33379
2021-05-06 01:53:48 | INFO | train_inner | epoch 002:  48889 / 60421 loss=1.052, ppl=2.07, wps=16517.4, ups=4.37, wpb=3783.7, bsz=114.9, num_updates=109300, lr=9.56511e-05, gnorm=1.151, loss_scale=8, train_wall=23, gb_free=10.9, wall=33401
2021-05-06 01:54:11 | INFO | train_inner | epoch 002:  48989 / 60421 loss=1.061, ppl=2.09, wps=16448.9, ups=4.34, wpb=3792.9, bsz=117.4, num_updates=109400, lr=9.56074e-05, gnorm=1.317, loss_scale=8, train_wall=23, gb_free=11.1, wall=33425
2021-05-06 01:54:34 | INFO | train_inner | epoch 002:  49089 / 60421 loss=0.954, ppl=1.94, wps=16104.4, ups=4.32, wpb=3729.7, bsz=149.5, num_updates=109500, lr=9.55637e-05, gnorm=1.159, loss_scale=8, train_wall=23, gb_free=10.8, wall=33448
2021-05-06 01:54:57 | INFO | train_inner | epoch 002:  49189 / 60421 loss=0.989, ppl=1.98, wps=16016.1, ups=4.27, wpb=3746.8, bsz=132.7, num_updates=109600, lr=9.55201e-05, gnorm=1.427, loss_scale=8, train_wall=23, gb_free=10.7, wall=33471
2021-05-06 01:55:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-06 01:55:21 | INFO | train_inner | epoch 002:  49290 / 60421 loss=1.034, ppl=2.05, wps=15888.2, ups=4.21, wpb=3776.1, bsz=129.7, num_updates=109700, lr=9.54765e-05, gnorm=1.082, loss_scale=4, train_wall=24, gb_free=10.8, wall=33495
2021-05-06 01:55:44 | INFO | train_inner | epoch 002:  49390 / 60421 loss=1.044, ppl=2.06, wps=15749.2, ups=4.25, wpb=3705.5, bsz=124, num_updates=109800, lr=9.54331e-05, gnorm=1.451, loss_scale=4, train_wall=23, gb_free=10.8, wall=33518
2021-05-06 01:56:08 | INFO | train_inner | epoch 002:  49490 / 60421 loss=1.026, ppl=2.04, wps=15669.2, ups=4.24, wpb=3695.5, bsz=134.6, num_updates=109900, lr=9.53896e-05, gnorm=1.485, loss_scale=4, train_wall=23, gb_free=11.1, wall=33542
2021-05-06 01:56:31 | INFO | train_inner | epoch 002:  49590 / 60421 loss=0.975, ppl=1.97, wps=16155.8, ups=4.34, wpb=3725.7, bsz=143.8, num_updates=110000, lr=9.53463e-05, gnorm=1.235, loss_scale=4, train_wall=23, gb_free=10.8, wall=33565
2021-05-06 01:56:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 01:56:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:56:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:56:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:56:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:56:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:56:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:56:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:56:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:56:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:56:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 01:56:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 01:56:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 01:57:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.807 | ppl 27.99 | bleu 28.96 | wps 2468.3 | wpb 2024.1 | bsz 97.5 | num_updates 110000 | best_bleu 29.19
2021-05-06 01:57:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 110000 updates
2021-05-06 01:57:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_110000.pt
2021-05-06 01:57:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_110000.pt
2021-05-06 01:57:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_110000.pt (epoch 2 @ 110000 updates, score 28.96) (writing took 8.360847522999393 seconds)
2021-05-06 01:58:07 | INFO | train_inner | epoch 002:  49690 / 60421 loss=1.054, ppl=2.08, wps=3972.6, ups=1.05, wpb=3795.1, bsz=139.2, num_updates=110100, lr=9.53029e-05, gnorm=1.297, loss_scale=4, train_wall=23, gb_free=10.9, wall=33661
2021-05-06 01:58:30 | INFO | train_inner | epoch 002:  49790 / 60421 loss=1.029, ppl=2.04, wps=16555.9, ups=4.37, wpb=3786.5, bsz=124, num_updates=110200, lr=9.52597e-05, gnorm=1.17, loss_scale=4, train_wall=23, gb_free=10.6, wall=33683
2021-05-06 01:58:52 | INFO | train_inner | epoch 002:  49890 / 60421 loss=1.096, ppl=2.14, wps=16498.5, ups=4.37, wpb=3774.5, bsz=131, num_updates=110300, lr=9.52165e-05, gnorm=1.414, loss_scale=4, train_wall=23, gb_free=10.8, wall=33706
2021-05-06 01:59:15 | INFO | train_inner | epoch 002:  49990 / 60421 loss=1.042, ppl=2.06, wps=16326.7, ups=4.35, wpb=3752.6, bsz=129.4, num_updates=110400, lr=9.51734e-05, gnorm=1.414, loss_scale=4, train_wall=23, gb_free=10.7, wall=33729
2021-05-06 01:59:38 | INFO | train_inner | epoch 002:  50090 / 60421 loss=1, ppl=2, wps=16255.3, ups=4.36, wpb=3730.4, bsz=133.9, num_updates=110500, lr=9.51303e-05, gnorm=1.196, loss_scale=4, train_wall=23, gb_free=10.9, wall=33752
2021-05-06 02:00:01 | INFO | train_inner | epoch 002:  50190 / 60421 loss=1.032, ppl=2.05, wps=16176.8, ups=4.33, wpb=3738.7, bsz=131.1, num_updates=110600, lr=9.50873e-05, gnorm=1.082, loss_scale=4, train_wall=23, gb_free=11.1, wall=33775
2021-05-06 02:00:25 | INFO | train_inner | epoch 002:  50290 / 60421 loss=1.004, ppl=2.01, wps=16040.7, ups=4.22, wpb=3799, bsz=133.1, num_updates=110700, lr=9.50443e-05, gnorm=1.075, loss_scale=4, train_wall=23, gb_free=10.7, wall=33799
2021-05-06 02:00:49 | INFO | train_inner | epoch 002:  50390 / 60421 loss=1.092, ppl=2.13, wps=15777.4, ups=4.22, wpb=3737.1, bsz=114.9, num_updates=110800, lr=9.50014e-05, gnorm=1.464, loss_scale=4, train_wall=23, gb_free=10.7, wall=33823
2021-05-06 02:01:12 | INFO | train_inner | epoch 002:  50490 / 60421 loss=1.033, ppl=2.05, wps=15773.4, ups=4.26, wpb=3701.8, bsz=117.5, num_updates=110900, lr=9.49586e-05, gnorm=1.228, loss_scale=4, train_wall=23, gb_free=10.7, wall=33846
2021-05-06 02:01:35 | INFO | train_inner | epoch 002:  50590 / 60421 loss=1.074, ppl=2.11, wps=16316.8, ups=4.35, wpb=3750.4, bsz=124.2, num_updates=111000, lr=9.49158e-05, gnorm=1.255, loss_scale=4, train_wall=23, gb_free=11, wall=33869
2021-05-06 02:01:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:01:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:01:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:01:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:01:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:01:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:01:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:01:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:01:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:01:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:01:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:01:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:01:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:01:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:01:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:01:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:02:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.816 | ppl 28.17 | bleu 29.25 | wps 2406.9 | wpb 2024.1 | bsz 97.5 | num_updates 111000 | best_bleu 29.25
2021-05-06 02:02:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 111000 updates
2021-05-06 02:02:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_111000.pt
2021-05-06 02:02:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_111000.pt
2021-05-06 02:02:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_111000.pt (epoch 2 @ 111000 updates, score 29.25) (writing took 14.594536568998592 seconds)
2021-05-06 02:03:18 | INFO | train_inner | epoch 002:  50690 / 60421 loss=1.045, ppl=2.06, wps=3574.9, ups=0.97, wpb=3682.5, bsz=126.2, num_updates=111100, lr=9.48731e-05, gnorm=1.545, loss_scale=4, train_wall=22, gb_free=10.7, wall=33972
2021-05-06 02:03:41 | INFO | train_inner | epoch 002:  50790 / 60421 loss=1.076, ppl=2.11, wps=16505.3, ups=4.4, wpb=3748.1, bsz=124.2, num_updates=111200, lr=9.48304e-05, gnorm=1.481, loss_scale=4, train_wall=23, gb_free=10.9, wall=33995
2021-05-06 02:04:04 | INFO | train_inner | epoch 002:  50890 / 60421 loss=1.013, ppl=2.02, wps=16282.2, ups=4.33, wpb=3762.5, bsz=144.5, num_updates=111300, lr=9.47878e-05, gnorm=1.376, loss_scale=4, train_wall=23, gb_free=10.7, wall=34018
2021-05-06 02:04:27 | INFO | train_inner | epoch 002:  50990 / 60421 loss=0.989, ppl=1.98, wps=16374, ups=4.35, wpb=3762.7, bsz=141, num_updates=111400, lr=9.47452e-05, gnorm=1.234, loss_scale=4, train_wall=23, gb_free=11, wall=34041
2021-05-06 02:04:50 | INFO | train_inner | epoch 002:  51090 / 60421 loss=0.966, ppl=1.95, wps=16078.5, ups=4.32, wpb=3721.3, bsz=138.2, num_updates=111500, lr=9.47027e-05, gnorm=1.318, loss_scale=4, train_wall=23, gb_free=10.7, wall=34064
2021-05-06 02:05:14 | INFO | train_inner | epoch 002:  51190 / 60421 loss=0.985, ppl=1.98, wps=15989.5, ups=4.28, wpb=3737.2, bsz=154.8, num_updates=111600, lr=9.46603e-05, gnorm=1.205, loss_scale=4, train_wall=23, gb_free=10.7, wall=34087
2021-05-06 02:05:37 | INFO | train_inner | epoch 002:  51290 / 60421 loss=1.039, ppl=2.05, wps=15742.8, ups=4.27, wpb=3687, bsz=128.6, num_updates=111700, lr=9.46179e-05, gnorm=1.492, loss_scale=4, train_wall=23, gb_free=10.7, wall=34111
2021-05-06 02:06:01 | INFO | train_inner | epoch 002:  51390 / 60421 loss=0.958, ppl=1.94, wps=15814.7, ups=4.2, wpb=3765.2, bsz=140.9, num_updates=111800, lr=9.45756e-05, gnorm=1.144, loss_scale=4, train_wall=24, gb_free=10.9, wall=34135
2021-05-06 02:06:24 | INFO | train_inner | epoch 002:  51490 / 60421 loss=1.022, ppl=2.03, wps=16217.3, ups=4.32, wpb=3755.4, bsz=139.8, num_updates=111900, lr=9.45333e-05, gnorm=1.127, loss_scale=4, train_wall=23, gb_free=10.9, wall=34158
2021-05-06 02:06:47 | INFO | train_inner | epoch 002:  51590 / 60421 loss=0.995, ppl=1.99, wps=16400.5, ups=4.37, wpb=3749.2, bsz=129.8, num_updates=112000, lr=9.44911e-05, gnorm=1.127, loss_scale=4, train_wall=23, gb_free=10.7, wall=34181
2021-05-06 02:06:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:06:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:06:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:06:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:07:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:07:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:07:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:07:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:07:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:07:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:07:50 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.835 | ppl 28.53 | bleu 29.25 | wps 2493.7 | wpb 2024.1 | bsz 97.5 | num_updates 112000 | best_bleu 29.25
2021-05-06 02:07:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 112000 updates
2021-05-06 02:07:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_112000.pt
2021-05-06 02:07:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_112000.pt
2021-05-06 02:08:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_112000.pt (epoch 2 @ 112000 updates, score 29.25) (writing took 14.582568251993507 seconds)
2021-05-06 02:08:28 | INFO | train_inner | epoch 002:  51690 / 60421 loss=1.003, ppl=2, wps=3718.1, ups=0.99, wpb=3757.9, bsz=143.3, num_updates=112100, lr=9.4449e-05, gnorm=1.192, loss_scale=4, train_wall=23, gb_free=10.8, wall=34282
2021-05-06 02:08:51 | INFO | train_inner | epoch 002:  51790 / 60421 loss=1.061, ppl=2.09, wps=16725.7, ups=4.37, wpb=3827.5, bsz=129.3, num_updates=112200, lr=9.44069e-05, gnorm=1.094, loss_scale=4, train_wall=23, gb_free=10.9, wall=34305
2021-05-06 02:09:14 | INFO | train_inner | epoch 002:  51890 / 60421 loss=0.967, ppl=1.95, wps=16225, ups=4.38, wpb=3708.2, bsz=152.3, num_updates=112300, lr=9.43648e-05, gnorm=1.296, loss_scale=4, train_wall=23, gb_free=11, wall=34328
2021-05-06 02:09:36 | INFO | train_inner | epoch 002:  51990 / 60421 loss=1.048, ppl=2.07, wps=16150.8, ups=4.39, wpb=3682.5, bsz=117.9, num_updates=112400, lr=9.43228e-05, gnorm=1.735, loss_scale=4, train_wall=23, gb_free=10.8, wall=34350
2021-05-06 02:10:00 | INFO | train_inner | epoch 002:  52090 / 60421 loss=0.999, ppl=2, wps=16174.9, ups=4.3, wpb=3763.5, bsz=140.6, num_updates=112500, lr=9.42809e-05, gnorm=1.223, loss_scale=4, train_wall=23, gb_free=10.7, wall=34374
2021-05-06 02:10:23 | INFO | train_inner | epoch 002:  52190 / 60421 loss=1.077, ppl=2.11, wps=15951.9, ups=4.28, wpb=3723.3, bsz=124.3, num_updates=112600, lr=9.4239e-05, gnorm=1.57, loss_scale=4, train_wall=23, gb_free=10.9, wall=34397
2021-05-06 02:10:47 | INFO | train_inner | epoch 002:  52290 / 60421 loss=0.987, ppl=1.98, wps=15643.6, ups=4.22, wpb=3709.7, bsz=134.3, num_updates=112700, lr=9.41972e-05, gnorm=1.362, loss_scale=4, train_wall=24, gb_free=10.8, wall=34421
2021-05-06 02:11:10 | INFO | train_inner | epoch 002:  52390 / 60421 loss=1.035, ppl=2.05, wps=15881.6, ups=4.24, wpb=3745.6, bsz=156, num_updates=112800, lr=9.41554e-05, gnorm=1.295, loss_scale=4, train_wall=23, gb_free=10.8, wall=34444
2021-05-06 02:11:33 | INFO | train_inner | epoch 002:  52490 / 60421 loss=1.118, ppl=2.17, wps=16420.9, ups=4.36, wpb=3768.1, bsz=117.3, num_updates=112900, lr=9.41137e-05, gnorm=1.57, loss_scale=4, train_wall=23, gb_free=10.8, wall=34467
2021-05-06 02:11:56 | INFO | train_inner | epoch 002:  52590 / 60421 loss=1.001, ppl=2, wps=16438.3, ups=4.39, wpb=3742.4, bsz=136.8, num_updates=113000, lr=9.40721e-05, gnorm=1.277, loss_scale=4, train_wall=23, gb_free=10.8, wall=34490
2021-05-06 02:11:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:12:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:12:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:12:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:12:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:12:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:12:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:12:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:12:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:12:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:13:00 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.83 | ppl 28.45 | bleu 29.33 | wps 2467.9 | wpb 2024.1 | bsz 97.5 | num_updates 113000 | best_bleu 29.33
2021-05-06 02:13:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 113000 updates
2021-05-06 02:13:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_113000.pt
2021-05-06 02:13:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_113000.pt
2021-05-06 02:13:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_113000.pt (epoch 2 @ 113000 updates, score 29.33) (writing took 14.517455510009313 seconds)
2021-05-06 02:13:38 | INFO | train_inner | epoch 002:  52690 / 60421 loss=1.005, ppl=2.01, wps=3714.5, ups=0.98, wpb=3779.5, bsz=132.6, num_updates=113100, lr=9.40305e-05, gnorm=1.344, loss_scale=4, train_wall=23, gb_free=10.8, wall=34592
2021-05-06 02:14:01 | INFO | train_inner | epoch 002:  52790 / 60421 loss=1.037, ppl=2.05, wps=16335.8, ups=4.41, wpb=3707.9, bsz=128.3, num_updates=113200, lr=9.39889e-05, gnorm=1.424, loss_scale=4, train_wall=23, gb_free=10.7, wall=34614
2021-05-06 02:14:23 | INFO | train_inner | epoch 002:  52890 / 60421 loss=1.04, ppl=2.06, wps=16216, ups=4.38, wpb=3705.6, bsz=126.9, num_updates=113300, lr=9.39475e-05, gnorm=1.635, loss_scale=4, train_wall=23, gb_free=10.8, wall=34637
2021-05-06 02:14:46 | INFO | train_inner | epoch 002:  52990 / 60421 loss=1.073, ppl=2.1, wps=16232, ups=4.37, wpb=3713.3, bsz=117, num_updates=113400, lr=9.3906e-05, gnorm=1.407, loss_scale=4, train_wall=23, gb_free=10.7, wall=34660
2021-05-06 02:15:09 | INFO | train_inner | epoch 002:  53090 / 60421 loss=1.033, ppl=2.05, wps=16103.2, ups=4.36, wpb=3690, bsz=131.1, num_updates=113500, lr=9.38647e-05, gnorm=1.335, loss_scale=4, train_wall=23, gb_free=11.1, wall=34683
2021-05-06 02:15:32 | INFO | train_inner | epoch 002:  53190 / 60421 loss=1.052, ppl=2.07, wps=15790.5, ups=4.33, wpb=3646.7, bsz=127.1, num_updates=113600, lr=9.38233e-05, gnorm=1.951, loss_scale=4, train_wall=23, gb_free=11.1, wall=34706
2021-05-06 02:15:56 | INFO | train_inner | epoch 002:  53290 / 60421 loss=1.028, ppl=2.04, wps=15767.8, ups=4.2, wpb=3756.4, bsz=145.5, num_updates=113700, lr=9.37821e-05, gnorm=1.414, loss_scale=4, train_wall=24, gb_free=11.1, wall=34730
2021-05-06 02:16:20 | INFO | train_inner | epoch 002:  53390 / 60421 loss=1.079, ppl=2.11, wps=16285.9, ups=4.26, wpb=3822, bsz=116.3, num_updates=113800, lr=9.37408e-05, gnorm=1.238, loss_scale=4, train_wall=23, gb_free=10.8, wall=34753
2021-05-06 02:16:42 | INFO | train_inner | epoch 002:  53490 / 60421 loss=0.952, ppl=1.93, wps=16384.5, ups=4.37, wpb=3748.3, bsz=147.5, num_updates=113900, lr=9.36997e-05, gnorm=1.062, loss_scale=4, train_wall=23, gb_free=11.1, wall=34776
2021-05-06 02:17:05 | INFO | train_inner | epoch 002:  53590 / 60421 loss=0.985, ppl=1.98, wps=16501.8, ups=4.37, wpb=3773.6, bsz=142.3, num_updates=114000, lr=9.36586e-05, gnorm=1.178, loss_scale=4, train_wall=23, gb_free=10.8, wall=34799
2021-05-06 02:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:17:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:17:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:17:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:17:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:17:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:17:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:18:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.843 | ppl 28.69 | bleu 29.29 | wps 2472.1 | wpb 2024.1 | bsz 97.5 | num_updates 114000 | best_bleu 29.33
2021-05-06 02:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 114000 updates
2021-05-06 02:18:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_114000.pt
2021-05-06 02:18:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_114000.pt
2021-05-06 02:18:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_114000.pt (epoch 2 @ 114000 updates, score 29.29) (writing took 10.323973021004349 seconds)
2021-05-06 02:18:42 | INFO | train_inner | epoch 002:  53690 / 60421 loss=1.062, ppl=2.09, wps=3841.7, ups=1.03, wpb=3729.6, bsz=116.6, num_updates=114100, lr=9.36175e-05, gnorm=1.297, loss_scale=4, train_wall=22, gb_free=10.7, wall=34896
2021-05-06 02:19:05 | INFO | train_inner | epoch 002:  53790 / 60421 loss=0.999, ppl=2, wps=16367, ups=4.4, wpb=3719.2, bsz=138.4, num_updates=114200, lr=9.35765e-05, gnorm=1.378, loss_scale=4, train_wall=23, gb_free=11, wall=34919
2021-05-06 02:19:28 | INFO | train_inner | epoch 002:  53890 / 60421 loss=1.005, ppl=2.01, wps=16444.4, ups=4.46, wpb=3688.7, bsz=128.1, num_updates=114300, lr=9.35356e-05, gnorm=1.321, loss_scale=4, train_wall=22, gb_free=10.7, wall=34941
2021-05-06 02:19:50 | INFO | train_inner | epoch 002:  53990 / 60421 loss=1.035, ppl=2.05, wps=16267.7, ups=4.41, wpb=3690.1, bsz=114.5, num_updates=114400, lr=9.34947e-05, gnorm=1.553, loss_scale=4, train_wall=22, gb_free=10.8, wall=34964
2021-05-06 02:20:13 | INFO | train_inner | epoch 002:  54090 / 60421 loss=1.108, ppl=2.16, wps=16577.4, ups=4.44, wpb=3736.8, bsz=127.1, num_updates=114500, lr=9.34539e-05, gnorm=1.464, loss_scale=4, train_wall=22, gb_free=11.6, wall=34987
2021-05-06 02:20:36 | INFO | train_inner | epoch 002:  54190 / 60421 loss=1.125, ppl=2.18, wps=16803.7, ups=4.39, wpb=3831.6, bsz=135.6, num_updates=114600, lr=9.34131e-05, gnorm=1.739, loss_scale=4, train_wall=23, gb_free=10.9, wall=35009
2021-05-06 02:20:58 | INFO | train_inner | epoch 002:  54290 / 60421 loss=1.069, ppl=2.1, wps=16762.5, ups=4.41, wpb=3802.2, bsz=114, num_updates=114700, lr=9.33724e-05, gnorm=1.287, loss_scale=4, train_wall=22, gb_free=11.1, wall=35032
2021-05-06 02:21:21 | INFO | train_inner | epoch 002:  54390 / 60421 loss=1.056, ppl=2.08, wps=16489.2, ups=4.41, wpb=3736, bsz=127.9, num_updates=114800, lr=9.33317e-05, gnorm=1.433, loss_scale=4, train_wall=22, gb_free=10.9, wall=35055
2021-05-06 02:21:43 | INFO | train_inner | epoch 002:  54490 / 60421 loss=1.022, ppl=2.03, wps=16460.1, ups=4.48, wpb=3676.5, bsz=134.1, num_updates=114900, lr=9.32911e-05, gnorm=1.57, loss_scale=4, train_wall=22, gb_free=10.8, wall=35077
2021-05-06 02:22:06 | INFO | train_inner | epoch 002:  54590 / 60421 loss=0.965, ppl=1.95, wps=16419.9, ups=4.43, wpb=3706, bsz=151.7, num_updates=115000, lr=9.32505e-05, gnorm=1.189, loss_scale=4, train_wall=22, gb_free=10.8, wall=35100
2021-05-06 02:22:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:22:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:22:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:22:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:22:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:22:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:22:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:22:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:22:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:22:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:22:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:22:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:22:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:23:10 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.832 | ppl 28.48 | bleu 29.09 | wps 2487.9 | wpb 2024.1 | bsz 97.5 | num_updates 115000 | best_bleu 29.33
2021-05-06 02:23:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 115000 updates
2021-05-06 02:23:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_115000.pt
2021-05-06 02:23:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_115000.pt
2021-05-06 02:23:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_115000.pt (epoch 2 @ 115000 updates, score 29.09) (writing took 8.236173765995773 seconds)
2021-05-06 02:23:41 | INFO | train_inner | epoch 002:  54690 / 60421 loss=1.001, ppl=2, wps=3919.3, ups=1.05, wpb=3717.6, bsz=129.2, num_updates=115100, lr=9.321e-05, gnorm=1.235, loss_scale=4, train_wall=23, gb_free=10.7, wall=35195
2021-05-06 02:24:03 | INFO | train_inner | epoch 002:  54790 / 60421 loss=1.037, ppl=2.05, wps=16442.1, ups=4.43, wpb=3710.2, bsz=119.2, num_updates=115200, lr=9.31695e-05, gnorm=1.43, loss_scale=4, train_wall=22, gb_free=10.6, wall=35217
2021-05-06 02:24:26 | INFO | train_inner | epoch 002:  54890 / 60421 loss=0.992, ppl=1.99, wps=16192.7, ups=4.36, wpb=3710.6, bsz=139.6, num_updates=115300, lr=9.31291e-05, gnorm=1.355, loss_scale=4, train_wall=23, gb_free=10.7, wall=35240
2021-05-06 02:24:49 | INFO | train_inner | epoch 002:  54990 / 60421 loss=1.036, ppl=2.05, wps=16295.3, ups=4.35, wpb=3748.9, bsz=124.6, num_updates=115400, lr=9.30887e-05, gnorm=1.456, loss_scale=4, train_wall=23, gb_free=10.8, wall=35263
2021-05-06 02:25:13 | INFO | train_inner | epoch 002:  55090 / 60421 loss=0.956, ppl=1.94, wps=15970.2, ups=4.27, wpb=3743.9, bsz=139.8, num_updates=115500, lr=9.30484e-05, gnorm=1.249, loss_scale=4, train_wall=23, gb_free=10.8, wall=35287
2021-05-06 02:25:36 | INFO | train_inner | epoch 002:  55190 / 60421 loss=1.022, ppl=2.03, wps=15692, ups=4.27, wpb=3675.8, bsz=133, num_updates=115600, lr=9.30082e-05, gnorm=1.459, loss_scale=4, train_wall=23, gb_free=10.8, wall=35310
2021-05-06 02:26:00 | INFO | train_inner | epoch 002:  55290 / 60421 loss=1.101, ppl=2.14, wps=15704.8, ups=4.23, wpb=3708.9, bsz=111.1, num_updates=115700, lr=9.2968e-05, gnorm=1.513, loss_scale=4, train_wall=23, gb_free=10.7, wall=35334
2021-05-06 02:26:23 | INFO | train_inner | epoch 002:  55390 / 60421 loss=0.969, ppl=1.96, wps=16099.3, ups=4.3, wpb=3746.3, bsz=156.6, num_updates=115800, lr=9.29278e-05, gnorm=1.241, loss_scale=4, train_wall=23, gb_free=10.9, wall=35357
2021-05-06 02:26:46 | INFO | train_inner | epoch 002:  55490 / 60421 loss=1.05, ppl=2.07, wps=16417.6, ups=4.36, wpb=3763.9, bsz=145, num_updates=115900, lr=9.28877e-05, gnorm=1.618, loss_scale=4, train_wall=23, gb_free=10.8, wall=35380
2021-05-06 02:27:09 | INFO | train_inner | epoch 002:  55590 / 60421 loss=1.022, ppl=2.03, wps=16522.3, ups=4.39, wpb=3762.2, bsz=126, num_updates=116000, lr=9.28477e-05, gnorm=1.219, loss_scale=4, train_wall=23, gb_free=10.8, wall=35403
2021-05-06 02:27:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:27:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:27:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:27:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:27:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:27:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:27:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:27:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:27:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:27:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:27:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:27:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:27:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:27:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:27:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:27:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:28:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.81 | ppl 28.06 | bleu 29.11 | wps 2466.2 | wpb 2024.1 | bsz 97.5 | num_updates 116000 | best_bleu 29.33
2021-05-06 02:28:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 116000 updates
2021-05-06 02:28:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_116000.pt
2021-05-06 02:28:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_116000.pt
2021-05-06 02:28:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_116000.pt (epoch 2 @ 116000 updates, score 29.11) (writing took 8.241621403984027 seconds)
2021-05-06 02:28:44 | INFO | train_inner | epoch 002:  55690 / 60421 loss=1.029, ppl=2.04, wps=3933.3, ups=1.05, wpb=3757.2, bsz=134.3, num_updates=116100, lr=9.28077e-05, gnorm=1.275, loss_scale=4, train_wall=23, gb_free=10.9, wall=35498
2021-05-06 02:29:07 | INFO | train_inner | epoch 002:  55790 / 60421 loss=0.978, ppl=1.97, wps=16195.5, ups=4.43, wpb=3657.2, bsz=121.8, num_updates=116200, lr=9.27677e-05, gnorm=1.439, loss_scale=4, train_wall=22, gb_free=10.8, wall=35521
2021-05-06 02:29:30 | INFO | train_inner | epoch 002:  55890 / 60421 loss=1.032, ppl=2.05, wps=16137.7, ups=4.39, wpb=3675.6, bsz=130.6, num_updates=116300, lr=9.27278e-05, gnorm=1.511, loss_scale=4, train_wall=23, gb_free=11.4, wall=35543
2021-05-06 02:29:53 | INFO | train_inner | epoch 002:  55990 / 60421 loss=1.009, ppl=2.01, wps=16191.2, ups=4.31, wpb=3753.8, bsz=146.6, num_updates=116400, lr=9.2688e-05, gnorm=1.37, loss_scale=4, train_wall=23, gb_free=10.6, wall=35567
2021-05-06 02:30:16 | INFO | train_inner | epoch 002:  56090 / 60421 loss=1.06, ppl=2.09, wps=16114.7, ups=4.33, wpb=3720.4, bsz=129.8, num_updates=116500, lr=9.26482e-05, gnorm=1.388, loss_scale=4, train_wall=23, gb_free=10.8, wall=35590
2021-05-06 02:30:39 | INFO | train_inner | epoch 002:  56190 / 60421 loss=1.017, ppl=2.02, wps=15676.7, ups=4.26, wpb=3676.7, bsz=131.9, num_updates=116600, lr=9.26085e-05, gnorm=1.499, loss_scale=4, train_wall=23, gb_free=10.9, wall=35613
2021-05-06 02:31:03 | INFO | train_inner | epoch 002:  56290 / 60421 loss=0.974, ppl=1.96, wps=15658.9, ups=4.22, wpb=3714.8, bsz=128.6, num_updates=116700, lr=9.25688e-05, gnorm=1.166, loss_scale=4, train_wall=24, gb_free=11, wall=35637
2021-05-06 02:31:26 | INFO | train_inner | epoch 002:  56390 / 60421 loss=1.007, ppl=2.01, wps=16159.6, ups=4.33, wpb=3730.6, bsz=130.5, num_updates=116800, lr=9.25292e-05, gnorm=1.305, loss_scale=4, train_wall=23, gb_free=10.9, wall=35660
2021-05-06 02:31:49 | INFO | train_inner | epoch 002:  56490 / 60421 loss=1.003, ppl=2, wps=16554.8, ups=4.38, wpb=3783.7, bsz=142.3, num_updates=116900, lr=9.24896e-05, gnorm=1.137, loss_scale=4, train_wall=23, gb_free=11.2, wall=35683
2021-05-06 02:32:12 | INFO | train_inner | epoch 002:  56590 / 60421 loss=1.061, ppl=2.09, wps=16587.2, ups=4.37, wpb=3798.2, bsz=125.5, num_updates=117000, lr=9.245e-05, gnorm=1.273, loss_scale=4, train_wall=23, gb_free=10.6, wall=35706
2021-05-06 02:32:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:32:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:32:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:32:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:32:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:32:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:32:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:32:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:32:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:32:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:32:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:32:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:32:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:32:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:32:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:32:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:33:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.853 | ppl 28.9 | bleu 29.24 | wps 2469.3 | wpb 2024.1 | bsz 97.5 | num_updates 117000 | best_bleu 29.33
2021-05-06 02:33:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 117000 updates
2021-05-06 02:33:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_117000.pt
2021-05-06 02:33:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_117000.pt
2021-05-06 02:33:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_117000.pt (epoch 2 @ 117000 updates, score 29.24) (writing took 8.322686770989094 seconds)
2021-05-06 02:33:47 | INFO | train_inner | epoch 002:  56690 / 60421 loss=0.998, ppl=2, wps=3921.5, ups=1.05, wpb=3737.7, bsz=118.5, num_updates=117100, lr=9.24105e-05, gnorm=1.054, loss_scale=4, train_wall=22, gb_free=10.8, wall=35801
2021-05-06 02:34:10 | INFO | train_inner | epoch 002:  56790 / 60421 loss=0.982, ppl=1.97, wps=16410.1, ups=4.37, wpb=3756.8, bsz=118.2, num_updates=117200, lr=9.23711e-05, gnorm=1.242, loss_scale=4, train_wall=23, gb_free=10.8, wall=35824
2021-05-06 02:34:33 | INFO | train_inner | epoch 002:  56890 / 60421 loss=1.053, ppl=2.08, wps=16192.6, ups=4.36, wpb=3711.8, bsz=113.3, num_updates=117300, lr=9.23317e-05, gnorm=1.487, loss_scale=4, train_wall=23, gb_free=10.7, wall=35847
2021-05-06 02:34:56 | INFO | train_inner | epoch 002:  56990 / 60421 loss=1.038, ppl=2.05, wps=16218.8, ups=4.3, wpb=3773.1, bsz=122.6, num_updates=117400, lr=9.22924e-05, gnorm=1.213, loss_scale=4, train_wall=23, gb_free=10.7, wall=35870
2021-05-06 02:35:19 | INFO | train_inner | epoch 002:  57090 / 60421 loss=1.041, ppl=2.06, wps=16018.9, ups=4.3, wpb=3721.6, bsz=127.1, num_updates=117500, lr=9.22531e-05, gnorm=1.474, loss_scale=4, train_wall=23, gb_free=10.7, wall=35893
2021-05-06 02:35:43 | INFO | train_inner | epoch 002:  57190 / 60421 loss=0.984, ppl=1.98, wps=15849, ups=4.23, wpb=3748.9, bsz=138.1, num_updates=117600, lr=9.22139e-05, gnorm=1.126, loss_scale=4, train_wall=23, gb_free=10.7, wall=35917
2021-05-06 02:36:07 | INFO | train_inner | epoch 002:  57290 / 60421 loss=1.025, ppl=2.04, wps=16105.2, ups=4.23, wpb=3806.4, bsz=127.8, num_updates=117700, lr=9.21747e-05, gnorm=1.189, loss_scale=4, train_wall=23, gb_free=11.1, wall=35941
2021-05-06 02:36:30 | INFO | train_inner | epoch 002:  57390 / 60421 loss=0.991, ppl=1.99, wps=16488.4, ups=4.29, wpb=3847.6, bsz=150.9, num_updates=117800, lr=9.21356e-05, gnorm=1.093, loss_scale=4, train_wall=23, gb_free=10.8, wall=35964
2021-05-06 02:36:53 | INFO | train_inner | epoch 002:  57490 / 60421 loss=1.021, ppl=2.03, wps=16532.7, ups=4.33, wpb=3816.6, bsz=122.1, num_updates=117900, lr=9.20965e-05, gnorm=1.082, loss_scale=4, train_wall=23, gb_free=10.6, wall=35987
2021-05-06 02:37:16 | INFO | train_inner | epoch 002:  57590 / 60421 loss=1.038, ppl=2.05, wps=16576.7, ups=4.4, wpb=3766.4, bsz=128.4, num_updates=118000, lr=9.20575e-05, gnorm=1.264, loss_scale=4, train_wall=23, gb_free=10.7, wall=36010
2021-05-06 02:37:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:37:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:37:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:37:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:37:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:37:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:37:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:37:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:37:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:37:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:37:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:37:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:37:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:37:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:37:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:37:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:38:22 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.91 | ppl 30.07 | bleu 29.36 | wps 2416.1 | wpb 2024.1 | bsz 97.5 | num_updates 118000 | best_bleu 29.36
2021-05-06 02:38:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 118000 updates
2021-05-06 02:38:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_118000.pt
2021-05-06 02:38:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_118000.pt
2021-05-06 02:38:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_118000.pt (epoch 2 @ 118000 updates, score 29.36) (writing took 14.603681070002494 seconds)
2021-05-06 02:38:59 | INFO | train_inner | epoch 002:  57690 / 60421 loss=1.002, ppl=2, wps=3625.1, ups=0.97, wpb=3734.3, bsz=135.7, num_updates=118100, lr=9.20185e-05, gnorm=1.24, loss_scale=4, train_wall=23, gb_free=11, wall=36113
2021-05-06 02:39:22 | INFO | train_inner | epoch 002:  57790 / 60421 loss=1.021, ppl=2.03, wps=16368.9, ups=4.41, wpb=3712.6, bsz=114.9, num_updates=118200, lr=9.19795e-05, gnorm=1.228, loss_scale=4, train_wall=22, gb_free=11, wall=36135
2021-05-06 02:39:45 | INFO | train_inner | epoch 002:  57890 / 60421 loss=0.983, ppl=1.98, wps=16135.8, ups=4.34, wpb=3717.9, bsz=145.9, num_updates=118300, lr=9.19407e-05, gnorm=1.514, loss_scale=4, train_wall=23, gb_free=11, wall=36159
2021-05-06 02:40:08 | INFO | train_inner | epoch 002:  57990 / 60421 loss=1.028, ppl=2.04, wps=16175, ups=4.3, wpb=3758.8, bsz=134.8, num_updates=118400, lr=9.19018e-05, gnorm=1.355, loss_scale=4, train_wall=23, gb_free=10.8, wall=36182
2021-05-06 02:40:31 | INFO | train_inner | epoch 002:  58090 / 60421 loss=1.034, ppl=2.05, wps=15900, ups=4.27, wpb=3724.4, bsz=128.2, num_updates=118500, lr=9.1863e-05, gnorm=1.316, loss_scale=4, train_wall=23, gb_free=11, wall=36205
2021-05-06 02:40:55 | INFO | train_inner | epoch 002:  58190 / 60421 loss=1.003, ppl=2, wps=15541.4, ups=4.28, wpb=3627.9, bsz=128.9, num_updates=118600, lr=9.18243e-05, gnorm=1.436, loss_scale=4, train_wall=23, gb_free=10.9, wall=36229
2021-05-06 02:41:18 | INFO | train_inner | epoch 002:  58290 / 60421 loss=0.984, ppl=1.98, wps=15919.4, ups=4.33, wpb=3679.5, bsz=136.4, num_updates=118700, lr=9.17856e-05, gnorm=1.244, loss_scale=4, train_wall=23, gb_free=10.7, wall=36252
2021-05-06 02:41:41 | INFO | train_inner | epoch 002:  58390 / 60421 loss=1.034, ppl=2.05, wps=16162.6, ups=4.39, wpb=3679.9, bsz=122.2, num_updates=118800, lr=9.1747e-05, gnorm=1.703, loss_scale=4, train_wall=23, gb_free=11.7, wall=36274
2021-05-06 02:42:03 | INFO | train_inner | epoch 002:  58490 / 60421 loss=1.023, ppl=2.03, wps=16448.4, ups=4.39, wpb=3744, bsz=134, num_updates=118900, lr=9.17084e-05, gnorm=1.373, loss_scale=4, train_wall=23, gb_free=10.8, wall=36297
2021-05-06 02:42:26 | INFO | train_inner | epoch 002:  58590 / 60421 loss=0.994, ppl=1.99, wps=16412.1, ups=4.43, wpb=3702.1, bsz=134.7, num_updates=119000, lr=9.16698e-05, gnorm=1.275, loss_scale=4, train_wall=22, gb_free=10.7, wall=36320
2021-05-06 02:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:42:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:42:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:42:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:42:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:42:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:42:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:42:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:42:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:42:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:42:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:42:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:42:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:42:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:42:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:42:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:43:30 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.89 | ppl 29.65 | bleu 29.02 | wps 2454.4 | wpb 2024.1 | bsz 97.5 | num_updates 119000 | best_bleu 29.36
2021-05-06 02:43:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 119000 updates
2021-05-06 02:43:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_119000.pt
2021-05-06 02:43:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_119000.pt
2021-05-06 02:43:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_119000.pt (epoch 2 @ 119000 updates, score 29.02) (writing took 8.424889107991476 seconds)
2021-05-06 02:44:02 | INFO | train_inner | epoch 002:  58690 / 60421 loss=0.931, ppl=1.91, wps=3925.3, ups=1.04, wpb=3767.5, bsz=143.9, num_updates=119100, lr=9.16314e-05, gnorm=1.098, loss_scale=4, train_wall=23, gb_free=10.7, wall=36416
2021-05-06 02:44:25 | INFO | train_inner | epoch 002:  58790 / 60421 loss=0.987, ppl=1.98, wps=16044.7, ups=4.41, wpb=3638, bsz=124.2, num_updates=119200, lr=9.15929e-05, gnorm=1.382, loss_scale=4, train_wall=22, gb_free=10.8, wall=36438
2021-05-06 02:44:47 | INFO | train_inner | epoch 002:  58890 / 60421 loss=1.05, ppl=2.07, wps=16190.1, ups=4.38, wpb=3699.8, bsz=114.1, num_updates=119300, lr=9.15545e-05, gnorm=1.237, loss_scale=4, train_wall=23, gb_free=11, wall=36461
2021-05-06 02:45:11 | INFO | train_inner | epoch 002:  58990 / 60421 loss=0.999, ppl=2, wps=16217.9, ups=4.29, wpb=3783, bsz=135.7, num_updates=119400, lr=9.15162e-05, gnorm=1.093, loss_scale=4, train_wall=23, gb_free=10.8, wall=36485
2021-05-06 02:45:34 | INFO | train_inner | epoch 002:  59090 / 60421 loss=0.996, ppl=1.99, wps=16048.3, ups=4.24, wpb=3783, bsz=133.1, num_updates=119500, lr=9.14779e-05, gnorm=1.305, loss_scale=4, train_wall=23, gb_free=10.8, wall=36508
2021-05-06 02:45:58 | INFO | train_inner | epoch 002:  59190 / 60421 loss=1.015, ppl=2.02, wps=15785.8, ups=4.19, wpb=3771.7, bsz=136.5, num_updates=119600, lr=9.14396e-05, gnorm=1.252, loss_scale=4, train_wall=24, gb_free=11.1, wall=36532
2021-05-06 02:46:21 | INFO | train_inner | epoch 002:  59290 / 60421 loss=0.995, ppl=1.99, wps=15985.7, ups=4.33, wpb=3693.5, bsz=143.2, num_updates=119700, lr=9.14014e-05, gnorm=1.318, loss_scale=4, train_wall=23, gb_free=10.8, wall=36555
2021-05-06 02:46:44 | INFO | train_inner | epoch 002:  59390 / 60421 loss=0.983, ppl=1.98, wps=16288, ups=4.37, wpb=3730.9, bsz=126.2, num_updates=119800, lr=9.13633e-05, gnorm=1.122, loss_scale=4, train_wall=23, gb_free=10.9, wall=36578
2021-05-06 02:47:07 | INFO | train_inner | epoch 002:  59490 / 60421 loss=1.038, ppl=2.05, wps=16416.4, ups=4.42, wpb=3715.9, bsz=122.6, num_updates=119900, lr=9.13252e-05, gnorm=1.228, loss_scale=4, train_wall=22, gb_free=11.2, wall=36601
2021-05-06 02:47:30 | INFO | train_inner | epoch 002:  59590 / 60421 loss=1.032, ppl=2.05, wps=16303.3, ups=4.41, wpb=3695.2, bsz=123.1, num_updates=120000, lr=9.12871e-05, gnorm=1.446, loss_scale=4, train_wall=22, gb_free=10.7, wall=36623
2021-05-06 02:47:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:47:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:47:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:47:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:47:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:47:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:47:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:47:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:47:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:47:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:47:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:47:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:47:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:48:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.89 | ppl 29.65 | bleu 29.21 | wps 2414.4 | wpb 2024.1 | bsz 97.5 | num_updates 120000 | best_bleu 29.36
2021-05-06 02:48:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 120000 updates
2021-05-06 02:48:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_120000.pt
2021-05-06 02:48:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_120000.pt
2021-05-06 02:48:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_2_120000.pt (epoch 2 @ 120000 updates, score 29.21) (writing took 8.449906692985678 seconds)
2021-05-06 02:49:06 | INFO | train_inner | epoch 002:  59690 / 60421 loss=1.032, ppl=2.04, wps=3837.9, ups=1.03, wpb=3716.2, bsz=135.8, num_updates=120100, lr=9.12491e-05, gnorm=1.446, loss_scale=4, train_wall=22, gb_free=10.9, wall=36720
2021-05-06 02:49:29 | INFO | train_inner | epoch 002:  59790 / 60421 loss=1.024, ppl=2.03, wps=16263.2, ups=4.4, wpb=3694.4, bsz=150.6, num_updates=120200, lr=9.12111e-05, gnorm=1.312, loss_scale=4, train_wall=23, gb_free=10.8, wall=36743
2021-05-06 02:49:52 | INFO | train_inner | epoch 002:  59890 / 60421 loss=0.981, ppl=1.97, wps=15895.3, ups=4.32, wpb=3682.3, bsz=130.8, num_updates=120300, lr=9.11732e-05, gnorm=1.408, loss_scale=4, train_wall=23, gb_free=10.8, wall=36766
2021-05-06 02:50:15 | INFO | train_inner | epoch 002:  59990 / 60421 loss=0.908, ppl=1.88, wps=16037.6, ups=4.31, wpb=3720.1, bsz=162.2, num_updates=120400, lr=9.11353e-05, gnorm=1.404, loss_scale=4, train_wall=23, gb_free=11.9, wall=36789
2021-05-06 02:50:39 | INFO | train_inner | epoch 002:  60090 / 60421 loss=1.1, ppl=2.14, wps=15836.5, ups=4.29, wpb=3692.3, bsz=125, num_updates=120500, lr=9.10975e-05, gnorm=1.931, loss_scale=4, train_wall=23, gb_free=11.1, wall=36813
2021-05-06 02:51:02 | INFO | train_inner | epoch 002:  60190 / 60421 loss=0.996, ppl=1.99, wps=15623.2, ups=4.22, wpb=3699.1, bsz=137, num_updates=120600, lr=9.10597e-05, gnorm=1.222, loss_scale=4, train_wall=23, gb_free=11.2, wall=36836
2021-05-06 02:51:26 | INFO | train_inner | epoch 002:  60290 / 60421 loss=0.989, ppl=1.99, wps=16100, ups=4.3, wpb=3745, bsz=148.6, num_updates=120700, lr=9.1022e-05, gnorm=1.297, loss_scale=4, train_wall=23, gb_free=10.8, wall=36860
2021-05-06 02:51:48 | INFO | train_inner | epoch 002:  60390 / 60421 loss=1, ppl=2, wps=16489.2, ups=4.4, wpb=3747.4, bsz=133, num_updates=120800, lr=9.09843e-05, gnorm=1.098, loss_scale=4, train_wall=23, gb_free=10.7, wall=36882
2021-05-06 02:51:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:52:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:52:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:52:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:52:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:52:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:52:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:52:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:52:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:52:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:52:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:52:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:52:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:52:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:52:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:52:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:53:00 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.871 | ppl 29.27 | bleu 29.23 | wps 2452.5 | wpb 2024.1 | bsz 97.5 | num_updates 120831 | best_bleu 29.36
2021-05-06 02:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 120831 updates
2021-05-06 02:53:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint2.pt
2021-05-06 02:53:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint2.pt
2021-05-06 02:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint2.pt (epoch 2 @ 120831 updates, score 29.23) (writing took 8.150658940983703 seconds)
2021-05-06 02:53:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-05-06 02:53:08 | INFO | train | epoch 002 | loss 1.081 | ppl 2.11 | wps 12237.6 | ups 3.27 | wpb 3737.5 | bsz 132.4 | num_updates 120831 | lr 9.09726e-05 | gnorm 1.4 | loss_scale 4 | train_wall 13770 | gb_free 10.7 | wall 36962
2021-05-06 02:53:08 | INFO | fairseq.trainer | begin training epoch 3
2021-05-06 02:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2021-05-06 02:53:24 | INFO | train_inner | epoch 003:     69 / 60421 loss=0.946, ppl=1.93, wps=3891.1, ups=1.04, wpb=3729, bsz=154.4, num_updates=120900, lr=9.09467e-05, gnorm=1.126, loss_scale=4, train_wall=23, gb_free=10.7, wall=36978
2021-05-06 02:53:47 | INFO | train_inner | epoch 003:    169 / 60421 loss=0.9, ppl=1.87, wps=16177.2, ups=4.4, wpb=3675.5, bsz=124.5, num_updates=121000, lr=9.09091e-05, gnorm=1.193, loss_scale=4, train_wall=23, gb_free=10.9, wall=37001
2021-05-06 02:53:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:53:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:53:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:53:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:54:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:54:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:54:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:54:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:54:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:54:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:54:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:54:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:54:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:54:51 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.899 | ppl 29.84 | bleu 29.31 | wps 2473.4 | wpb 2024.1 | bsz 97.5 | num_updates 121000 | best_bleu 29.36
2021-05-06 02:54:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 121000 updates
2021-05-06 02:54:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_121000.pt
2021-05-06 02:54:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_121000.pt
2021-05-06 02:54:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_121000.pt (epoch 3 @ 121000 updates, score 29.31) (writing took 7.958657349983696 seconds)
2021-05-06 02:55:22 | INFO | train_inner | epoch 003:    269 / 60421 loss=0.919, ppl=1.89, wps=3919.6, ups=1.05, wpb=3729.3, bsz=141, num_updates=121100, lr=9.08715e-05, gnorm=1.174, loss_scale=4, train_wall=23, gb_free=10.9, wall=37096
2021-05-06 02:55:45 | INFO | train_inner | epoch 003:    369 / 60421 loss=0.956, ppl=1.94, wps=15957.2, ups=4.32, wpb=3692.6, bsz=125, num_updates=121200, lr=9.08341e-05, gnorm=1.258, loss_scale=4, train_wall=23, gb_free=10.9, wall=37119
2021-05-06 02:56:08 | INFO | train_inner | epoch 003:    469 / 60421 loss=0.962, ppl=1.95, wps=15775.2, ups=4.33, wpb=3646.1, bsz=125.2, num_updates=121300, lr=9.07966e-05, gnorm=1.307, loss_scale=4, train_wall=23, gb_free=10.8, wall=37142
2021-05-06 02:56:31 | INFO | train_inner | epoch 003:    569 / 60421 loss=0.982, ppl=1.98, wps=16306.4, ups=4.37, wpb=3728, bsz=132.5, num_updates=121400, lr=9.07592e-05, gnorm=1.274, loss_scale=4, train_wall=23, gb_free=10.6, wall=37165
2021-05-06 02:56:54 | INFO | train_inner | epoch 003:    669 / 60421 loss=0.991, ppl=1.99, wps=16345.6, ups=4.4, wpb=3715.9, bsz=116, num_updates=121500, lr=9.07218e-05, gnorm=1.158, loss_scale=4, train_wall=23, gb_free=10.9, wall=37188
2021-05-06 02:57:17 | INFO | train_inner | epoch 003:    769 / 60421 loss=0.911, ppl=1.88, wps=16434.3, ups=4.4, wpb=3735.4, bsz=132.1, num_updates=121600, lr=9.06845e-05, gnorm=1.051, loss_scale=4, train_wall=23, gb_free=10.7, wall=37211
2021-05-06 02:57:39 | INFO | train_inner | epoch 003:    869 / 60421 loss=0.978, ppl=1.97, wps=16539.2, ups=4.42, wpb=3743.6, bsz=125.4, num_updates=121700, lr=9.06473e-05, gnorm=1.071, loss_scale=4, train_wall=22, gb_free=10.7, wall=37233
2021-05-06 02:58:02 | INFO | train_inner | epoch 003:    969 / 60421 loss=0.896, ppl=1.86, wps=16377, ups=4.39, wpb=3733.6, bsz=145, num_updates=121800, lr=9.061e-05, gnorm=1.102, loss_scale=4, train_wall=23, gb_free=11, wall=37256
2021-05-06 02:58:25 | INFO | train_inner | epoch 003:   1069 / 60421 loss=0.952, ppl=1.93, wps=16426, ups=4.39, wpb=3739.6, bsz=113.9, num_updates=121900, lr=9.05729e-05, gnorm=1.099, loss_scale=4, train_wall=23, gb_free=10.7, wall=37279
2021-05-06 02:58:48 | INFO | train_inner | epoch 003:   1169 / 60421 loss=0.975, ppl=1.97, wps=16294.2, ups=4.34, wpb=3750.3, bsz=126.7, num_updates=122000, lr=9.05357e-05, gnorm=1.15, loss_scale=4, train_wall=23, gb_free=10.9, wall=37302
2021-05-06 02:58:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 02:58:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:58:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:58:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:59:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:59:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:59:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:59:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:59:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:59:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:59:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:59:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:59:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:59:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 02:59:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 02:59:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 02:59:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.929 | ppl 30.47 | bleu 28.99 | wps 2450.4 | wpb 2024.1 | bsz 97.5 | num_updates 122000 | best_bleu 29.36
2021-05-06 02:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 122000 updates
2021-05-06 02:59:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_122000.pt
2021-05-06 02:59:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_122000.pt
2021-05-06 03:00:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_122000.pt (epoch 3 @ 122000 updates, score 28.99) (writing took 7.90085895700031 seconds)
2021-05-06 03:00:24 | INFO | train_inner | epoch 003:   1269 / 60421 loss=0.91, ppl=1.88, wps=3873.3, ups=1.04, wpb=3708.4, bsz=135, num_updates=122100, lr=9.04987e-05, gnorm=1.201, loss_scale=4, train_wall=23, gb_free=11.1, wall=37398
2021-05-06 03:00:47 | INFO | train_inner | epoch 003:   1369 / 60421 loss=0.949, ppl=1.93, wps=15964.5, ups=4.25, wpb=3753.8, bsz=148.4, num_updates=122200, lr=9.04616e-05, gnorm=1.073, loss_scale=4, train_wall=23, gb_free=10.8, wall=37421
2021-05-06 03:01:10 | INFO | train_inner | epoch 003:   1469 / 60421 loss=0.961, ppl=1.95, wps=15953.9, ups=4.3, wpb=3708.4, bsz=125.4, num_updates=122300, lr=9.04246e-05, gnorm=1.344, loss_scale=4, train_wall=23, gb_free=10.8, wall=37444
2021-05-06 03:01:33 | INFO | train_inner | epoch 003:   1569 / 60421 loss=0.91, ppl=1.88, wps=16317.7, ups=4.36, wpb=3741.6, bsz=147.3, num_updates=122400, lr=9.03877e-05, gnorm=1.025, loss_scale=4, train_wall=23, gb_free=10.8, wall=37467
2021-05-06 03:01:56 | INFO | train_inner | epoch 003:   1669 / 60421 loss=0.929, ppl=1.9, wps=16471.7, ups=4.37, wpb=3769.1, bsz=138.8, num_updates=122500, lr=9.03508e-05, gnorm=1.066, loss_scale=4, train_wall=23, gb_free=10.7, wall=37490
2021-05-06 03:02:19 | INFO | train_inner | epoch 003:   1769 / 60421 loss=0.953, ppl=1.94, wps=16644.7, ups=4.39, wpb=3795.7, bsz=136.9, num_updates=122600, lr=9.03139e-05, gnorm=1.159, loss_scale=4, train_wall=23, gb_free=10.8, wall=37513
2021-05-06 03:02:42 | INFO | train_inner | epoch 003:   1869 / 60421 loss=0.922, ppl=1.9, wps=16561, ups=4.4, wpb=3763.2, bsz=145, num_updates=122700, lr=9.02771e-05, gnorm=1.061, loss_scale=4, train_wall=23, gb_free=11.1, wall=37536
2021-05-06 03:03:04 | INFO | train_inner | epoch 003:   1969 / 60421 loss=0.957, ppl=1.94, wps=16555.3, ups=4.42, wpb=3747.2, bsz=132, num_updates=122800, lr=9.02404e-05, gnorm=1.197, loss_scale=4, train_wall=22, gb_free=10.7, wall=37558
2021-05-06 03:03:27 | INFO | train_inner | epoch 003:   2069 / 60421 loss=0.995, ppl=1.99, wps=16386.6, ups=4.42, wpb=3707.3, bsz=114.9, num_updates=122900, lr=9.02036e-05, gnorm=1.477, loss_scale=4, train_wall=22, gb_free=10.6, wall=37581
2021-05-06 03:03:50 | INFO | train_inner | epoch 003:   2169 / 60421 loss=0.91, ppl=1.88, wps=16235, ups=4.41, wpb=3682.8, bsz=147.6, num_updates=123000, lr=9.0167e-05, gnorm=1.189, loss_scale=4, train_wall=22, gb_free=10.8, wall=37604
2021-05-06 03:03:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:04:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:04:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:04:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:04:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:04:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:04:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:04:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:04:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:04:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:04:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:04:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:04:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:04:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:04:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:04:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:04:55 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.917 | ppl 30.22 | bleu 29.3 | wps 2416.6 | wpb 2024.1 | bsz 97.5 | num_updates 123000 | best_bleu 29.36
2021-05-06 03:04:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 123000 updates
2021-05-06 03:04:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_123000.pt
2021-05-06 03:04:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_123000.pt
2021-05-06 03:05:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_123000.pt (epoch 3 @ 123000 updates, score 29.3) (writing took 7.942896018008469 seconds)
2021-05-06 03:05:26 | INFO | train_inner | epoch 003:   2269 / 60421 loss=1.009, ppl=2.01, wps=3837.1, ups=1.03, wpb=3711.3, bsz=119.6, num_updates=123100, lr=9.01303e-05, gnorm=1.223, loss_scale=4, train_wall=23, gb_free=10.8, wall=37700
2021-05-06 03:05:50 | INFO | train_inner | epoch 003:   2369 / 60421 loss=0.963, ppl=1.95, wps=15712.4, ups=4.26, wpb=3684.4, bsz=127.4, num_updates=123200, lr=9.00937e-05, gnorm=1.043, loss_scale=4, train_wall=23, gb_free=10.9, wall=37724
2021-05-06 03:06:13 | INFO | train_inner | epoch 003:   2469 / 60421 loss=0.942, ppl=1.92, wps=15934.7, ups=4.29, wpb=3715.2, bsz=129, num_updates=123300, lr=9.00572e-05, gnorm=1.094, loss_scale=4, train_wall=23, gb_free=10.8, wall=37747
2021-05-06 03:06:36 | INFO | train_inner | epoch 003:   2569 / 60421 loss=0.897, ppl=1.86, wps=16095.5, ups=4.44, wpb=3627, bsz=138.6, num_updates=123400, lr=9.00207e-05, gnorm=1.191, loss_scale=4, train_wall=22, gb_free=10.7, wall=37770
2021-05-06 03:06:59 | INFO | train_inner | epoch 003:   2669 / 60421 loss=1.012, ppl=2.02, wps=16508.5, ups=4.34, wpb=3800.3, bsz=117.4, num_updates=123500, lr=8.99843e-05, gnorm=1.119, loss_scale=4, train_wall=23, gb_free=11, wall=37793
2021-05-06 03:07:22 | INFO | train_inner | epoch 003:   2769 / 60421 loss=0.928, ppl=1.9, wps=16432.2, ups=4.36, wpb=3767.8, bsz=147, num_updates=123600, lr=8.99478e-05, gnorm=1.038, loss_scale=4, train_wall=23, gb_free=10.8, wall=37816
2021-05-06 03:07:44 | INFO | train_inner | epoch 003:   2869 / 60421 loss=0.939, ppl=1.92, wps=16376.6, ups=4.43, wpb=3698.8, bsz=137.8, num_updates=123700, lr=8.99115e-05, gnorm=1.109, loss_scale=4, train_wall=22, gb_free=10.8, wall=37838
2021-05-06 03:08:07 | INFO | train_inner | epoch 003:   2969 / 60421 loss=0.954, ppl=1.94, wps=16195, ups=4.39, wpb=3690.7, bsz=136.6, num_updates=123800, lr=8.98752e-05, gnorm=1.237, loss_scale=4, train_wall=23, gb_free=10.8, wall=37861
2021-05-06 03:08:30 | INFO | train_inner | epoch 003:   3069 / 60421 loss=0.965, ppl=1.95, wps=16565.2, ups=4.32, wpb=3832.4, bsz=143.4, num_updates=123900, lr=8.98389e-05, gnorm=1.044, loss_scale=4, train_wall=23, gb_free=10.8, wall=37884
2021-05-06 03:08:53 | INFO | train_inner | epoch 003:   3169 / 60421 loss=1.021, ppl=2.03, wps=16604.9, ups=4.33, wpb=3833.9, bsz=117.6, num_updates=124000, lr=8.98027e-05, gnorm=1.182, loss_scale=4, train_wall=23, gb_free=10.7, wall=37907
2021-05-06 03:08:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:09:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:09:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:09:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:09:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:09:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:09:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:09:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:09:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:09:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:09:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:09:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:09:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:09:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:09:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:09:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:09:59 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.924 | ppl 30.36 | bleu 29.21 | wps 2395.5 | wpb 2024.1 | bsz 97.5 | num_updates 124000 | best_bleu 29.36
2021-05-06 03:09:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 124000 updates
2021-05-06 03:09:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_124000.pt
2021-05-06 03:10:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_124000.pt
2021-05-06 03:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_124000.pt (epoch 3 @ 124000 updates, score 29.21) (writing took 7.980797488999087 seconds)
2021-05-06 03:10:31 | INFO | train_inner | epoch 003:   3269 / 60421 loss=0.958, ppl=1.94, wps=3834.9, ups=1.03, wpb=3740.5, bsz=135.8, num_updates=124100, lr=8.97665e-05, gnorm=1.233, loss_scale=4, train_wall=23, gb_free=11.7, wall=38005
2021-05-06 03:10:54 | INFO | train_inner | epoch 003:   3369 / 60421 loss=1.022, ppl=2.03, wps=15931, ups=4.28, wpb=3725.8, bsz=125.9, num_updates=124200, lr=8.97303e-05, gnorm=1.221, loss_scale=4, train_wall=23, gb_free=10.9, wall=38028
2021-05-06 03:11:17 | INFO | train_inner | epoch 003:   3469 / 60421 loss=0.923, ppl=1.9, wps=16027, ups=4.32, wpb=3711, bsz=146.1, num_updates=124300, lr=8.96942e-05, gnorm=1.238, loss_scale=4, train_wall=23, gb_free=10.7, wall=38051
2021-05-06 03:11:40 | INFO | train_inner | epoch 003:   3569 / 60421 loss=1.019, ppl=2.03, wps=16410.2, ups=4.34, wpb=3779.1, bsz=123.8, num_updates=124400, lr=8.96582e-05, gnorm=1.326, loss_scale=4, train_wall=23, gb_free=10.8, wall=38074
2021-05-06 03:12:03 | INFO | train_inner | epoch 003:   3669 / 60421 loss=0.916, ppl=1.89, wps=16398, ups=4.34, wpb=3779, bsz=153.3, num_updates=124500, lr=8.96221e-05, gnorm=1.234, loss_scale=4, train_wall=23, gb_free=10.7, wall=38097
2021-05-06 03:12:26 | INFO | train_inner | epoch 003:   3769 / 60421 loss=0.935, ppl=1.91, wps=16451.5, ups=4.45, wpb=3693.4, bsz=129.1, num_updates=124600, lr=8.95862e-05, gnorm=1.022, loss_scale=4, train_wall=22, gb_free=11, wall=38120
2021-05-06 03:12:48 | INFO | train_inner | epoch 003:   3869 / 60421 loss=0.982, ppl=1.98, wps=16520.3, ups=4.45, wpb=3713, bsz=124.9, num_updates=124700, lr=8.95502e-05, gnorm=1.23, loss_scale=4, train_wall=22, gb_free=10.5, wall=38142
2021-05-06 03:13:11 | INFO | train_inner | epoch 003:   3969 / 60421 loss=0.899, ppl=1.86, wps=16299.4, ups=4.42, wpb=3690.7, bsz=136, num_updates=124800, lr=8.95144e-05, gnorm=1.189, loss_scale=4, train_wall=22, gb_free=10.9, wall=38165
2021-05-06 03:13:34 | INFO | train_inner | epoch 003:   4069 / 60421 loss=0.996, ppl=1.99, wps=16499.8, ups=4.37, wpb=3778.1, bsz=112, num_updates=124900, lr=8.94785e-05, gnorm=1.103, loss_scale=4, train_wall=23, gb_free=10.8, wall=38188
2021-05-06 03:13:57 | INFO | train_inner | epoch 003:   4169 / 60421 loss=0.934, ppl=1.91, wps=16399.9, ups=4.33, wpb=3787.9, bsz=136.5, num_updates=125000, lr=8.94427e-05, gnorm=1.212, loss_scale=4, train_wall=23, gb_free=10.7, wall=38211
2021-05-06 03:13:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:14:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:14:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:14:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:14:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:14:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:14:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:14:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:14:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:14:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:15:03 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.965 | ppl 31.23 | bleu 29 | wps 2402.3 | wpb 2024.1 | bsz 97.5 | num_updates 125000 | best_bleu 29.36
2021-05-06 03:15:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 125000 updates
2021-05-06 03:15:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_125000.pt
2021-05-06 03:15:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_125000.pt
2021-05-06 03:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_125000.pt (epoch 3 @ 125000 updates, score 29.0) (writing took 7.981234250997659 seconds)
2021-05-06 03:15:34 | INFO | train_inner | epoch 003:   4269 / 60421 loss=0.975, ppl=1.97, wps=3828.5, ups=1.03, wpb=3725.5, bsz=122.2, num_updates=125100, lr=8.9407e-05, gnorm=1.135, loss_scale=4, train_wall=23, gb_free=11, wall=38308
2021-05-06 03:15:58 | INFO | train_inner | epoch 003:   4369 / 60421 loss=0.975, ppl=1.97, wps=15834.4, ups=4.24, wpb=3731.4, bsz=121.4, num_updates=125200, lr=8.93713e-05, gnorm=1.109, loss_scale=4, train_wall=23, gb_free=10.8, wall=38332
2021-05-06 03:16:21 | INFO | train_inner | epoch 003:   4469 / 60421 loss=0.994, ppl=1.99, wps=15948.2, ups=4.42, wpb=3609.4, bsz=115.7, num_updates=125300, lr=8.93356e-05, gnorm=1.292, loss_scale=4, train_wall=22, gb_free=10.6, wall=38354
2021-05-06 03:16:44 | INFO | train_inner | epoch 003:   4569 / 60421 loss=0.978, ppl=1.97, wps=16499.1, ups=4.34, wpb=3801.9, bsz=125, num_updates=125400, lr=8.93e-05, gnorm=1.039, loss_scale=4, train_wall=23, gb_free=10.7, wall=38377
2021-05-06 03:17:06 | INFO | train_inner | epoch 003:   4669 / 60421 loss=0.971, ppl=1.96, wps=16520.2, ups=4.37, wpb=3782.4, bsz=137.8, num_updates=125500, lr=8.92644e-05, gnorm=1.086, loss_scale=4, train_wall=23, gb_free=10.8, wall=38400
2021-05-06 03:17:29 | INFO | train_inner | epoch 003:   4769 / 60421 loss=0.99, ppl=1.99, wps=16598.1, ups=4.44, wpb=3738.6, bsz=123.9, num_updates=125600, lr=8.92288e-05, gnorm=1.165, loss_scale=4, train_wall=22, gb_free=10.7, wall=38423
2021-05-06 03:17:51 | INFO | train_inner | epoch 003:   4869 / 60421 loss=0.913, ppl=1.88, wps=16466.8, ups=4.46, wpb=3694.2, bsz=136.7, num_updates=125700, lr=8.91933e-05, gnorm=1.083, loss_scale=4, train_wall=22, gb_free=10.9, wall=38445
2021-05-06 03:18:14 | INFO | train_inner | epoch 003:   4969 / 60421 loss=0.936, ppl=1.91, wps=16491.6, ups=4.43, wpb=3723.4, bsz=141.8, num_updates=125800, lr=8.91579e-05, gnorm=1.078, loss_scale=4, train_wall=22, gb_free=10.8, wall=38468
2021-05-06 03:18:36 | INFO | train_inner | epoch 003:   5069 / 60421 loss=1.008, ppl=2.01, wps=16602.2, ups=4.46, wpb=3724.5, bsz=112.9, num_updates=125900, lr=8.91225e-05, gnorm=1.12, loss_scale=4, train_wall=22, gb_free=10.8, wall=38490
2021-05-06 03:18:59 | INFO | train_inner | epoch 003:   5169 / 60421 loss=0.955, ppl=1.94, wps=16564.6, ups=4.43, wpb=3740.9, bsz=130, num_updates=126000, lr=8.90871e-05, gnorm=1.245, loss_scale=4, train_wall=22, gb_free=10.8, wall=38513
2021-05-06 03:18:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:19:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:19:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:19:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:19:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:19:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:19:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:19:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:19:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:19:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:19:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:19:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:19:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:19:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:19:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:19:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:20:03 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.963 | ppl 31.2 | bleu 28.7 | wps 2460.9 | wpb 2024.1 | bsz 97.5 | num_updates 126000 | best_bleu 29.36
2021-05-06 03:20:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 126000 updates
2021-05-06 03:20:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_126000.pt
2021-05-06 03:20:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_126000.pt
2021-05-06 03:20:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_126000.pt (epoch 3 @ 126000 updates, score 28.7) (writing took 7.999153073003981 seconds)
2021-05-06 03:20:34 | INFO | train_inner | epoch 003:   5269 / 60421 loss=1.004, ppl=2.01, wps=3975.5, ups=1.05, wpb=3788, bsz=118.6, num_updates=126100, lr=8.90517e-05, gnorm=1.123, loss_scale=8, train_wall=23, gb_free=10.8, wall=38608
2021-05-06 03:20:57 | INFO | train_inner | epoch 003:   5369 / 60421 loss=0.951, ppl=1.93, wps=16667.7, ups=4.46, wpb=3735, bsz=126.6, num_updates=126200, lr=8.90165e-05, gnorm=1.228, loss_scale=8, train_wall=22, gb_free=10.7, wall=38631
2021-05-06 03:21:19 | INFO | train_inner | epoch 003:   5469 / 60421 loss=0.949, ppl=1.93, wps=16658.7, ups=4.44, wpb=3751.8, bsz=132.5, num_updates=126300, lr=8.89812e-05, gnorm=1.144, loss_scale=8, train_wall=22, gb_free=10.7, wall=38653
2021-05-06 03:21:42 | INFO | train_inner | epoch 003:   5569 / 60421 loss=0.984, ppl=1.98, wps=16656.3, ups=4.44, wpb=3749.5, bsz=137.4, num_updates=126400, lr=8.8946e-05, gnorm=1.224, loss_scale=8, train_wall=22, gb_free=10.7, wall=38676
2021-05-06 03:22:04 | INFO | train_inner | epoch 003:   5669 / 60421 loss=0.914, ppl=1.88, wps=16491.4, ups=4.48, wpb=3681.2, bsz=128.4, num_updates=126500, lr=8.89108e-05, gnorm=1.204, loss_scale=8, train_wall=22, gb_free=10.8, wall=38698
2021-05-06 03:22:27 | INFO | train_inner | epoch 003:   5769 / 60421 loss=0.938, ppl=1.92, wps=16556.5, ups=4.43, wpb=3736.8, bsz=152.5, num_updates=126600, lr=8.88757e-05, gnorm=1.221, loss_scale=8, train_wall=22, gb_free=10.6, wall=38721
2021-05-06 03:22:50 | INFO | train_inner | epoch 003:   5869 / 60421 loss=0.943, ppl=1.92, wps=16626.6, ups=4.36, wpb=3810.2, bsz=147, num_updates=126700, lr=8.88406e-05, gnorm=1.015, loss_scale=8, train_wall=23, gb_free=10.9, wall=38743
2021-05-06 03:23:12 | INFO | train_inner | epoch 003:   5969 / 60421 loss=0.984, ppl=1.98, wps=16542.3, ups=4.36, wpb=3791.9, bsz=130.5, num_updates=126800, lr=8.88056e-05, gnorm=1.19, loss_scale=8, train_wall=23, gb_free=10.7, wall=38766
2021-05-06 03:23:35 | INFO | train_inner | epoch 003:   6069 / 60421 loss=0.898, ppl=1.86, wps=16341.1, ups=4.39, wpb=3720.6, bsz=139.1, num_updates=126900, lr=8.87706e-05, gnorm=1.138, loss_scale=8, train_wall=23, gb_free=10.9, wall=38789
2021-05-06 03:23:58 | INFO | train_inner | epoch 003:   6169 / 60421 loss=0.942, ppl=1.92, wps=16393.9, ups=4.38, wpb=3742.7, bsz=151.3, num_updates=127000, lr=8.87357e-05, gnorm=1.341, loss_scale=8, train_wall=23, gb_free=10.7, wall=38812
2021-05-06 03:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:24:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:24:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:24:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:24:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:24:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:24:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:24:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:24:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:24:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:24:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:24:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:24:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:25:03 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.006 | ppl 32.13 | bleu 28.79 | wps 2434.3 | wpb 2024.1 | bsz 97.5 | num_updates 127000 | best_bleu 29.36
2021-05-06 03:25:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 127000 updates
2021-05-06 03:25:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_127000.pt
2021-05-06 03:25:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_127000.pt
2021-05-06 03:25:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_127000.pt (epoch 3 @ 127000 updates, score 28.79) (writing took 7.945559754996793 seconds)
2021-05-06 03:25:35 | INFO | train_inner | epoch 003:   6269 / 60421 loss=0.938, ppl=1.92, wps=3914.6, ups=1.03, wpb=3783, bsz=148.6, num_updates=127100, lr=8.87007e-05, gnorm=1.229, loss_scale=8, train_wall=23, gb_free=10.7, wall=38909
2021-05-06 03:25:58 | INFO | train_inner | epoch 003:   6369 / 60421 loss=0.94, ppl=1.92, wps=15934.2, ups=4.25, wpb=3751.1, bsz=141.9, num_updates=127200, lr=8.86659e-05, gnorm=1.09, loss_scale=8, train_wall=23, gb_free=10.8, wall=38932
2021-05-06 03:26:22 | INFO | train_inner | epoch 003:   6469 / 60421 loss=0.985, ppl=1.98, wps=16235.8, ups=4.29, wpb=3784.4, bsz=137.2, num_updates=127300, lr=8.8631e-05, gnorm=1.061, loss_scale=8, train_wall=23, gb_free=10.7, wall=38955
2021-05-06 03:26:44 | INFO | train_inner | epoch 003:   6569 / 60421 loss=0.961, ppl=1.95, wps=16553.3, ups=4.38, wpb=3776.1, bsz=133.3, num_updates=127400, lr=8.85962e-05, gnorm=1.147, loss_scale=8, train_wall=23, gb_free=10.8, wall=38978
2021-05-06 03:27:07 | INFO | train_inner | epoch 003:   6669 / 60421 loss=0.897, ppl=1.86, wps=16600.4, ups=4.34, wpb=3827.1, bsz=151.1, num_updates=127500, lr=8.85615e-05, gnorm=1.14, loss_scale=8, train_wall=23, gb_free=10.9, wall=39001
2021-05-06 03:27:30 | INFO | train_inner | epoch 003:   6769 / 60421 loss=0.953, ppl=1.94, wps=16553, ups=4.42, wpb=3748.8, bsz=137.8, num_updates=127600, lr=8.85268e-05, gnorm=1.342, loss_scale=8, train_wall=22, gb_free=10.8, wall=39024
2021-05-06 03:27:52 | INFO | train_inner | epoch 003:   6869 / 60421 loss=0.926, ppl=1.9, wps=16446.2, ups=4.48, wpb=3673, bsz=132.8, num_updates=127700, lr=8.84921e-05, gnorm=1.138, loss_scale=8, train_wall=22, gb_free=10.7, wall=39046
2021-05-06 03:28:15 | INFO | train_inner | epoch 003:   6969 / 60421 loss=0.95, ppl=1.93, wps=16357.5, ups=4.43, wpb=3689.8, bsz=133, num_updates=127800, lr=8.84575e-05, gnorm=1.138, loss_scale=8, train_wall=22, gb_free=10.7, wall=39069
2021-05-06 03:28:38 | INFO | train_inner | epoch 003:   7069 / 60421 loss=0.936, ppl=1.91, wps=16339.7, ups=4.39, wpb=3717.8, bsz=133.6, num_updates=127900, lr=8.84229e-05, gnorm=1.163, loss_scale=8, train_wall=23, gb_free=10.8, wall=39092
2021-05-06 03:29:01 | INFO | train_inner | epoch 003:   7169 / 60421 loss=0.911, ppl=1.88, wps=16329.7, ups=4.38, wpb=3728.8, bsz=126.8, num_updates=128000, lr=8.83883e-05, gnorm=1.166, loss_scale=8, train_wall=23, gb_free=10.8, wall=39114
2021-05-06 03:29:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:29:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:29:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:29:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:29:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:29:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:29:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:29:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:29:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:29:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:29:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:29:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:29:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:29:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:29:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:29:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:30:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.019 | ppl 32.42 | bleu 29.46 | wps 2425.6 | wpb 2024.1 | bsz 97.5 | num_updates 128000 | best_bleu 29.46
2021-05-06 03:30:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 128000 updates
2021-05-06 03:30:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_128000.pt
2021-05-06 03:30:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_128000.pt
2021-05-06 03:30:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_128000.pt (epoch 3 @ 128000 updates, score 29.46) (writing took 14.650892328994814 seconds)
2021-05-06 03:30:44 | INFO | train_inner | epoch 003:   7269 / 60421 loss=0.95, ppl=1.93, wps=3603.1, ups=0.97, wpb=3724.3, bsz=134.1, num_updates=128100, lr=8.83538e-05, gnorm=1.177, loss_scale=8, train_wall=23, gb_free=10.8, wall=39218
2021-05-06 03:31:07 | INFO | train_inner | epoch 003:   7369 / 60421 loss=0.95, ppl=1.93, wps=15955.8, ups=4.28, wpb=3732.3, bsz=135.7, num_updates=128200, lr=8.83194e-05, gnorm=1.287, loss_scale=8, train_wall=23, gb_free=10.8, wall=39241
2021-05-06 03:31:30 | INFO | train_inner | epoch 003:   7469 / 60421 loss=0.986, ppl=1.98, wps=16368.3, ups=4.36, wpb=3755.4, bsz=139.7, num_updates=128300, lr=8.82849e-05, gnorm=1.119, loss_scale=8, train_wall=23, gb_free=10.8, wall=39264
2021-05-06 03:31:53 | INFO | train_inner | epoch 003:   7569 / 60421 loss=0.932, ppl=1.91, wps=16355, ups=4.45, wpb=3676, bsz=132.2, num_updates=128400, lr=8.82506e-05, gnorm=1.19, loss_scale=8, train_wall=22, gb_free=10.6, wall=39287
2021-05-06 03:32:15 | INFO | train_inner | epoch 003:   7669 / 60421 loss=0.907, ppl=1.87, wps=16457.7, ups=4.44, wpb=3705.9, bsz=138.2, num_updates=128500, lr=8.82162e-05, gnorm=1.044, loss_scale=8, train_wall=22, gb_free=10.8, wall=39309
2021-05-06 03:32:38 | INFO | train_inner | epoch 003:   7769 / 60421 loss=0.94, ppl=1.92, wps=16593.8, ups=4.38, wpb=3789.3, bsz=142.2, num_updates=128600, lr=8.81819e-05, gnorm=1.102, loss_scale=8, train_wall=23, gb_free=10.9, wall=39332
2021-05-06 03:33:01 | INFO | train_inner | epoch 003:   7869 / 60421 loss=0.917, ppl=1.89, wps=16384.1, ups=4.43, wpb=3695.4, bsz=126.1, num_updates=128700, lr=8.81476e-05, gnorm=1.262, loss_scale=8, train_wall=22, gb_free=10.9, wall=39355
2021-05-06 03:33:23 | INFO | train_inner | epoch 003:   7969 / 60421 loss=0.966, ppl=1.95, wps=16317.8, ups=4.43, wpb=3684.1, bsz=123.6, num_updates=128800, lr=8.81134e-05, gnorm=1.204, loss_scale=8, train_wall=22, gb_free=10.8, wall=39377
2021-05-06 03:33:46 | INFO | train_inner | epoch 003:   8069 / 60421 loss=0.921, ppl=1.89, wps=16453.7, ups=4.32, wpb=3809.1, bsz=162.2, num_updates=128900, lr=8.80792e-05, gnorm=1.029, loss_scale=8, train_wall=23, gb_free=10.8, wall=39400
2021-05-06 03:34:09 | INFO | train_inner | epoch 003:   8169 / 60421 loss=0.918, ppl=1.89, wps=16037.1, ups=4.41, wpb=3633.4, bsz=131.7, num_updates=129000, lr=8.80451e-05, gnorm=1.23, loss_scale=8, train_wall=22, gb_free=10.9, wall=39423
2021-05-06 03:34:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:34:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:34:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:34:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:34:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:34:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:34:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:34:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:34:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:34:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:34:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:34:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:34:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:34:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:34:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:34:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:35:14 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.004 | ppl 32.08 | bleu 29.28 | wps 2428.2 | wpb 2024.1 | bsz 97.5 | num_updates 129000 | best_bleu 29.46
2021-05-06 03:35:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 129000 updates
2021-05-06 03:35:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_129000.pt
2021-05-06 03:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_129000.pt
2021-05-06 03:35:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_129000.pt (epoch 3 @ 129000 updates, score 29.28) (writing took 7.940123431006214 seconds)
2021-05-06 03:35:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-06 03:35:46 | INFO | train_inner | epoch 003:   8270 / 60421 loss=1.002, ppl=2, wps=3862.4, ups=1.03, wpb=3742.3, bsz=118.7, num_updates=129100, lr=8.8011e-05, gnorm=1.188, loss_scale=4, train_wall=23, gb_free=10.9, wall=39520
2021-05-06 03:36:10 | INFO | train_inner | epoch 003:   8370 / 60421 loss=0.958, ppl=1.94, wps=16091.7, ups=4.23, wpb=3805.2, bsz=122.2, num_updates=129200, lr=8.79769e-05, gnorm=1.13, loss_scale=4, train_wall=23, gb_free=11, wall=39543
2021-05-06 03:36:33 | INFO | train_inner | epoch 003:   8470 / 60421 loss=0.987, ppl=1.98, wps=16558.9, ups=4.35, wpb=3807.1, bsz=128.1, num_updates=129300, lr=8.79429e-05, gnorm=1.04, loss_scale=4, train_wall=23, gb_free=10.6, wall=39566
2021-05-06 03:36:55 | INFO | train_inner | epoch 003:   8570 / 60421 loss=0.978, ppl=1.97, wps=16570.1, ups=4.37, wpb=3789.7, bsz=119.3, num_updates=129400, lr=8.79089e-05, gnorm=1.034, loss_scale=4, train_wall=23, gb_free=10.9, wall=39589
2021-05-06 03:37:18 | INFO | train_inner | epoch 003:   8670 / 60421 loss=0.976, ppl=1.97, wps=16560, ups=4.39, wpb=3775.4, bsz=127.1, num_updates=129500, lr=8.7875e-05, gnorm=1.05, loss_scale=4, train_wall=23, gb_free=10.7, wall=39612
2021-05-06 03:37:41 | INFO | train_inner | epoch 003:   8770 / 60421 loss=0.942, ppl=1.92, wps=16381, ups=4.42, wpb=3706.4, bsz=131.9, num_updates=129600, lr=8.7841e-05, gnorm=1.108, loss_scale=4, train_wall=22, gb_free=11.1, wall=39635
2021-05-06 03:38:04 | INFO | train_inner | epoch 003:   8870 / 60421 loss=1.013, ppl=2.02, wps=16575.5, ups=4.39, wpb=3773.8, bsz=127.7, num_updates=129700, lr=8.78072e-05, gnorm=1.123, loss_scale=4, train_wall=23, gb_free=11, wall=39658
2021-05-06 03:38:27 | INFO | train_inner | epoch 003:   8970 / 60421 loss=0.924, ppl=1.9, wps=16454, ups=4.36, wpb=3769.9, bsz=147.8, num_updates=129800, lr=8.77733e-05, gnorm=1, loss_scale=4, train_wall=23, gb_free=10.6, wall=39680
2021-05-06 03:38:49 | INFO | train_inner | epoch 003:   9070 / 60421 loss=0.985, ppl=1.98, wps=16178.3, ups=4.42, wpb=3661.4, bsz=122.4, num_updates=129900, lr=8.77396e-05, gnorm=1.258, loss_scale=4, train_wall=22, gb_free=10.9, wall=39703
2021-05-06 03:39:12 | INFO | train_inner | epoch 003:   9170 / 60421 loss=0.939, ppl=1.92, wps=16267.6, ups=4.45, wpb=3658.2, bsz=125.4, num_updates=130000, lr=8.77058e-05, gnorm=1.546, loss_scale=4, train_wall=22, gb_free=10.9, wall=39726
2021-05-06 03:39:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:39:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:39:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:39:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:39:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:39:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:39:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:39:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:39:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:39:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:39:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:39:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:39:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:39:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:39:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:39:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:40:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.095 | ppl 34.18 | bleu 28.96 | wps 2436.5 | wpb 2024.1 | bsz 97.5 | num_updates 130000 | best_bleu 29.46
2021-05-06 03:40:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 130000 updates
2021-05-06 03:40:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_130000.pt
2021-05-06 03:40:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_130000.pt
2021-05-06 03:40:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_130000.pt (epoch 3 @ 130000 updates, score 28.96) (writing took 7.888326321000932 seconds)
2021-05-06 03:40:49 | INFO | train_inner | epoch 003:   9270 / 60421 loss=0.94, ppl=1.92, wps=3899.1, ups=1.03, wpb=3779.1, bsz=145.2, num_updates=130100, lr=8.76721e-05, gnorm=1.086, loss_scale=4, train_wall=24, gb_free=10.8, wall=39822
2021-05-06 03:41:12 | INFO | train_inner | epoch 003:   9370 / 60421 loss=0.949, ppl=1.93, wps=15908, ups=4.27, wpb=3727.6, bsz=139.4, num_updates=130200, lr=8.76384e-05, gnorm=1.117, loss_scale=4, train_wall=23, gb_free=10.8, wall=39846
2021-05-06 03:41:35 | INFO | train_inner | epoch 003:   9470 / 60421 loss=0.995, ppl=1.99, wps=16324.6, ups=4.39, wpb=3715.6, bsz=127.6, num_updates=130300, lr=8.76048e-05, gnorm=1.205, loss_scale=4, train_wall=23, gb_free=10.8, wall=39869
2021-05-06 03:41:58 | INFO | train_inner | epoch 003:   9570 / 60421 loss=0.925, ppl=1.9, wps=16506.2, ups=4.36, wpb=3781.7, bsz=141.1, num_updates=130400, lr=8.75712e-05, gnorm=1.087, loss_scale=4, train_wall=23, gb_free=10.7, wall=39892
2021-05-06 03:42:21 | INFO | train_inner | epoch 003:   9670 / 60421 loss=0.978, ppl=1.97, wps=16605.9, ups=4.39, wpb=3785.2, bsz=121.2, num_updates=130500, lr=8.75376e-05, gnorm=1.094, loss_scale=4, train_wall=23, gb_free=10.7, wall=39914
2021-05-06 03:42:43 | INFO | train_inner | epoch 003:   9770 / 60421 loss=0.943, ppl=1.92, wps=16514.1, ups=4.37, wpb=3774.7, bsz=135.5, num_updates=130600, lr=8.75041e-05, gnorm=0.98, loss_scale=4, train_wall=23, gb_free=10.6, wall=39937
2021-05-06 03:43:06 | INFO | train_inner | epoch 003:   9870 / 60421 loss=0.911, ppl=1.88, wps=16271.2, ups=4.44, wpb=3668.1, bsz=139.6, num_updates=130700, lr=8.74706e-05, gnorm=1.114, loss_scale=4, train_wall=22, gb_free=10.9, wall=39960
2021-05-06 03:43:29 | INFO | train_inner | epoch 003:   9970 / 60421 loss=0.949, ppl=1.93, wps=16638, ups=4.35, wpb=3822.5, bsz=141.8, num_updates=130800, lr=8.74372e-05, gnorm=0.972, loss_scale=4, train_wall=23, gb_free=10.9, wall=39983
2021-05-06 03:43:52 | INFO | train_inner | epoch 003:  10070 / 60421 loss=0.962, ppl=1.95, wps=16393.1, ups=4.38, wpb=3740.8, bsz=126.4, num_updates=130900, lr=8.74038e-05, gnorm=1.186, loss_scale=4, train_wall=23, gb_free=10.7, wall=40006
2021-05-06 03:44:15 | INFO | train_inner | epoch 003:  10170 / 60421 loss=0.96, ppl=1.95, wps=16314.1, ups=4.38, wpb=3727.1, bsz=123.8, num_updates=131000, lr=8.73704e-05, gnorm=1.267, loss_scale=4, train_wall=23, gb_free=10.8, wall=40028
2021-05-06 03:44:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:44:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:44:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:44:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:44:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:44:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:44:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:45:20 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.01 | ppl 32.22 | bleu 29.35 | wps 2423 | wpb 2024.1 | bsz 97.5 | num_updates 131000 | best_bleu 29.46
2021-05-06 03:45:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 131000 updates
2021-05-06 03:45:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_131000.pt
2021-05-06 03:45:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_131000.pt
2021-05-06 03:45:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_131000.pt (epoch 3 @ 131000 updates, score 29.35) (writing took 7.89885426001274 seconds)
2021-05-06 03:45:51 | INFO | train_inner | epoch 003:  10270 / 60421 loss=0.969, ppl=1.96, wps=3808.1, ups=1.03, wpb=3687.6, bsz=137.3, num_updates=131100, lr=8.73371e-05, gnorm=1.389, loss_scale=4, train_wall=23, gb_free=10.8, wall=40125
2021-05-06 03:46:15 | INFO | train_inner | epoch 003:  10370 / 60421 loss=0.928, ppl=1.9, wps=16070.6, ups=4.27, wpb=3761.8, bsz=165.8, num_updates=131200, lr=8.73038e-05, gnorm=1.066, loss_scale=4, train_wall=23, gb_free=10.8, wall=40149
2021-05-06 03:46:38 | INFO | train_inner | epoch 003:  10470 / 60421 loss=0.945, ppl=1.93, wps=16517, ups=4.38, wpb=3774.8, bsz=144.2, num_updates=131300, lr=8.72705e-05, gnorm=1.073, loss_scale=4, train_wall=23, gb_free=10.7, wall=40172
2021-05-06 03:47:00 | INFO | train_inner | epoch 003:  10570 / 60421 loss=0.954, ppl=1.94, wps=16307.3, ups=4.43, wpb=3681.3, bsz=131.5, num_updates=131400, lr=8.72373e-05, gnorm=1.121, loss_scale=4, train_wall=22, gb_free=10.7, wall=40194
2021-05-06 03:47:23 | INFO | train_inner | epoch 003:  10670 / 60421 loss=1.031, ppl=2.04, wps=16440.5, ups=4.44, wpb=3703, bsz=114.2, num_updates=131500, lr=8.72041e-05, gnorm=1.362, loss_scale=4, train_wall=22, gb_free=10.7, wall=40217
2021-05-06 03:47:45 | INFO | train_inner | epoch 003:  10770 / 60421 loss=0.996, ppl=1.99, wps=16400.5, ups=4.49, wpb=3652.2, bsz=105.4, num_updates=131600, lr=8.7171e-05, gnorm=1.143, loss_scale=4, train_wall=22, gb_free=10.8, wall=40239
2021-05-06 03:48:08 | INFO | train_inner | epoch 003:  10870 / 60421 loss=0.937, ppl=1.91, wps=16535.7, ups=4.39, wpb=3764.8, bsz=129.4, num_updates=131700, lr=8.71379e-05, gnorm=1.003, loss_scale=4, train_wall=23, gb_free=11, wall=40262
2021-05-06 03:48:30 | INFO | train_inner | epoch 003:  10970 / 60421 loss=0.954, ppl=1.94, wps=16429.4, ups=4.42, wpb=3720.1, bsz=124, num_updates=131800, lr=8.71048e-05, gnorm=1.109, loss_scale=4, train_wall=22, gb_free=11.1, wall=40284
2021-05-06 03:48:53 | INFO | train_inner | epoch 003:  11070 / 60421 loss=0.893, ppl=1.86, wps=16224.6, ups=4.39, wpb=3691.8, bsz=148.8, num_updates=131900, lr=8.70718e-05, gnorm=1.16, loss_scale=4, train_wall=23, gb_free=10.8, wall=40307
2021-05-06 03:49:16 | INFO | train_inner | epoch 003:  11170 / 60421 loss=0.973, ppl=1.96, wps=16240, ups=4.41, wpb=3685.8, bsz=128.4, num_updates=132000, lr=8.70388e-05, gnorm=1.355, loss_scale=4, train_wall=23, gb_free=10.9, wall=40330
2021-05-06 03:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:49:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:49:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:49:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:49:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:49:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:49:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:49:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:49:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:49:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:49:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:49:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:49:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:49:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:49:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:49:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:50:21 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.981 | ppl 31.58 | bleu 29.13 | wps 2432.2 | wpb 2024.1 | bsz 97.5 | num_updates 132000 | best_bleu 29.46
2021-05-06 03:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 132000 updates
2021-05-06 03:50:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_132000.pt
2021-05-06 03:50:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_132000.pt
2021-05-06 03:50:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_132000.pt (epoch 3 @ 132000 updates, score 29.13) (writing took 7.941968006984098 seconds)
2021-05-06 03:50:53 | INFO | train_inner | epoch 003:  11270 / 60421 loss=0.963, ppl=1.95, wps=3925.2, ups=1.03, wpb=3798.1, bsz=124.6, num_updates=132100, lr=8.70059e-05, gnorm=1.146, loss_scale=4, train_wall=23, gb_free=10.9, wall=40427
2021-05-06 03:51:16 | INFO | train_inner | epoch 003:  11370 / 60421 loss=0.977, ppl=1.97, wps=16228, ups=4.3, wpb=3774.1, bsz=141.1, num_updates=132200, lr=8.6973e-05, gnorm=1.148, loss_scale=4, train_wall=23, gb_free=10.7, wall=40450
2021-05-06 03:51:39 | INFO | train_inner | epoch 003:  11470 / 60421 loss=0.961, ppl=1.95, wps=16399.6, ups=4.35, wpb=3772.8, bsz=130.6, num_updates=132300, lr=8.69401e-05, gnorm=1.138, loss_scale=4, train_wall=23, gb_free=10.9, wall=40473
2021-05-06 03:52:01 | INFO | train_inner | epoch 003:  11570 / 60421 loss=0.965, ppl=1.95, wps=16336.6, ups=4.44, wpb=3679.4, bsz=121.2, num_updates=132400, lr=8.69072e-05, gnorm=1.248, loss_scale=4, train_wall=22, gb_free=10.8, wall=40495
2021-05-06 03:52:24 | INFO | train_inner | epoch 003:  11670 / 60421 loss=0.983, ppl=1.98, wps=16472.8, ups=4.41, wpb=3737.4, bsz=125.3, num_updates=132500, lr=8.68744e-05, gnorm=1.271, loss_scale=4, train_wall=23, gb_free=10.8, wall=40518
2021-05-06 03:52:47 | INFO | train_inner | epoch 003:  11770 / 60421 loss=0.909, ppl=1.88, wps=16541.2, ups=4.37, wpb=3782.4, bsz=161, num_updates=132600, lr=8.68417e-05, gnorm=1.138, loss_scale=4, train_wall=23, gb_free=10.8, wall=40541
2021-05-06 03:53:10 | INFO | train_inner | epoch 003:  11870 / 60421 loss=0.936, ppl=1.91, wps=16318.3, ups=4.4, wpb=3712.3, bsz=125, num_updates=132700, lr=8.6809e-05, gnorm=1.115, loss_scale=4, train_wall=23, gb_free=10.8, wall=40564
2021-05-06 03:53:33 | INFO | train_inner | epoch 003:  11970 / 60421 loss=0.99, ppl=1.99, wps=16304.7, ups=4.37, wpb=3734.7, bsz=112.1, num_updates=132800, lr=8.67763e-05, gnorm=1.158, loss_scale=4, train_wall=23, gb_free=11.1, wall=40587
2021-05-06 03:53:56 | INFO | train_inner | epoch 003:  12070 / 60421 loss=0.965, ppl=1.95, wps=16385.2, ups=4.33, wpb=3787, bsz=159.4, num_updates=132900, lr=8.67436e-05, gnorm=1.302, loss_scale=4, train_wall=23, gb_free=10.7, wall=40610
2021-05-06 03:54:19 | INFO | train_inner | epoch 003:  12170 / 60421 loss=0.995, ppl=1.99, wps=16277.8, ups=4.3, wpb=3785.3, bsz=125.8, num_updates=133000, lr=8.6711e-05, gnorm=1.16, loss_scale=4, train_wall=23, gb_free=10.9, wall=40633
2021-05-06 03:54:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:54:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:54:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:54:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:54:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:54:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:54:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:54:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:54:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:54:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:54:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:54:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:54:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:55:24 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.049 | ppl 33.1 | bleu 29.45 | wps 2422.4 | wpb 2024.1 | bsz 97.5 | num_updates 133000 | best_bleu 29.46
2021-05-06 03:55:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 133000 updates
2021-05-06 03:55:24 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_133000.pt
2021-05-06 03:55:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_133000.pt
2021-05-06 03:55:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_133000.pt (epoch 3 @ 133000 updates, score 29.45) (writing took 7.928768223995576 seconds)
2021-05-06 03:55:56 | INFO | train_inner | epoch 003:  12270 / 60421 loss=0.954, ppl=1.94, wps=3845.6, ups=1.03, wpb=3734.4, bsz=137.9, num_updates=133100, lr=8.66784e-05, gnorm=1.311, loss_scale=4, train_wall=23, gb_free=10.7, wall=40730
2021-05-06 03:56:19 | INFO | train_inner | epoch 003:  12370 / 60421 loss=0.968, ppl=1.96, wps=16305.5, ups=4.3, wpb=3793.4, bsz=123.7, num_updates=133200, lr=8.66459e-05, gnorm=1.116, loss_scale=4, train_wall=23, gb_free=10.8, wall=40753
2021-05-06 03:56:42 | INFO | train_inner | epoch 003:  12470 / 60421 loss=1.01, ppl=2.01, wps=16632, ups=4.34, wpb=3833.7, bsz=133, num_updates=133300, lr=8.66134e-05, gnorm=1.238, loss_scale=4, train_wall=23, gb_free=10.8, wall=40776
2021-05-06 03:57:05 | INFO | train_inner | epoch 003:  12570 / 60421 loss=0.968, ppl=1.96, wps=16505.8, ups=4.44, wpb=3714.8, bsz=113.8, num_updates=133400, lr=8.65809e-05, gnorm=1.154, loss_scale=4, train_wall=22, gb_free=10.9, wall=40799
2021-05-06 03:57:28 | INFO | train_inner | epoch 003:  12670 / 60421 loss=0.875, ppl=1.83, wps=16416.6, ups=4.38, wpb=3751.6, bsz=148.8, num_updates=133500, lr=8.65485e-05, gnorm=1.005, loss_scale=4, train_wall=23, gb_free=10.6, wall=40822
2021-05-06 03:57:50 | INFO | train_inner | epoch 003:  12770 / 60421 loss=0.924, ppl=1.9, wps=16292, ups=4.48, wpb=3632.9, bsz=141.3, num_updates=133600, lr=8.65161e-05, gnorm=1.39, loss_scale=4, train_wall=22, gb_free=10.9, wall=40844
2021-05-06 03:58:13 | INFO | train_inner | epoch 003:  12870 / 60421 loss=0.935, ppl=1.91, wps=16549.5, ups=4.37, wpb=3790, bsz=138.8, num_updates=133700, lr=8.64837e-05, gnorm=1.18, loss_scale=4, train_wall=23, gb_free=10.6, wall=40867
2021-05-06 03:58:36 | INFO | train_inner | epoch 003:  12970 / 60421 loss=1.02, ppl=2.03, wps=16468, ups=4.37, wpb=3768.1, bsz=117.8, num_updates=133800, lr=8.64514e-05, gnorm=1.203, loss_scale=4, train_wall=23, gb_free=10.9, wall=40890
2021-05-06 03:58:59 | INFO | train_inner | epoch 003:  13070 / 60421 loss=0.958, ppl=1.94, wps=16485.6, ups=4.34, wpb=3800.6, bsz=137, num_updates=133900, lr=8.64191e-05, gnorm=1.068, loss_scale=4, train_wall=23, gb_free=10.9, wall=40913
2021-05-06 03:59:22 | INFO | train_inner | epoch 003:  13170 / 60421 loss=0.949, ppl=1.93, wps=16165, ups=4.38, wpb=3690.4, bsz=124.5, num_updates=134000, lr=8.63868e-05, gnorm=1.151, loss_scale=4, train_wall=23, gb_free=10.7, wall=40936
2021-05-06 03:59:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 03:59:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:59:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:59:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:59:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:59:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:59:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:59:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:59:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:59:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:59:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:59:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:59:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 03:59:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 03:59:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 03:59:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:00:27 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.038 | ppl 32.86 | bleu 29.31 | wps 2448.6 | wpb 2024.1 | bsz 97.5 | num_updates 134000 | best_bleu 29.46
2021-05-06 04:00:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 134000 updates
2021-05-06 04:00:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_134000.pt
2021-05-06 04:00:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_134000.pt
2021-05-06 04:00:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_134000.pt (epoch 3 @ 134000 updates, score 29.31) (writing took 7.8914202580053825 seconds)
2021-05-06 04:00:58 | INFO | train_inner | epoch 003:  13270 / 60421 loss=0.969, ppl=1.96, wps=3926.9, ups=1.04, wpb=3786.8, bsz=134.3, num_updates=134100, lr=8.63546e-05, gnorm=1.123, loss_scale=4, train_wall=24, gb_free=10.8, wall=41032
2021-05-06 04:01:21 | INFO | train_inner | epoch 003:  13370 / 60421 loss=0.92, ppl=1.89, wps=16167.6, ups=4.33, wpb=3732.2, bsz=139.9, num_updates=134200, lr=8.63224e-05, gnorm=1.234, loss_scale=4, train_wall=23, gb_free=10.8, wall=41055
2021-05-06 04:01:44 | INFO | train_inner | epoch 003:  13470 / 60421 loss=0.988, ppl=1.98, wps=16380.9, ups=4.41, wpb=3716.6, bsz=120.5, num_updates=134300, lr=8.62903e-05, gnorm=1.255, loss_scale=4, train_wall=23, gb_free=10.7, wall=41078
2021-05-06 04:02:07 | INFO | train_inner | epoch 003:  13570 / 60421 loss=0.951, ppl=1.93, wps=16558.8, ups=4.36, wpb=3795.7, bsz=126, num_updates=134400, lr=8.62582e-05, gnorm=0.995, loss_scale=4, train_wall=23, gb_free=10.8, wall=41101
2021-05-06 04:02:30 | INFO | train_inner | epoch 003:  13670 / 60421 loss=0.893, ppl=1.86, wps=16315.6, ups=4.4, wpb=3705.9, bsz=156.9, num_updates=134500, lr=8.62261e-05, gnorm=1.266, loss_scale=4, train_wall=23, gb_free=10.8, wall=41123
2021-05-06 04:02:52 | INFO | train_inner | epoch 003:  13770 / 60421 loss=0.942, ppl=1.92, wps=16430.4, ups=4.44, wpb=3697.5, bsz=128.5, num_updates=134600, lr=8.61941e-05, gnorm=1.107, loss_scale=4, train_wall=22, gb_free=10.9, wall=41146
2021-05-06 04:03:15 | INFO | train_inner | epoch 003:  13870 / 60421 loss=1.009, ppl=2.01, wps=16494.2, ups=4.42, wpb=3734, bsz=136.6, num_updates=134700, lr=8.61621e-05, gnorm=1.178, loss_scale=4, train_wall=22, gb_free=11.2, wall=41169
2021-05-06 04:03:38 | INFO | train_inner | epoch 003:  13970 / 60421 loss=0.959, ppl=1.94, wps=16494.7, ups=4.35, wpb=3788.4, bsz=121.5, num_updates=134800, lr=8.61301e-05, gnorm=1.028, loss_scale=4, train_wall=23, gb_free=11.2, wall=41192
2021-05-06 04:04:01 | INFO | train_inner | epoch 003:  14070 / 60421 loss=0.994, ppl=1.99, wps=16540, ups=4.36, wpb=3795.4, bsz=119.4, num_updates=134900, lr=8.60982e-05, gnorm=1.134, loss_scale=4, train_wall=23, gb_free=10.7, wall=41215
2021-05-06 04:04:23 | INFO | train_inner | epoch 003:  14170 / 60421 loss=0.952, ppl=1.93, wps=16236.5, ups=4.39, wpb=3699.2, bsz=119.4, num_updates=135000, lr=8.60663e-05, gnorm=0.987, loss_scale=4, train_wall=23, gb_free=10.7, wall=41237
2021-05-06 04:04:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:04:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:04:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:04:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:04:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:04:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:04:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:04:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:04:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:04:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:04:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:04:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:04:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:04:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:04:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:04:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:05:29 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.05 | ppl 33.12 | bleu 29.47 | wps 2430.9 | wpb 2024.1 | bsz 97.5 | num_updates 135000 | best_bleu 29.47
2021-05-06 04:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 135000 updates
2021-05-06 04:05:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_135000.pt
2021-05-06 04:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_135000.pt
2021-05-06 04:05:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_135000.pt (epoch 3 @ 135000 updates, score 29.47) (writing took 14.617131487990264 seconds)
2021-05-06 04:06:07 | INFO | train_inner | epoch 003:  14270 / 60421 loss=0.935, ppl=1.91, wps=3596.4, ups=0.97, wpb=3707.1, bsz=120.2, num_updates=135100, lr=8.60344e-05, gnorm=1.094, loss_scale=4, train_wall=23, gb_free=10.8, wall=41340
2021-05-06 04:06:29 | INFO | train_inner | epoch 003:  14370 / 60421 loss=0.936, ppl=1.91, wps=16519.3, ups=4.36, wpb=3786.6, bsz=139, num_updates=135200, lr=8.60026e-05, gnorm=0.983, loss_scale=4, train_wall=23, gb_free=10.8, wall=41363
2021-05-06 04:06:52 | INFO | train_inner | epoch 003:  14470 / 60421 loss=0.973, ppl=1.96, wps=16451.2, ups=4.4, wpb=3741.9, bsz=130.1, num_updates=135300, lr=8.59708e-05, gnorm=1.186, loss_scale=4, train_wall=23, gb_free=10.9, wall=41386
2021-05-06 04:07:15 | INFO | train_inner | epoch 003:  14570 / 60421 loss=0.95, ppl=1.93, wps=16461.2, ups=4.37, wpb=3764.9, bsz=131, num_updates=135400, lr=8.59391e-05, gnorm=1.169, loss_scale=4, train_wall=23, gb_free=10.8, wall=41409
2021-05-06 04:07:38 | INFO | train_inner | epoch 003:  14670 / 60421 loss=0.975, ppl=1.97, wps=16606.2, ups=4.37, wpb=3799.1, bsz=137.3, num_updates=135500, lr=8.59074e-05, gnorm=1.124, loss_scale=4, train_wall=23, gb_free=10.9, wall=41432
2021-05-06 04:08:01 | INFO | train_inner | epoch 003:  14770 / 60421 loss=0.947, ppl=1.93, wps=16349.8, ups=4.41, wpb=3704.6, bsz=146.3, num_updates=135600, lr=8.58757e-05, gnorm=1.04, loss_scale=4, train_wall=22, gb_free=11.1, wall=41455
2021-05-06 04:08:23 | INFO | train_inner | epoch 003:  14870 / 60421 loss=0.955, ppl=1.94, wps=16580.9, ups=4.42, wpb=3755.2, bsz=129.5, num_updates=135700, lr=8.5844e-05, gnorm=1.074, loss_scale=4, train_wall=22, gb_free=10.7, wall=41477
2021-05-06 04:08:46 | INFO | train_inner | epoch 003:  14970 / 60421 loss=0.947, ppl=1.93, wps=16304.3, ups=4.35, wpb=3746.8, bsz=140.9, num_updates=135800, lr=8.58124e-05, gnorm=1.104, loss_scale=4, train_wall=23, gb_free=10.8, wall=41500
2021-05-06 04:09:09 | INFO | train_inner | epoch 003:  15070 / 60421 loss=0.958, ppl=1.94, wps=16199.7, ups=4.35, wpb=3727.1, bsz=119.1, num_updates=135900, lr=8.57808e-05, gnorm=1.224, loss_scale=4, train_wall=23, gb_free=11.2, wall=41523
2021-05-06 04:09:32 | INFO | train_inner | epoch 003:  15170 / 60421 loss=0.898, ppl=1.86, wps=16191.3, ups=4.39, wpb=3685.2, bsz=162.3, num_updates=136000, lr=8.57493e-05, gnorm=1.272, loss_scale=4, train_wall=23, gb_free=10.9, wall=41546
2021-05-06 04:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:09:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:09:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:09:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:09:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:09:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:09:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:09:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:09:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:09:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:09:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:09:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:09:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:09:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:09:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:09:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:10:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.036 | ppl 32.8 | bleu 29.27 | wps 2421.4 | wpb 2024.1 | bsz 97.5 | num_updates 136000 | best_bleu 29.47
2021-05-06 04:10:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 136000 updates
2021-05-06 04:10:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_136000.pt
2021-05-06 04:10:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_136000.pt
2021-05-06 04:10:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_136000.pt (epoch 3 @ 136000 updates, score 29.27) (writing took 8.506285709998338 seconds)
2021-05-06 04:11:10 | INFO | train_inner | epoch 003:  15270 / 60421 loss=0.927, ppl=1.9, wps=3808.6, ups=1.03, wpb=3714.5, bsz=146.6, num_updates=136100, lr=8.57178e-05, gnorm=1.237, loss_scale=4, train_wall=23, gb_free=10.8, wall=41643
2021-05-06 04:11:33 | INFO | train_inner | epoch 003:  15370 / 60421 loss=0.915, ppl=1.89, wps=16091.5, ups=4.35, wpb=3696.2, bsz=144.4, num_updates=136200, lr=8.56863e-05, gnorm=1.141, loss_scale=4, train_wall=23, gb_free=10.5, wall=41666
2021-05-06 04:11:55 | INFO | train_inner | epoch 003:  15470 / 60421 loss=0.931, ppl=1.91, wps=16560.2, ups=4.37, wpb=3789.5, bsz=133.8, num_updates=136300, lr=8.56549e-05, gnorm=0.991, loss_scale=4, train_wall=23, gb_free=10.8, wall=41689
2021-05-06 04:12:18 | INFO | train_inner | epoch 003:  15570 / 60421 loss=0.912, ppl=1.88, wps=16370.7, ups=4.41, wpb=3715.1, bsz=136.6, num_updates=136400, lr=8.56235e-05, gnorm=1.163, loss_scale=4, train_wall=23, gb_free=10.7, wall=41712
2021-05-06 04:12:41 | INFO | train_inner | epoch 003:  15670 / 60421 loss=0.893, ppl=1.86, wps=16432.8, ups=4.36, wpb=3772.2, bsz=149.8, num_updates=136500, lr=8.55921e-05, gnorm=1.166, loss_scale=4, train_wall=23, gb_free=10.9, wall=41735
2021-05-06 04:13:04 | INFO | train_inner | epoch 003:  15770 / 60421 loss=0.931, ppl=1.91, wps=16426.8, ups=4.43, wpb=3710.5, bsz=132.8, num_updates=136600, lr=8.55608e-05, gnorm=1.217, loss_scale=4, train_wall=22, gb_free=10.9, wall=41758
2021-05-06 04:13:26 | INFO | train_inner | epoch 003:  15870 / 60421 loss=0.921, ppl=1.89, wps=16407.6, ups=4.39, wpb=3735.2, bsz=128.4, num_updates=136700, lr=8.55295e-05, gnorm=1.078, loss_scale=4, train_wall=23, gb_free=10.9, wall=41780
2021-05-06 04:13:49 | INFO | train_inner | epoch 003:  15970 / 60421 loss=0.935, ppl=1.91, wps=16230.9, ups=4.42, wpb=3674, bsz=120.7, num_updates=136800, lr=8.54982e-05, gnorm=1.305, loss_scale=4, train_wall=22, gb_free=11, wall=41803
2021-05-06 04:14:12 | INFO | train_inner | epoch 003:  16070 / 60421 loss=0.863, ppl=1.82, wps=16081.5, ups=4.4, wpb=3656.6, bsz=129.2, num_updates=136900, lr=8.5467e-05, gnorm=1.199, loss_scale=4, train_wall=23, gb_free=10.7, wall=41826
2021-05-06 04:14:35 | INFO | train_inner | epoch 003:  16170 / 60421 loss=0.96, ppl=1.95, wps=16247.5, ups=4.27, wpb=3802.5, bsz=129.9, num_updates=137000, lr=8.54358e-05, gnorm=1.152, loss_scale=4, train_wall=23, gb_free=11, wall=41849
2021-05-06 04:14:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:14:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:14:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:14:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:14:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:14:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:14:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:14:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:14:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:14:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:14:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:14:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:14:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:14:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:14:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:14:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:15:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.08 | ppl 33.83 | bleu 29.39 | wps 2416.7 | wpb 2024.1 | bsz 97.5 | num_updates 137000 | best_bleu 29.47
2021-05-06 04:15:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 137000 updates
2021-05-06 04:15:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_137000.pt
2021-05-06 04:15:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_137000.pt
2021-05-06 04:15:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_137000.pt (epoch 3 @ 137000 updates, score 29.39) (writing took 7.891022112977225 seconds)
2021-05-06 04:16:12 | INFO | train_inner | epoch 003:  16270 / 60421 loss=0.952, ppl=1.93, wps=3810.8, ups=1.03, wpb=3689.8, bsz=143.9, num_updates=137100, lr=8.54046e-05, gnorm=1.169, loss_scale=4, train_wall=23, gb_free=10.9, wall=41946
2021-05-06 04:16:34 | INFO | train_inner | epoch 003:  16370 / 60421 loss=1.008, ppl=2.01, wps=16105, ups=4.48, wpb=3591.8, bsz=103.8, num_updates=137200, lr=8.53735e-05, gnorm=1.338, loss_scale=4, train_wall=22, gb_free=10.8, wall=41968
2021-05-06 04:16:57 | INFO | train_inner | epoch 003:  16470 / 60421 loss=1.008, ppl=2.01, wps=16558.8, ups=4.37, wpb=3792.7, bsz=142.8, num_updates=137300, lr=8.53424e-05, gnorm=1.047, loss_scale=4, train_wall=23, gb_free=10.8, wall=41991
2021-05-06 04:17:20 | INFO | train_inner | epoch 003:  16570 / 60421 loss=0.917, ppl=1.89, wps=16524.8, ups=4.42, wpb=3741.4, bsz=131.2, num_updates=137400, lr=8.53113e-05, gnorm=1.158, loss_scale=4, train_wall=22, gb_free=10.8, wall=42014
2021-05-06 04:17:43 | INFO | train_inner | epoch 003:  16670 / 60421 loss=0.913, ppl=1.88, wps=16522, ups=4.38, wpb=3768.8, bsz=150.7, num_updates=137500, lr=8.52803e-05, gnorm=1.141, loss_scale=4, train_wall=23, gb_free=10.7, wall=42037
2021-05-06 04:18:05 | INFO | train_inner | epoch 003:  16770 / 60421 loss=0.891, ppl=1.85, wps=16401.3, ups=4.45, wpb=3684.2, bsz=126.1, num_updates=137600, lr=8.52493e-05, gnorm=1.09, loss_scale=4, train_wall=22, gb_free=10.8, wall=42059
2021-05-06 04:18:28 | INFO | train_inner | epoch 003:  16870 / 60421 loss=0.954, ppl=1.94, wps=16427.8, ups=4.44, wpb=3701.3, bsz=123.8, num_updates=137700, lr=8.52183e-05, gnorm=1.036, loss_scale=4, train_wall=22, gb_free=11.3, wall=42082
2021-05-06 04:18:50 | INFO | train_inner | epoch 003:  16970 / 60421 loss=0.928, ppl=1.9, wps=16670.4, ups=4.4, wpb=3785.4, bsz=159.5, num_updates=137800, lr=8.51874e-05, gnorm=1.083, loss_scale=4, train_wall=23, gb_free=10.8, wall=42104
2021-05-06 04:19:13 | INFO | train_inner | epoch 003:  17070 / 60421 loss=1.003, ppl=2, wps=16818.4, ups=4.42, wpb=3806.2, bsz=146.2, num_updates=137900, lr=8.51565e-05, gnorm=1.229, loss_scale=4, train_wall=22, gb_free=10.8, wall=42127
2021-05-06 04:19:36 | INFO | train_inner | epoch 003:  17170 / 60421 loss=0.981, ppl=1.97, wps=16693.5, ups=4.44, wpb=3760.1, bsz=128.1, num_updates=138000, lr=8.51257e-05, gnorm=1.106, loss_scale=4, train_wall=22, gb_free=10.8, wall=42149
2021-05-06 04:19:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:19:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:19:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:19:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:19:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:19:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:19:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:19:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:19:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:19:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:19:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:19:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:19:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:19:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:19:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:19:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:20:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.028 | ppl 32.63 | bleu 29.38 | wps 2441.4 | wpb 2024.1 | bsz 97.5 | num_updates 138000 | best_bleu 29.47
2021-05-06 04:20:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 138000 updates
2021-05-06 04:20:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_138000.pt
2021-05-06 04:20:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_138000.pt
2021-05-06 04:20:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_138000.pt (epoch 3 @ 138000 updates, score 29.38) (writing took 7.926702773984289 seconds)
2021-05-06 04:21:11 | INFO | train_inner | epoch 003:  17270 / 60421 loss=0.965, ppl=1.95, wps=3945.1, ups=1.05, wpb=3773.6, bsz=138.6, num_updates=138100, lr=8.50948e-05, gnorm=1.136, loss_scale=4, train_wall=23, gb_free=10.7, wall=42245
2021-05-06 04:21:34 | INFO | train_inner | epoch 003:  17370 / 60421 loss=0.912, ppl=1.88, wps=16696.3, ups=4.39, wpb=3804.3, bsz=145.2, num_updates=138200, lr=8.5064e-05, gnorm=1.169, loss_scale=4, train_wall=23, gb_free=10.7, wall=42268
2021-05-06 04:21:56 | INFO | train_inner | epoch 003:  17470 / 60421 loss=0.961, ppl=1.95, wps=16372.5, ups=4.49, wpb=3648.4, bsz=116.3, num_updates=138300, lr=8.50333e-05, gnorm=1.134, loss_scale=4, train_wall=22, gb_free=10.9, wall=42290
2021-05-06 04:22:19 | INFO | train_inner | epoch 003:  17570 / 60421 loss=0.988, ppl=1.98, wps=16646.6, ups=4.42, wpb=3765.1, bsz=118.4, num_updates=138400, lr=8.50026e-05, gnorm=1.043, loss_scale=4, train_wall=22, gb_free=10.9, wall=42313
2021-05-06 04:22:42 | INFO | train_inner | epoch 003:  17670 / 60421 loss=0.964, ppl=1.95, wps=16580.6, ups=4.4, wpb=3768.6, bsz=129.8, num_updates=138500, lr=8.49719e-05, gnorm=1.101, loss_scale=4, train_wall=23, gb_free=10.8, wall=42336
2021-05-06 04:23:04 | INFO | train_inner | epoch 003:  17770 / 60421 loss=1.035, ppl=2.05, wps=16540.2, ups=4.4, wpb=3761.7, bsz=132, num_updates=138600, lr=8.49412e-05, gnorm=1.4, loss_scale=4, train_wall=23, gb_free=10.7, wall=42358
2021-05-06 04:23:27 | INFO | train_inner | epoch 003:  17870 / 60421 loss=0.96, ppl=1.95, wps=16361.4, ups=4.34, wpb=3769.7, bsz=137.5, num_updates=138700, lr=8.49106e-05, gnorm=1.327, loss_scale=4, train_wall=23, gb_free=10.8, wall=42381
2021-05-06 04:23:50 | INFO | train_inner | epoch 003:  17970 / 60421 loss=0.974, ppl=1.96, wps=16356.1, ups=4.42, wpb=3702.5, bsz=116.2, num_updates=138800, lr=8.488e-05, gnorm=1.497, loss_scale=4, train_wall=22, gb_free=11.2, wall=42404
2021-05-06 04:24:13 | INFO | train_inner | epoch 003:  18070 / 60421 loss=0.928, ppl=1.9, wps=16236.4, ups=4.36, wpb=3725.2, bsz=144.1, num_updates=138900, lr=8.48494e-05, gnorm=1.102, loss_scale=4, train_wall=23, gb_free=10.8, wall=42427
2021-05-06 04:24:36 | INFO | train_inner | epoch 003:  18170 / 60421 loss=0.951, ppl=1.93, wps=16282.1, ups=4.35, wpb=3740.8, bsz=132.3, num_updates=139000, lr=8.48189e-05, gnorm=1.185, loss_scale=4, train_wall=23, gb_free=10.8, wall=42450
2021-05-06 04:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:24:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:24:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:24:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:24:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:24:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:24:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:24:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:24:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:24:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:24:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:24:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:24:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:24:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:24:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:24:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:25:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.053 | ppl 33.19 | bleu 29.3 | wps 2427.7 | wpb 2024.1 | bsz 97.5 | num_updates 139000 | best_bleu 29.47
2021-05-06 04:25:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 139000 updates
2021-05-06 04:25:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_139000.pt
2021-05-06 04:25:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_139000.pt
2021-05-06 04:25:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_139000.pt (epoch 3 @ 139000 updates, score 29.3) (writing took 7.943006007000804 seconds)
2021-05-06 04:26:13 | INFO | train_inner | epoch 003:  18270 / 60421 loss=0.95, ppl=1.93, wps=3853.9, ups=1.04, wpb=3720.4, bsz=112.9, num_updates=139100, lr=8.47884e-05, gnorm=1.284, loss_scale=4, train_wall=23, gb_free=10.9, wall=42546
2021-05-06 04:26:36 | INFO | train_inner | epoch 003:  18370 / 60421 loss=0.96, ppl=1.95, wps=16447.5, ups=4.31, wpb=3817.9, bsz=137.4, num_updates=139200, lr=8.47579e-05, gnorm=1.213, loss_scale=4, train_wall=23, gb_free=10.8, wall=42570
2021-05-06 04:26:59 | INFO | train_inner | epoch 003:  18470 / 60421 loss=0.954, ppl=1.94, wps=16479.7, ups=4.37, wpb=3768.8, bsz=125.4, num_updates=139300, lr=8.47275e-05, gnorm=1.064, loss_scale=4, train_wall=23, gb_free=10.7, wall=42592
2021-05-06 04:27:21 | INFO | train_inner | epoch 003:  18570 / 60421 loss=0.883, ppl=1.84, wps=16377, ups=4.42, wpb=3701, bsz=150.8, num_updates=139400, lr=8.46971e-05, gnorm=1.107, loss_scale=4, train_wall=22, gb_free=10.8, wall=42615
2021-05-06 04:27:44 | INFO | train_inner | epoch 003:  18670 / 60421 loss=0.953, ppl=1.94, wps=16537.6, ups=4.44, wpb=3726.4, bsz=131.3, num_updates=139500, lr=8.46668e-05, gnorm=1.088, loss_scale=4, train_wall=22, gb_free=10.9, wall=42638
2021-05-06 04:28:06 | INFO | train_inner | epoch 003:  18770 / 60421 loss=0.933, ppl=1.91, wps=16046.8, ups=4.44, wpb=3616.1, bsz=128.9, num_updates=139600, lr=8.46364e-05, gnorm=1.318, loss_scale=4, train_wall=22, gb_free=10.9, wall=42660
2021-05-06 04:28:29 | INFO | train_inner | epoch 003:  18870 / 60421 loss=0.925, ppl=1.9, wps=16469.9, ups=4.39, wpb=3750.5, bsz=144.6, num_updates=139700, lr=8.46061e-05, gnorm=1.131, loss_scale=4, train_wall=23, gb_free=10.7, wall=42683
2021-05-06 04:28:52 | INFO | train_inner | epoch 003:  18970 / 60421 loss=0.911, ppl=1.88, wps=16346.5, ups=4.33, wpb=3779, bsz=136.6, num_updates=139800, lr=8.45759e-05, gnorm=0.936, loss_scale=4, train_wall=23, gb_free=10.7, wall=42706
2021-05-06 04:29:15 | INFO | train_inner | epoch 003:  19070 / 60421 loss=0.965, ppl=1.95, wps=16360.4, ups=4.4, wpb=3717.9, bsz=110.2, num_updates=139900, lr=8.45456e-05, gnorm=1.105, loss_scale=4, train_wall=23, gb_free=10.8, wall=42729
2021-05-06 04:29:38 | INFO | train_inner | epoch 003:  19170 / 60421 loss=0.948, ppl=1.93, wps=16411.7, ups=4.34, wpb=3780.7, bsz=129.9, num_updates=140000, lr=8.45154e-05, gnorm=1.19, loss_scale=4, train_wall=23, gb_free=11, wall=42752
2021-05-06 04:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:29:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:29:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:29:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:29:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:29:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:29:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:29:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:29:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:29:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:30:43 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.088 | ppl 34.02 | bleu 29.17 | wps 2450.3 | wpb 2024.1 | bsz 97.5 | num_updates 140000 | best_bleu 29.47
2021-05-06 04:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 140000 updates
2021-05-06 04:30:43 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_140000.pt
2021-05-06 04:30:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_140000.pt
2021-05-06 04:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_140000.pt (epoch 3 @ 140000 updates, score 29.17) (writing took 10.265428501996212 seconds)
2021-05-06 04:31:16 | INFO | train_inner | epoch 003:  19270 / 60421 loss=0.929, ppl=1.9, wps=3693, ups=1.02, wpb=3615.4, bsz=126.8, num_updates=140100, lr=8.44853e-05, gnorm=1.331, loss_scale=4, train_wall=23, gb_free=11.3, wall=42850
2021-05-06 04:31:39 | INFO | train_inner | epoch 003:  19370 / 60421 loss=0.925, ppl=1.9, wps=16483.9, ups=4.38, wpb=3761.5, bsz=133.9, num_updates=140200, lr=8.44551e-05, gnorm=1.197, loss_scale=4, train_wall=23, gb_free=10.7, wall=42873
2021-05-06 04:32:02 | INFO | train_inner | epoch 003:  19470 / 60421 loss=0.949, ppl=1.93, wps=16452, ups=4.36, wpb=3770.3, bsz=133, num_updates=140300, lr=8.4425e-05, gnorm=1.131, loss_scale=4, train_wall=23, gb_free=10.8, wall=42895
2021-05-06 04:32:24 | INFO | train_inner | epoch 003:  19570 / 60421 loss=0.918, ppl=1.89, wps=16512.8, ups=4.43, wpb=3727.6, bsz=126.5, num_updates=140400, lr=8.43949e-05, gnorm=1.088, loss_scale=4, train_wall=22, gb_free=11.1, wall=42918
2021-05-06 04:32:47 | INFO | train_inner | epoch 003:  19670 / 60421 loss=0.97, ppl=1.96, wps=16364, ups=4.41, wpb=3707.2, bsz=132.7, num_updates=140500, lr=8.43649e-05, gnorm=1.275, loss_scale=4, train_wall=22, gb_free=10.9, wall=42941
2021-05-06 04:33:10 | INFO | train_inner | epoch 003:  19770 / 60421 loss=0.926, ppl=1.9, wps=16529.7, ups=4.39, wpb=3764.7, bsz=139.6, num_updates=140600, lr=8.43349e-05, gnorm=1.09, loss_scale=4, train_wall=23, gb_free=10.7, wall=42963
2021-05-06 04:33:32 | INFO | train_inner | epoch 003:  19870 / 60421 loss=0.94, ppl=1.92, wps=16486.7, ups=4.39, wpb=3754.2, bsz=137.4, num_updates=140700, lr=8.43049e-05, gnorm=1.051, loss_scale=4, train_wall=23, gb_free=10.7, wall=42986
2021-05-06 04:33:55 | INFO | train_inner | epoch 003:  19970 / 60421 loss=0.919, ppl=1.89, wps=16169.9, ups=4.42, wpb=3657.8, bsz=126.3, num_updates=140800, lr=8.4275e-05, gnorm=1.49, loss_scale=4, train_wall=22, gb_free=10.8, wall=43009
2021-05-06 04:34:18 | INFO | train_inner | epoch 003:  20070 / 60421 loss=0.939, ppl=1.92, wps=16264.2, ups=4.37, wpb=3723.3, bsz=139.6, num_updates=140900, lr=8.42451e-05, gnorm=1.25, loss_scale=4, train_wall=23, gb_free=10.9, wall=43032
2021-05-06 04:34:41 | INFO | train_inner | epoch 003:  20170 / 60421 loss=0.897, ppl=1.86, wps=15966.7, ups=4.34, wpb=3675.4, bsz=136.6, num_updates=141000, lr=8.42152e-05, gnorm=1.209, loss_scale=4, train_wall=23, gb_free=11, wall=43055
2021-05-06 04:34:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:34:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:34:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:34:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:34:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:34:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:34:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:35:47 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.065 | ppl 33.48 | bleu 29.24 | wps 2388.4 | wpb 2024.1 | bsz 97.5 | num_updates 141000 | best_bleu 29.47
2021-05-06 04:35:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 141000 updates
2021-05-06 04:35:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_141000.pt
2021-05-06 04:35:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_141000.pt
2021-05-06 04:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_141000.pt (epoch 3 @ 141000 updates, score 29.24) (writing took 7.87173410601099 seconds)
2021-05-06 04:36:18 | INFO | train_inner | epoch 003:  20270 / 60421 loss=0.943, ppl=1.92, wps=3840.4, ups=1.02, wpb=3747.6, bsz=129, num_updates=141100, lr=8.41853e-05, gnorm=1.055, loss_scale=4, train_wall=23, gb_free=10.7, wall=43152
2021-05-06 04:36:41 | INFO | train_inner | epoch 003:  20370 / 60421 loss=0.96, ppl=1.94, wps=16492.4, ups=4.37, wpb=3774.9, bsz=127, num_updates=141200, lr=8.41555e-05, gnorm=1.081, loss_scale=4, train_wall=23, gb_free=10.7, wall=43175
2021-05-06 04:37:04 | INFO | train_inner | epoch 003:  20470 / 60421 loss=0.906, ppl=1.87, wps=16306.4, ups=4.41, wpb=3700.9, bsz=139.8, num_updates=141300, lr=8.41257e-05, gnorm=1.183, loss_scale=4, train_wall=23, gb_free=11, wall=43198
2021-05-06 04:37:27 | INFO | train_inner | epoch 003:  20570 / 60421 loss=0.885, ppl=1.85, wps=16262.6, ups=4.38, wpb=3712.8, bsz=155.4, num_updates=141400, lr=8.4096e-05, gnorm=1.205, loss_scale=4, train_wall=23, gb_free=10.9, wall=43221
2021-05-06 04:37:50 | INFO | train_inner | epoch 003:  20670 / 60421 loss=0.937, ppl=1.91, wps=16408, ups=4.41, wpb=3716.9, bsz=132.6, num_updates=141500, lr=8.40663e-05, gnorm=1.127, loss_scale=4, train_wall=22, gb_free=10.7, wall=43243
2021-05-06 04:38:12 | INFO | train_inner | epoch 003:  20770 / 60421 loss=0.968, ppl=1.96, wps=16534, ups=4.38, wpb=3777.1, bsz=135.4, num_updates=141600, lr=8.40366e-05, gnorm=1.219, loss_scale=4, train_wall=23, gb_free=10.8, wall=43266
2021-05-06 04:38:36 | INFO | train_inner | epoch 003:  20870 / 60421 loss=0.988, ppl=1.98, wps=16409.6, ups=4.33, wpb=3791.7, bsz=121.1, num_updates=141700, lr=8.40069e-05, gnorm=1.047, loss_scale=4, train_wall=23, gb_free=10.7, wall=43289
2021-05-06 04:38:58 | INFO | train_inner | epoch 003:  20970 / 60421 loss=0.91, ppl=1.88, wps=16333.7, ups=4.41, wpb=3699.9, bsz=134.6, num_updates=141800, lr=8.39773e-05, gnorm=1.075, loss_scale=4, train_wall=22, gb_free=10.7, wall=43312
2021-05-06 04:39:21 | INFO | train_inner | epoch 003:  21070 / 60421 loss=0.965, ppl=1.95, wps=16382, ups=4.34, wpb=3774.8, bsz=123.8, num_updates=141900, lr=8.39477e-05, gnorm=1.064, loss_scale=4, train_wall=23, gb_free=10.8, wall=43335
2021-05-06 04:39:44 | INFO | train_inner | epoch 003:  21170 / 60421 loss=0.968, ppl=1.96, wps=16281.3, ups=4.36, wpb=3732.8, bsz=134.6, num_updates=142000, lr=8.39181e-05, gnorm=1.111, loss_scale=4, train_wall=23, gb_free=10.8, wall=43358
2021-05-06 04:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:40:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.102 | ppl 34.33 | bleu 29.45 | wps 2402.4 | wpb 2024.1 | bsz 97.5 | num_updates 142000 | best_bleu 29.47
2021-05-06 04:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 142000 updates
2021-05-06 04:40:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_142000.pt
2021-05-06 04:40:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_142000.pt
2021-05-06 04:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_142000.pt (epoch 3 @ 142000 updates, score 29.45) (writing took 8.448855961993104 seconds)
2021-05-06 04:41:22 | INFO | train_inner | epoch 003:  21270 / 60421 loss=0.916, ppl=1.89, wps=3860.9, ups=1.02, wpb=3774.8, bsz=156.8, num_updates=142100, lr=8.38886e-05, gnorm=1.041, loss_scale=4, train_wall=23, gb_free=10.8, wall=43456
2021-05-06 04:41:45 | INFO | train_inner | epoch 003:  21370 / 60421 loss=0.917, ppl=1.89, wps=16559.7, ups=4.38, wpb=3778.1, bsz=128.4, num_updates=142200, lr=8.38591e-05, gnorm=1.026, loss_scale=4, train_wall=23, gb_free=10.9, wall=43479
2021-05-06 04:42:07 | INFO | train_inner | epoch 003:  21470 / 60421 loss=0.973, ppl=1.96, wps=16473.3, ups=4.44, wpb=3706.5, bsz=119.3, num_updates=142300, lr=8.38296e-05, gnorm=1.198, loss_scale=4, train_wall=22, gb_free=11, wall=43501
2021-05-06 04:42:30 | INFO | train_inner | epoch 003:  21570 / 60421 loss=0.949, ppl=1.93, wps=16463.7, ups=4.43, wpb=3712.8, bsz=118.6, num_updates=142400, lr=8.38002e-05, gnorm=1.106, loss_scale=4, train_wall=22, gb_free=10.9, wall=43524
2021-05-06 04:42:53 | INFO | train_inner | epoch 003:  21670 / 60421 loss=0.925, ppl=1.9, wps=16553.1, ups=4.39, wpb=3766.4, bsz=127.9, num_updates=142500, lr=8.37708e-05, gnorm=0.975, loss_scale=4, train_wall=23, gb_free=10.9, wall=43546
2021-05-06 04:43:15 | INFO | train_inner | epoch 003:  21770 / 60421 loss=0.931, ppl=1.91, wps=16499.9, ups=4.41, wpb=3743.1, bsz=139.2, num_updates=142600, lr=8.37414e-05, gnorm=1.139, loss_scale=4, train_wall=22, gb_free=11.1, wall=43569
2021-05-06 04:43:38 | INFO | train_inner | epoch 003:  21870 / 60421 loss=0.963, ppl=1.95, wps=16358.6, ups=4.4, wpb=3717.7, bsz=132.3, num_updates=142700, lr=8.37121e-05, gnorm=1.271, loss_scale=4, train_wall=23, gb_free=10.7, wall=43592
2021-05-06 04:44:01 | INFO | train_inner | epoch 003:  21970 / 60421 loss=0.929, ppl=1.9, wps=16330.5, ups=4.36, wpb=3744.4, bsz=135.3, num_updates=142800, lr=8.36827e-05, gnorm=1.139, loss_scale=4, train_wall=23, gb_free=11, wall=43615
2021-05-06 04:44:24 | INFO | train_inner | epoch 003:  22070 / 60421 loss=0.971, ppl=1.96, wps=16496.2, ups=4.33, wpb=3810, bsz=124.6, num_updates=142900, lr=8.36535e-05, gnorm=1.094, loss_scale=4, train_wall=23, gb_free=10.8, wall=43638
2021-05-06 04:44:47 | INFO | train_inner | epoch 003:  22170 / 60421 loss=0.923, ppl=1.9, wps=16218.1, ups=4.31, wpb=3767.1, bsz=151.4, num_updates=143000, lr=8.36242e-05, gnorm=1.018, loss_scale=4, train_wall=23, gb_free=10.8, wall=43661
2021-05-06 04:44:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:44:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:44:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:44:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:45:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:45:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:45:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:45:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:45:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:45:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:45:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:45:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:45:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:45:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:45:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:45:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:45:53 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.133 | ppl 35.1 | bleu 29.35 | wps 2404.6 | wpb 2024.1 | bsz 97.5 | num_updates 143000 | best_bleu 29.47
2021-05-06 04:45:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 143000 updates
2021-05-06 04:45:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_143000.pt
2021-05-06 04:45:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_143000.pt
2021-05-06 04:46:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_143000.pt (epoch 3 @ 143000 updates, score 29.35) (writing took 8.55681810202077 seconds)
2021-05-06 04:46:25 | INFO | train_inner | epoch 003:  22270 / 60421 loss=0.95, ppl=1.93, wps=3846.9, ups=1.02, wpb=3758.1, bsz=120.7, num_updates=143100, lr=8.3595e-05, gnorm=1.099, loss_scale=4, train_wall=23, gb_free=10.8, wall=43759
2021-05-06 04:46:48 | INFO | train_inner | epoch 003:  22370 / 60421 loss=0.968, ppl=1.96, wps=16437.8, ups=4.4, wpb=3735.2, bsz=119.8, num_updates=143200, lr=8.35658e-05, gnorm=1.022, loss_scale=4, train_wall=23, gb_free=10.4, wall=43781
2021-05-06 04:47:10 | INFO | train_inner | epoch 003:  22470 / 60421 loss=0.933, ppl=1.91, wps=16448, ups=4.46, wpb=3689.9, bsz=124.3, num_updates=143300, lr=8.35366e-05, gnorm=1.15, loss_scale=4, train_wall=22, gb_free=10.8, wall=43804
2021-05-06 04:47:33 | INFO | train_inner | epoch 003:  22570 / 60421 loss=0.914, ppl=1.88, wps=16412.8, ups=4.41, wpb=3721.4, bsz=133.4, num_updates=143400, lr=8.35075e-05, gnorm=1.021, loss_scale=4, train_wall=22, gb_free=10.7, wall=43827
2021-05-06 04:47:55 | INFO | train_inner | epoch 003:  22670 / 60421 loss=0.949, ppl=1.93, wps=16636.3, ups=4.4, wpb=3777.3, bsz=132.4, num_updates=143500, lr=8.34784e-05, gnorm=1.038, loss_scale=4, train_wall=23, gb_free=11, wall=43849
2021-05-06 04:48:18 | INFO | train_inner | epoch 003:  22770 / 60421 loss=0.946, ppl=1.93, wps=16331.5, ups=4.41, wpb=3699.6, bsz=125, num_updates=143600, lr=8.34493e-05, gnorm=1.195, loss_scale=4, train_wall=22, gb_free=10.7, wall=43872
2021-05-06 04:48:41 | INFO | train_inner | epoch 003:  22870 / 60421 loss=0.918, ppl=1.89, wps=16416.4, ups=4.42, wpb=3718, bsz=129.7, num_updates=143700, lr=8.34203e-05, gnorm=0.985, loss_scale=4, train_wall=22, gb_free=10.7, wall=43895
2021-05-06 04:49:03 | INFO | train_inner | epoch 003:  22970 / 60421 loss=1.008, ppl=2.01, wps=16279, ups=4.44, wpb=3665.9, bsz=108.4, num_updates=143800, lr=8.33913e-05, gnorm=1.279, loss_scale=4, train_wall=22, gb_free=11.2, wall=43917
2021-05-06 04:49:26 | INFO | train_inner | epoch 003:  23070 / 60421 loss=0.954, ppl=1.94, wps=16279.9, ups=4.31, wpb=3779.4, bsz=133.3, num_updates=143900, lr=8.33623e-05, gnorm=1.209, loss_scale=4, train_wall=23, gb_free=10.9, wall=43940
2021-05-06 04:49:49 | INFO | train_inner | epoch 003:  23170 / 60421 loss=0.907, ppl=1.88, wps=16011.5, ups=4.34, wpb=3686.1, bsz=135.8, num_updates=144000, lr=8.33333e-05, gnorm=1.347, loss_scale=4, train_wall=23, gb_free=10.8, wall=43963
2021-05-06 04:49:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:50:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:50:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:50:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:50:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:50:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:50:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:50:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:50:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:50:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:50:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:50:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:50:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:50:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:50:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:50:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:50:55 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.168 | ppl 35.96 | bleu 28.87 | wps 2414.2 | wpb 2024.1 | bsz 97.5 | num_updates 144000 | best_bleu 29.47
2021-05-06 04:50:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 144000 updates
2021-05-06 04:50:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_144000.pt
2021-05-06 04:50:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_144000.pt
2021-05-06 04:51:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_144000.pt (epoch 3 @ 144000 updates, score 28.87) (writing took 7.921535237983335 seconds)
2021-05-06 04:51:26 | INFO | train_inner | epoch 003:  23270 / 60421 loss=1.001, ppl=2, wps=3876.9, ups=1.03, wpb=3750, bsz=118.8, num_updates=144100, lr=8.33044e-05, gnorm=1.161, loss_scale=4, train_wall=23, gb_free=10.9, wall=44060
2021-05-06 04:51:49 | INFO | train_inner | epoch 003:  23370 / 60421 loss=0.927, ppl=1.9, wps=16590.6, ups=4.34, wpb=3822.8, bsz=162.9, num_updates=144200, lr=8.32755e-05, gnorm=1.063, loss_scale=4, train_wall=23, gb_free=10.8, wall=44083
2021-05-06 04:52:12 | INFO | train_inner | epoch 003:  23470 / 60421 loss=0.939, ppl=1.92, wps=16445.1, ups=4.44, wpb=3707.9, bsz=128.8, num_updates=144300, lr=8.32467e-05, gnorm=1.185, loss_scale=4, train_wall=22, gb_free=11.1, wall=44106
2021-05-06 04:52:35 | INFO | train_inner | epoch 003:  23570 / 60421 loss=0.961, ppl=1.95, wps=16615, ups=4.38, wpb=3794.7, bsz=144, num_updates=144400, lr=8.32178e-05, gnorm=1.111, loss_scale=4, train_wall=23, gb_free=10.9, wall=44129
2021-05-06 04:52:57 | INFO | train_inner | epoch 003:  23670 / 60421 loss=0.934, ppl=1.91, wps=16497.5, ups=4.44, wpb=3714.8, bsz=137.4, num_updates=144500, lr=8.3189e-05, gnorm=1.216, loss_scale=4, train_wall=22, gb_free=10.6, wall=44151
2021-05-06 04:53:20 | INFO | train_inner | epoch 003:  23770 / 60421 loss=0.988, ppl=1.98, wps=16629.8, ups=4.36, wpb=3813.5, bsz=131.3, num_updates=144600, lr=8.31603e-05, gnorm=1.241, loss_scale=4, train_wall=23, gb_free=10.9, wall=44174
2021-05-06 04:53:43 | INFO | train_inner | epoch 003:  23870 / 60421 loss=0.979, ppl=1.97, wps=16463, ups=4.45, wpb=3702.6, bsz=118.6, num_updates=144700, lr=8.31315e-05, gnorm=1.184, loss_scale=4, train_wall=22, gb_free=10.8, wall=44196
2021-05-06 04:54:05 | INFO | train_inner | epoch 003:  23970 / 60421 loss=0.953, ppl=1.94, wps=16357.7, ups=4.42, wpb=3704.6, bsz=125.8, num_updates=144800, lr=8.31028e-05, gnorm=1.252, loss_scale=4, train_wall=22, gb_free=10.7, wall=44219
2021-05-06 04:54:28 | INFO | train_inner | epoch 003:  24070 / 60421 loss=0.995, ppl=1.99, wps=16249, ups=4.33, wpb=3751.1, bsz=138.6, num_updates=144900, lr=8.30741e-05, gnorm=1.319, loss_scale=4, train_wall=23, gb_free=11.1, wall=44242
2021-05-06 04:54:51 | INFO | train_inner | epoch 003:  24170 / 60421 loss=0.955, ppl=1.94, wps=16172.4, ups=4.32, wpb=3747.5, bsz=135.1, num_updates=145000, lr=8.30455e-05, gnorm=1.178, loss_scale=4, train_wall=23, gb_free=10.8, wall=44265
2021-05-06 04:54:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 04:55:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:55:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:55:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:55:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:55:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:55:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:55:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 04:55:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 04:55:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 04:55:58 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.156 | ppl 35.66 | bleu 28.93 | wps 2392.8 | wpb 2024.1 | bsz 97.5 | num_updates 145000 | best_bleu 29.47
2021-05-06 04:55:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 145000 updates
2021-05-06 04:55:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_145000.pt
2021-05-06 04:56:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_145000.pt
2021-05-06 04:56:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_145000.pt (epoch 3 @ 145000 updates, score 28.93) (writing took 7.715171800984535 seconds)
2021-05-06 04:56:29 | INFO | train_inner | epoch 003:  24270 / 60421 loss=0.929, ppl=1.9, wps=3883.9, ups=1.03, wpb=3776.8, bsz=169, num_updates=145100, lr=8.30169e-05, gnorm=1.091, loss_scale=4, train_wall=23, gb_free=10.8, wall=44363
2021-05-06 04:56:51 | INFO | train_inner | epoch 003:  24370 / 60421 loss=0.931, ppl=1.91, wps=16350.2, ups=4.4, wpb=3715, bsz=124.2, num_updates=145200, lr=8.29883e-05, gnorm=1.097, loss_scale=4, train_wall=23, gb_free=10.9, wall=44385
2021-05-06 04:57:14 | INFO | train_inner | epoch 003:  24470 / 60421 loss=0.926, ppl=1.9, wps=16366, ups=4.42, wpb=3699, bsz=127, num_updates=145300, lr=8.29597e-05, gnorm=1.081, loss_scale=4, train_wall=22, gb_free=10.9, wall=44408
2021-05-06 04:57:37 | INFO | train_inner | epoch 003:  24570 / 60421 loss=0.918, ppl=1.89, wps=16430.8, ups=4.4, wpb=3730.4, bsz=147.9, num_updates=145400, lr=8.29312e-05, gnorm=1.112, loss_scale=4, train_wall=23, gb_free=10.7, wall=44431
2021-05-06 04:57:59 | INFO | train_inner | epoch 003:  24670 / 60421 loss=0.958, ppl=1.94, wps=16304.9, ups=4.43, wpb=3678.1, bsz=126, num_updates=145500, lr=8.29027e-05, gnorm=1.194, loss_scale=8, train_wall=22, gb_free=10.9, wall=44453
2021-05-06 04:58:22 | INFO | train_inner | epoch 003:  24770 / 60421 loss=0.962, ppl=1.95, wps=16254.5, ups=4.39, wpb=3698.6, bsz=136.4, num_updates=145600, lr=8.28742e-05, gnorm=1.405, loss_scale=8, train_wall=23, gb_free=10.9, wall=44476
2021-05-06 04:58:45 | INFO | train_inner | epoch 003:  24870 / 60421 loss=0.932, ppl=1.91, wps=16396, ups=4.37, wpb=3749, bsz=144.2, num_updates=145700, lr=8.28457e-05, gnorm=1.146, loss_scale=8, train_wall=23, gb_free=10.8, wall=44499
2021-05-06 04:59:08 | INFO | train_inner | epoch 003:  24970 / 60421 loss=0.929, ppl=1.9, wps=16246.7, ups=4.37, wpb=3717.8, bsz=118.4, num_updates=145800, lr=8.28173e-05, gnorm=1.113, loss_scale=8, train_wall=23, gb_free=11.1, wall=44522
2021-05-06 04:59:31 | INFO | train_inner | epoch 003:  25070 / 60421 loss=0.962, ppl=1.95, wps=16300.5, ups=4.32, wpb=3769.7, bsz=125, num_updates=145900, lr=8.27889e-05, gnorm=1.091, loss_scale=8, train_wall=23, gb_free=10.8, wall=44545
2021-05-06 04:59:54 | INFO | train_inner | epoch 003:  25170 / 60421 loss=0.958, ppl=1.94, wps=16137, ups=4.29, wpb=3761.4, bsz=130.2, num_updates=146000, lr=8.27606e-05, gnorm=1.084, loss_scale=8, train_wall=23, gb_free=10.9, wall=44568
2021-05-06 04:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:00:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:00:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:00:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:00:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:00:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:00:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:00:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:00:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:00:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:00:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:00:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:00:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:00:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:00:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:00:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:00:59 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.111 | ppl 34.55 | bleu 28.98 | wps 2435.7 | wpb 2024.1 | bsz 97.5 | num_updates 146000 | best_bleu 29.47
2021-05-06 05:00:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 146000 updates
2021-05-06 05:00:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_146000.pt
2021-05-06 05:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_146000.pt
2021-05-06 05:01:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_146000.pt (epoch 3 @ 146000 updates, score 28.98) (writing took 7.969225095002912 seconds)
2021-05-06 05:01:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-06 05:01:31 | INFO | train_inner | epoch 003:  25271 / 60421 loss=0.913, ppl=1.88, wps=3898.8, ups=1.04, wpb=3758.5, bsz=139.8, num_updates=146100, lr=8.27323e-05, gnorm=1.341, loss_scale=4, train_wall=23, gb_free=10.9, wall=44665
2021-05-06 05:01:53 | INFO | train_inner | epoch 003:  25371 / 60421 loss=0.959, ppl=1.94, wps=16423.1, ups=4.41, wpb=3721.2, bsz=129.8, num_updates=146200, lr=8.2704e-05, gnorm=1.226, loss_scale=4, train_wall=22, gb_free=10.7, wall=44687
2021-05-06 05:02:16 | INFO | train_inner | epoch 003:  25471 / 60421 loss=0.901, ppl=1.87, wps=16306.9, ups=4.45, wpb=3666, bsz=129.5, num_updates=146300, lr=8.26757e-05, gnorm=1.19, loss_scale=4, train_wall=22, gb_free=10.9, wall=44710
2021-05-06 05:02:38 | INFO | train_inner | epoch 003:  25571 / 60421 loss=0.94, ppl=1.92, wps=16361.5, ups=4.44, wpb=3687.8, bsz=123.1, num_updates=146400, lr=8.26475e-05, gnorm=1.194, loss_scale=4, train_wall=22, gb_free=10.8, wall=44732
2021-05-06 05:03:01 | INFO | train_inner | epoch 003:  25671 / 60421 loss=0.943, ppl=1.92, wps=16509.3, ups=4.41, wpb=3742.7, bsz=123.5, num_updates=146500, lr=8.26192e-05, gnorm=1.049, loss_scale=4, train_wall=22, gb_free=11, wall=44755
2021-05-06 05:03:24 | INFO | train_inner | epoch 003:  25771 / 60421 loss=0.956, ppl=1.94, wps=16437.4, ups=4.41, wpb=3726.3, bsz=114.4, num_updates=146600, lr=8.25911e-05, gnorm=1.11, loss_scale=4, train_wall=22, gb_free=10.8, wall=44778
2021-05-06 05:03:47 | INFO | train_inner | epoch 003:  25871 / 60421 loss=0.936, ppl=1.91, wps=16375.6, ups=4.36, wpb=3756.4, bsz=133, num_updates=146700, lr=8.25629e-05, gnorm=1.063, loss_scale=4, train_wall=23, gb_free=10.8, wall=44801
2021-05-06 05:04:10 | INFO | train_inner | epoch 003:  25971 / 60421 loss=0.879, ppl=1.84, wps=16379, ups=4.33, wpb=3781.8, bsz=159.4, num_updates=146800, lr=8.25348e-05, gnorm=1.006, loss_scale=4, train_wall=23, gb_free=10.8, wall=44824
2021-05-06 05:04:33 | INFO | train_inner | epoch 003:  26071 / 60421 loss=0.953, ppl=1.94, wps=16112, ups=4.38, wpb=3677.9, bsz=115.3, num_updates=146900, lr=8.25067e-05, gnorm=1.284, loss_scale=4, train_wall=23, gb_free=10.7, wall=44846
2021-05-06 05:04:56 | INFO | train_inner | epoch 003:  26171 / 60421 loss=0.937, ppl=1.91, wps=15904.3, ups=4.31, wpb=3688.8, bsz=131.1, num_updates=147000, lr=8.24786e-05, gnorm=1.161, loss_scale=4, train_wall=23, gb_free=10.8, wall=44870
2021-05-06 05:04:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:05:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:05:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:05:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:05:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:05:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:05:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:05:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:05:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:05:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:05:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:05:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:05:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:06:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.143 | ppl 35.33 | bleu 29.29 | wps 2405.8 | wpb 2024.1 | bsz 97.5 | num_updates 147000 | best_bleu 29.47
2021-05-06 05:06:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 147000 updates
2021-05-06 05:06:02 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_147000.pt
2021-05-06 05:06:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_147000.pt
2021-05-06 05:06:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_147000.pt (epoch 3 @ 147000 updates, score 29.29) (writing took 7.976764880004339 seconds)
2021-05-06 05:06:33 | INFO | train_inner | epoch 003:  26271 / 60421 loss=0.981, ppl=1.97, wps=3868, ups=1.03, wpb=3748.7, bsz=115.6, num_updates=147100, lr=8.24506e-05, gnorm=1.059, loss_scale=4, train_wall=23, gb_free=11, wall=44967
2021-05-06 05:06:55 | INFO | train_inner | epoch 003:  26371 / 60421 loss=0.926, ppl=1.9, wps=16458, ups=4.39, wpb=3746.8, bsz=150.6, num_updates=147200, lr=8.24226e-05, gnorm=1.218, loss_scale=4, train_wall=23, gb_free=10.8, wall=44989
2021-05-06 05:07:18 | INFO | train_inner | epoch 003:  26471 / 60421 loss=0.913, ppl=1.88, wps=16179.1, ups=4.43, wpb=3654.5, bsz=135.8, num_updates=147300, lr=8.23946e-05, gnorm=1.233, loss_scale=4, train_wall=22, gb_free=10.9, wall=45012
2021-05-06 05:07:41 | INFO | train_inner | epoch 003:  26571 / 60421 loss=0.839, ppl=1.79, wps=16364, ups=4.39, wpb=3731.1, bsz=164.1, num_updates=147400, lr=8.23666e-05, gnorm=0.997, loss_scale=4, train_wall=23, gb_free=11.5, wall=45035
2021-05-06 05:08:04 | INFO | train_inner | epoch 003:  26671 / 60421 loss=0.961, ppl=1.95, wps=16717.7, ups=4.39, wpb=3806, bsz=131.2, num_updates=147500, lr=8.23387e-05, gnorm=0.982, loss_scale=4, train_wall=23, gb_free=10.8, wall=45057
2021-05-06 05:08:26 | INFO | train_inner | epoch 003:  26771 / 60421 loss=0.929, ppl=1.9, wps=16379.2, ups=4.44, wpb=3690.3, bsz=134, num_updates=147600, lr=8.23108e-05, gnorm=1.283, loss_scale=4, train_wall=22, gb_free=10.7, wall=45080
2021-05-06 05:08:49 | INFO | train_inner | epoch 003:  26871 / 60421 loss=0.973, ppl=1.96, wps=16458, ups=4.36, wpb=3776.2, bsz=132.2, num_updates=147700, lr=8.22829e-05, gnorm=1.168, loss_scale=4, train_wall=23, gb_free=10.8, wall=45103
2021-05-06 05:09:12 | INFO | train_inner | epoch 003:  26971 / 60421 loss=0.963, ppl=1.95, wps=16422.7, ups=4.33, wpb=3793.2, bsz=122.6, num_updates=147800, lr=8.22551e-05, gnorm=0.997, loss_scale=4, train_wall=23, gb_free=10.9, wall=45126
2021-05-06 05:09:36 | INFO | train_inner | epoch 003:  27071 / 60421 loss=0.961, ppl=1.95, wps=16432.1, ups=4.27, wpb=3846.1, bsz=136.8, num_updates=147900, lr=8.22273e-05, gnorm=1.138, loss_scale=4, train_wall=23, gb_free=10.7, wall=45149
2021-05-06 05:09:59 | INFO | train_inner | epoch 003:  27171 / 60421 loss=0.967, ppl=1.95, wps=16021.7, ups=4.3, wpb=3728.3, bsz=132.9, num_updates=148000, lr=8.21995e-05, gnorm=1.314, loss_scale=4, train_wall=23, gb_free=10.9, wall=45173
2021-05-06 05:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:10:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:10:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:10:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:10:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:10:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:10:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:10:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:10:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:10:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:11:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.138 | ppl 35.2 | bleu 29.21 | wps 2423.7 | wpb 2024.1 | bsz 97.5 | num_updates 148000 | best_bleu 29.47
2021-05-06 05:11:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 148000 updates
2021-05-06 05:11:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_148000.pt
2021-05-06 05:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_148000.pt
2021-05-06 05:11:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_148000.pt (epoch 3 @ 148000 updates, score 29.21) (writing took 7.979748037992977 seconds)
2021-05-06 05:11:35 | INFO | train_inner | epoch 003:  27271 / 60421 loss=0.939, ppl=1.92, wps=3833.4, ups=1.04, wpb=3684.7, bsz=117.1, num_updates=148100, lr=8.21717e-05, gnorm=1.174, loss_scale=4, train_wall=22, gb_free=10.7, wall=45269
2021-05-06 05:11:58 | INFO | train_inner | epoch 003:  27371 / 60421 loss=0.881, ppl=1.84, wps=16353.5, ups=4.41, wpb=3704.3, bsz=132.2, num_updates=148200, lr=8.2144e-05, gnorm=1.139, loss_scale=4, train_wall=22, gb_free=10.8, wall=45291
2021-05-06 05:12:20 | INFO | train_inner | epoch 003:  27471 / 60421 loss=0.928, ppl=1.9, wps=16383.9, ups=4.4, wpb=3721.3, bsz=135, num_updates=148300, lr=8.21163e-05, gnorm=1.288, loss_scale=4, train_wall=23, gb_free=10.9, wall=45314
2021-05-06 05:12:43 | INFO | train_inner | epoch 003:  27571 / 60421 loss=0.923, ppl=1.9, wps=16295.1, ups=4.4, wpb=3706.3, bsz=142.9, num_updates=148400, lr=8.20886e-05, gnorm=1.187, loss_scale=4, train_wall=23, gb_free=10.6, wall=45337
2021-05-06 05:13:06 | INFO | train_inner | epoch 003:  27671 / 60421 loss=0.934, ppl=1.91, wps=16477, ups=4.42, wpb=3728.8, bsz=140.6, num_updates=148500, lr=8.2061e-05, gnorm=1.171, loss_scale=4, train_wall=22, gb_free=11.3, wall=45360
2021-05-06 05:13:28 | INFO | train_inner | epoch 003:  27771 / 60421 loss=0.935, ppl=1.91, wps=16394.4, ups=4.4, wpb=3724.2, bsz=133, num_updates=148600, lr=8.20334e-05, gnorm=1.29, loss_scale=4, train_wall=23, gb_free=10.8, wall=45382
2021-05-06 05:13:51 | INFO | train_inner | epoch 003:  27871 / 60421 loss=0.955, ppl=1.94, wps=16261.3, ups=4.41, wpb=3686, bsz=127.7, num_updates=148700, lr=8.20058e-05, gnorm=1.202, loss_scale=4, train_wall=22, gb_free=10.8, wall=45405
2021-05-06 05:14:14 | INFO | train_inner | epoch 003:  27971 / 60421 loss=0.944, ppl=1.92, wps=16275.3, ups=4.35, wpb=3744.4, bsz=151.3, num_updates=148800, lr=8.19782e-05, gnorm=1.099, loss_scale=4, train_wall=23, gb_free=10.8, wall=45428
2021-05-06 05:14:37 | INFO | train_inner | epoch 003:  28071 / 60421 loss=0.879, ppl=1.84, wps=15991, ups=4.37, wpb=3656, bsz=151.7, num_updates=148900, lr=8.19507e-05, gnorm=1.367, loss_scale=4, train_wall=23, gb_free=11, wall=45451
2021-05-06 05:15:00 | INFO | train_inner | epoch 003:  28171 / 60421 loss=0.917, ppl=1.89, wps=16016.8, ups=4.3, wpb=3728.3, bsz=126.1, num_updates=149000, lr=8.19232e-05, gnorm=1.237, loss_scale=4, train_wall=23, gb_free=10.9, wall=45474
2021-05-06 05:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:15:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:15:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:15:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:15:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:15:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:15:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:15:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:15:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:15:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:16:05 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.133 | ppl 35.08 | bleu 29.08 | wps 2444.8 | wpb 2024.1 | bsz 97.5 | num_updates 149000 | best_bleu 29.47
2021-05-06 05:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 149000 updates
2021-05-06 05:16:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_149000.pt
2021-05-06 05:16:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_149000.pt
2021-05-06 05:16:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_149000.pt (epoch 3 @ 149000 updates, score 29.08) (writing took 7.948897053982364 seconds)
2021-05-06 05:16:36 | INFO | train_inner | epoch 003:  28271 / 60421 loss=0.894, ppl=1.86, wps=3921, ups=1.04, wpb=3759.9, bsz=134.7, num_updates=149100, lr=8.18957e-05, gnorm=1.01, loss_scale=4, train_wall=23, gb_free=10.8, wall=45570
2021-05-06 05:16:59 | INFO | train_inner | epoch 003:  28371 / 60421 loss=0.929, ppl=1.9, wps=16523, ups=4.42, wpb=3736.6, bsz=129.9, num_updates=149200, lr=8.18683e-05, gnorm=1.052, loss_scale=4, train_wall=22, gb_free=10.9, wall=45593
2021-05-06 05:17:22 | INFO | train_inner | epoch 003:  28471 / 60421 loss=0.963, ppl=1.95, wps=16524.7, ups=4.37, wpb=3780, bsz=134.9, num_updates=149300, lr=8.18408e-05, gnorm=1.175, loss_scale=4, train_wall=23, gb_free=10.8, wall=45616
2021-05-06 05:17:44 | INFO | train_inner | epoch 003:  28571 / 60421 loss=0.962, ppl=1.95, wps=16570.5, ups=4.41, wpb=3757.9, bsz=125, num_updates=149400, lr=8.18134e-05, gnorm=1.101, loss_scale=4, train_wall=22, gb_free=10.9, wall=45638
2021-05-06 05:18:07 | INFO | train_inner | epoch 003:  28671 / 60421 loss=0.964, ppl=1.95, wps=16746.4, ups=4.41, wpb=3795.2, bsz=142.2, num_updates=149500, lr=8.17861e-05, gnorm=1.116, loss_scale=4, train_wall=22, gb_free=10.9, wall=45661
2021-05-06 05:18:30 | INFO | train_inner | epoch 003:  28771 / 60421 loss=0.891, ppl=1.86, wps=16582.4, ups=4.42, wpb=3750, bsz=138, num_updates=149600, lr=8.17587e-05, gnorm=1.142, loss_scale=4, train_wall=22, gb_free=10.8, wall=45683
2021-05-06 05:18:52 | INFO | train_inner | epoch 003:  28871 / 60421 loss=0.93, ppl=1.91, wps=16601.9, ups=4.44, wpb=3743.3, bsz=133.7, num_updates=149700, lr=8.17314e-05, gnorm=1.192, loss_scale=4, train_wall=22, gb_free=10.9, wall=45706
2021-05-06 05:19:15 | INFO | train_inner | epoch 003:  28971 / 60421 loss=0.969, ppl=1.96, wps=16570.4, ups=4.42, wpb=3746.4, bsz=126.4, num_updates=149800, lr=8.17041e-05, gnorm=1.299, loss_scale=4, train_wall=22, gb_free=11, wall=45729
2021-05-06 05:19:37 | INFO | train_inner | epoch 003:  29071 / 60421 loss=0.907, ppl=1.88, wps=16574.6, ups=4.44, wpb=3729.5, bsz=133.8, num_updates=149900, lr=8.16769e-05, gnorm=1.053, loss_scale=4, train_wall=22, gb_free=10.8, wall=45751
2021-05-06 05:20:00 | INFO | train_inner | epoch 003:  29171 / 60421 loss=0.931, ppl=1.91, wps=16337.5, ups=4.49, wpb=3642.5, bsz=142.5, num_updates=150000, lr=8.16497e-05, gnorm=1.632, loss_scale=4, train_wall=22, gb_free=10.7, wall=45773
2021-05-06 05:20:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:20:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:20:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:20:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:20:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:20:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:20:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:20:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:20:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:20:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:20:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:20:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:20:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:20:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:20:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:20:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:21:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.15 | ppl 35.51 | bleu 29.02 | wps 2470.1 | wpb 2024.1 | bsz 97.5 | num_updates 150000 | best_bleu 29.47
2021-05-06 05:21:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 150000 updates
2021-05-06 05:21:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_150000.pt
2021-05-06 05:21:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_150000.pt
2021-05-06 05:21:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_150000.pt (epoch 3 @ 150000 updates, score 29.02) (writing took 7.711148794012843 seconds)
2021-05-06 05:21:34 | INFO | train_inner | epoch 003:  29271 / 60421 loss=0.985, ppl=1.98, wps=3909.7, ups=1.06, wpb=3687.8, bsz=114, num_updates=150100, lr=8.16225e-05, gnorm=1.371, loss_scale=4, train_wall=22, gb_free=11, wall=45868
2021-05-06 05:21:56 | INFO | train_inner | epoch 003:  29371 / 60421 loss=0.925, ppl=1.9, wps=16415.5, ups=4.48, wpb=3664.3, bsz=122.7, num_updates=150200, lr=8.15953e-05, gnorm=1.116, loss_scale=4, train_wall=22, gb_free=10.8, wall=45890
2021-05-06 05:22:19 | INFO | train_inner | epoch 003:  29471 / 60421 loss=0.951, ppl=1.93, wps=16556.9, ups=4.45, wpb=3723.1, bsz=121.7, num_updates=150300, lr=8.15681e-05, gnorm=1.054, loss_scale=4, train_wall=22, gb_free=10.7, wall=45913
2021-05-06 05:22:41 | INFO | train_inner | epoch 003:  29571 / 60421 loss=0.952, ppl=1.93, wps=16538.6, ups=4.44, wpb=3726.9, bsz=135.8, num_updates=150400, lr=8.1541e-05, gnorm=1.121, loss_scale=4, train_wall=22, gb_free=10.6, wall=45935
2021-05-06 05:23:04 | INFO | train_inner | epoch 003:  29671 / 60421 loss=0.957, ppl=1.94, wps=16428.2, ups=4.41, wpb=3727.5, bsz=124.2, num_updates=150500, lr=8.15139e-05, gnorm=1.244, loss_scale=4, train_wall=23, gb_free=11.1, wall=45958
2021-05-06 05:23:27 | INFO | train_inner | epoch 003:  29771 / 60421 loss=0.919, ppl=1.89, wps=16467, ups=4.39, wpb=3748.8, bsz=137.8, num_updates=150600, lr=8.14868e-05, gnorm=1.115, loss_scale=4, train_wall=23, gb_free=10.9, wall=45981
2021-05-06 05:23:50 | INFO | train_inner | epoch 003:  29871 / 60421 loss=0.95, ppl=1.93, wps=16524, ups=4.37, wpb=3784.7, bsz=132.4, num_updates=150700, lr=8.14598e-05, gnorm=1.095, loss_scale=4, train_wall=23, gb_free=10.7, wall=46003
2021-05-06 05:24:12 | INFO | train_inner | epoch 003:  29971 / 60421 loss=0.929, ppl=1.9, wps=16230.6, ups=4.4, wpb=3692.4, bsz=132.9, num_updates=150800, lr=8.14328e-05, gnorm=1.173, loss_scale=4, train_wall=23, gb_free=10.8, wall=46026
2021-05-06 05:24:35 | INFO | train_inner | epoch 003:  30071 / 60421 loss=0.95, ppl=1.93, wps=16427.9, ups=4.33, wpb=3793.5, bsz=161.5, num_updates=150900, lr=8.14058e-05, gnorm=1.176, loss_scale=4, train_wall=23, gb_free=11.8, wall=46049
2021-05-06 05:24:59 | INFO | train_inner | epoch 003:  30171 / 60421 loss=0.945, ppl=1.93, wps=16021, ups=4.3, wpb=3725.6, bsz=132, num_updates=151000, lr=8.13788e-05, gnorm=1.192, loss_scale=4, train_wall=23, gb_free=10.8, wall=46073
2021-05-06 05:24:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:25:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:25:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:25:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:25:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:25:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:25:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:25:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:25:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:25:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:25:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:25:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:25:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:25:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:25:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:25:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:26:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.134 | ppl 35.1 | bleu 29.24 | wps 2407 | wpb 2024.1 | bsz 97.5 | num_updates 151000 | best_bleu 29.47
2021-05-06 05:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 151000 updates
2021-05-06 05:26:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_151000.pt
2021-05-06 05:26:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_151000.pt
2021-05-06 05:26:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_151000.pt (epoch 3 @ 151000 updates, score 29.24) (writing took 8.582287960016401 seconds)
2021-05-06 05:26:36 | INFO | train_inner | epoch 003:  30271 / 60421 loss=0.968, ppl=1.96, wps=3872.4, ups=1.03, wpb=3776.5, bsz=127.7, num_updates=151100, lr=8.13519e-05, gnorm=1.342, loss_scale=4, train_wall=23, gb_free=10.8, wall=46170
2021-05-06 05:26:59 | INFO | train_inner | epoch 003:  30371 / 60421 loss=0.92, ppl=1.89, wps=16469.1, ups=4.47, wpb=3688.2, bsz=131.4, num_updates=151200, lr=8.1325e-05, gnorm=1.037, loss_scale=4, train_wall=22, gb_free=10.8, wall=46192
2021-05-06 05:27:21 | INFO | train_inner | epoch 003:  30471 / 60421 loss=0.903, ppl=1.87, wps=16537.1, ups=4.46, wpb=3709.7, bsz=138.1, num_updates=151300, lr=8.12981e-05, gnorm=1.096, loss_scale=4, train_wall=22, gb_free=10.8, wall=46215
2021-05-06 05:27:44 | INFO | train_inner | epoch 003:  30571 / 60421 loss=0.928, ppl=1.9, wps=16502.9, ups=4.41, wpb=3740.9, bsz=143, num_updates=151400, lr=8.12713e-05, gnorm=1.045, loss_scale=4, train_wall=22, gb_free=10.9, wall=46238
2021-05-06 05:28:06 | INFO | train_inner | epoch 003:  30671 / 60421 loss=0.943, ppl=1.92, wps=16440.1, ups=4.42, wpb=3721.7, bsz=132.3, num_updates=151500, lr=8.12444e-05, gnorm=1.217, loss_scale=4, train_wall=22, gb_free=10.9, wall=46260
2021-05-06 05:28:29 | INFO | train_inner | epoch 003:  30771 / 60421 loss=0.988, ppl=1.98, wps=16402, ups=4.4, wpb=3730.1, bsz=123.4, num_updates=151600, lr=8.12176e-05, gnorm=1.289, loss_scale=4, train_wall=23, gb_free=10.8, wall=46283
2021-05-06 05:28:52 | INFO | train_inner | epoch 003:  30871 / 60421 loss=0.904, ppl=1.87, wps=16212.1, ups=4.41, wpb=3672.7, bsz=124.1, num_updates=151700, lr=8.11909e-05, gnorm=1.301, loss_scale=4, train_wall=22, gb_free=11, wall=46306
2021-05-06 05:29:14 | INFO | train_inner | epoch 003:  30971 / 60421 loss=0.967, ppl=1.96, wps=16279.5, ups=4.4, wpb=3702.3, bsz=132.6, num_updates=151800, lr=8.11641e-05, gnorm=1.297, loss_scale=4, train_wall=23, gb_free=10.8, wall=46328
2021-05-06 05:29:37 | INFO | train_inner | epoch 003:  31071 / 60421 loss=0.989, ppl=1.99, wps=16233.2, ups=4.36, wpb=3721.4, bsz=117, num_updates=151900, lr=8.11374e-05, gnorm=1.192, loss_scale=4, train_wall=23, gb_free=10.7, wall=46351
2021-05-06 05:30:01 | INFO | train_inner | epoch 003:  31171 / 60421 loss=0.936, ppl=1.91, wps=16164.6, ups=4.3, wpb=3762.2, bsz=151.9, num_updates=152000, lr=8.11107e-05, gnorm=1.115, loss_scale=4, train_wall=23, gb_free=10.6, wall=46375
2021-05-06 05:30:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:30:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:30:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:30:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:30:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:30:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:30:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:30:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:30:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:30:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:30:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:30:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:30:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:31:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.165 | ppl 35.89 | bleu 29.38 | wps 2416.3 | wpb 2024.1 | bsz 97.5 | num_updates 152000 | best_bleu 29.47
2021-05-06 05:31:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 152000 updates
2021-05-06 05:31:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_152000.pt
2021-05-06 05:31:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_152000.pt
2021-05-06 05:31:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_152000.pt (epoch 3 @ 152000 updates, score 29.38) (writing took 7.954670353006804 seconds)
2021-05-06 05:31:37 | INFO | train_inner | epoch 003:  31271 / 60421 loss=0.967, ppl=1.95, wps=3850.8, ups=1.04, wpb=3712, bsz=115.3, num_updates=152100, lr=8.1084e-05, gnorm=1.102, loss_scale=4, train_wall=23, gb_free=10.7, wall=46471
2021-05-06 05:32:00 | INFO | train_inner | epoch 003:  31371 / 60421 loss=0.939, ppl=1.92, wps=16381.5, ups=4.43, wpb=3698.6, bsz=129.3, num_updates=152200, lr=8.10574e-05, gnorm=1.203, loss_scale=4, train_wall=22, gb_free=11.3, wall=46494
2021-05-06 05:32:23 | INFO | train_inner | epoch 003:  31471 / 60421 loss=0.961, ppl=1.95, wps=16597.6, ups=4.35, wpb=3817.7, bsz=132, num_updates=152300, lr=8.10308e-05, gnorm=1.103, loss_scale=4, train_wall=23, gb_free=10.9, wall=46517
2021-05-06 05:32:45 | INFO | train_inner | epoch 003:  31571 / 60421 loss=0.937, ppl=1.91, wps=16543.1, ups=4.39, wpb=3764.6, bsz=123.1, num_updates=152400, lr=8.10042e-05, gnorm=1.139, loss_scale=4, train_wall=23, gb_free=10.8, wall=46539
2021-05-06 05:33:08 | INFO | train_inner | epoch 003:  31671 / 60421 loss=0.96, ppl=1.94, wps=16432.3, ups=4.44, wpb=3704.6, bsz=115.5, num_updates=152500, lr=8.09776e-05, gnorm=1.1, loss_scale=4, train_wall=22, gb_free=10.8, wall=46562
2021-05-06 05:33:31 | INFO | train_inner | epoch 003:  31771 / 60421 loss=0.924, ppl=1.9, wps=16323.5, ups=4.37, wpb=3736.3, bsz=128.4, num_updates=152600, lr=8.09511e-05, gnorm=1.102, loss_scale=4, train_wall=23, gb_free=10.6, wall=46585
2021-05-06 05:33:54 | INFO | train_inner | epoch 003:  31871 / 60421 loss=0.936, ppl=1.91, wps=16230.1, ups=4.4, wpb=3688.2, bsz=114.2, num_updates=152700, lr=8.09246e-05, gnorm=1.269, loss_scale=4, train_wall=23, gb_free=10.7, wall=46607
2021-05-06 05:34:17 | INFO | train_inner | epoch 003:  31971 / 60421 loss=0.838, ppl=1.79, wps=16154.2, ups=4.36, wpb=3707.6, bsz=164.2, num_updates=152800, lr=8.08981e-05, gnorm=0.968, loss_scale=4, train_wall=23, gb_free=10.8, wall=46630
2021-05-06 05:34:40 | INFO | train_inner | epoch 003:  32071 / 60421 loss=0.948, ppl=1.93, wps=16231.1, ups=4.33, wpb=3746.8, bsz=117.1, num_updates=152900, lr=8.08716e-05, gnorm=1.131, loss_scale=4, train_wall=23, gb_free=10.7, wall=46653
2021-05-06 05:35:03 | INFO | train_inner | epoch 003:  32171 / 60421 loss=0.935, ppl=1.91, wps=15979.3, ups=4.36, wpb=3665, bsz=150.4, num_updates=153000, lr=8.08452e-05, gnorm=1.351, loss_scale=4, train_wall=23, gb_free=10.8, wall=46676
2021-05-06 05:35:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:35:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:35:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:35:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:35:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:35:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:35:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:35:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:35:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:35:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:35:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:35:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:35:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:36:08 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.197 | ppl 36.69 | bleu 29.08 | wps 2430.3 | wpb 2024.1 | bsz 97.5 | num_updates 153000 | best_bleu 29.47
2021-05-06 05:36:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 153000 updates
2021-05-06 05:36:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_153000.pt
2021-05-06 05:36:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_153000.pt
2021-05-06 05:36:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_153000.pt (epoch 3 @ 153000 updates, score 29.08) (writing took 10.272432578000007 seconds)
2021-05-06 05:36:41 | INFO | train_inner | epoch 003:  32271 / 60421 loss=0.933, ppl=1.91, wps=3822.3, ups=1.02, wpb=3763.3, bsz=123.4, num_updates=153100, lr=8.08188e-05, gnorm=1.025, loss_scale=4, train_wall=23, gb_free=10.6, wall=46775
2021-05-06 05:37:04 | INFO | train_inner | epoch 003:  32371 / 60421 loss=0.987, ppl=1.98, wps=16510.1, ups=4.41, wpb=3743.4, bsz=125.7, num_updates=153200, lr=8.07924e-05, gnorm=1.137, loss_scale=4, train_wall=22, gb_free=10.7, wall=46798
2021-05-06 05:37:26 | INFO | train_inner | epoch 003:  32471 / 60421 loss=0.906, ppl=1.87, wps=16442.9, ups=4.44, wpb=3702.2, bsz=130.8, num_updates=153300, lr=8.07661e-05, gnorm=1.373, loss_scale=4, train_wall=22, gb_free=10.8, wall=46820
2021-05-06 05:37:49 | INFO | train_inner | epoch 003:  32571 / 60421 loss=0.929, ppl=1.9, wps=16434.3, ups=4.47, wpb=3680.3, bsz=141.8, num_updates=153400, lr=8.07397e-05, gnorm=1.445, loss_scale=4, train_wall=22, gb_free=10.9, wall=46842
2021-05-06 05:38:11 | INFO | train_inner | epoch 003:  32671 / 60421 loss=0.843, ppl=1.79, wps=16449.9, ups=4.43, wpb=3714.1, bsz=162.9, num_updates=153500, lr=8.07134e-05, gnorm=1.122, loss_scale=4, train_wall=22, gb_free=10.7, wall=46865
2021-05-06 05:38:34 | INFO | train_inner | epoch 003:  32771 / 60421 loss=0.997, ppl=2, wps=16536.1, ups=4.37, wpb=3782.8, bsz=113.2, num_updates=153600, lr=8.06872e-05, gnorm=1.108, loss_scale=4, train_wall=23, gb_free=10.8, wall=46888
2021-05-06 05:38:57 | INFO | train_inner | epoch 003:  32871 / 60421 loss=0.903, ppl=1.87, wps=16386, ups=4.36, wpb=3760.5, bsz=128.1, num_updates=153700, lr=8.06609e-05, gnorm=1.015, loss_scale=4, train_wall=23, gb_free=10.8, wall=46911
2021-05-06 05:39:20 | INFO | train_inner | epoch 003:  32971 / 60421 loss=0.969, ppl=1.96, wps=16326.5, ups=4.34, wpb=3764.9, bsz=129.9, num_updates=153800, lr=8.06347e-05, gnorm=1.217, loss_scale=4, train_wall=23, gb_free=10.9, wall=46934
2021-05-06 05:39:43 | INFO | train_inner | epoch 003:  33071 / 60421 loss=1.003, ppl=2, wps=16299.9, ups=4.32, wpb=3775.5, bsz=119.3, num_updates=153900, lr=8.06085e-05, gnorm=1.297, loss_scale=4, train_wall=23, gb_free=10.7, wall=46957
2021-05-06 05:40:07 | INFO | train_inner | epoch 003:  33171 / 60421 loss=1.052, ppl=2.07, wps=16243.7, ups=4.29, wpb=3787.9, bsz=113, num_updates=154000, lr=8.05823e-05, gnorm=1.17, loss_scale=4, train_wall=23, gb_free=10.9, wall=46980
2021-05-06 05:40:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:40:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:40:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:40:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:40:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:40:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:40:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:40:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:40:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:40:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:40:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:40:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:40:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:40:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:40:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:40:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:41:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.145 | ppl 35.39 | bleu 29.13 | wps 2412.9 | wpb 2024.1 | bsz 97.5 | num_updates 154000 | best_bleu 29.47
2021-05-06 05:41:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 154000 updates
2021-05-06 05:41:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_154000.pt
2021-05-06 05:41:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_154000.pt
2021-05-06 05:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_154000.pt (epoch 3 @ 154000 updates, score 29.13) (writing took 10.275060448009754 seconds)
2021-05-06 05:41:46 | INFO | train_inner | epoch 003:  33271 / 60421 loss=0.983, ppl=1.98, wps=3788.5, ups=1.01, wpb=3751.1, bsz=122.1, num_updates=154100, lr=8.05561e-05, gnorm=1.125, loss_scale=4, train_wall=23, gb_free=10.7, wall=47079
2021-05-06 05:42:08 | INFO | train_inner | epoch 003:  33371 / 60421 loss=0.941, ppl=1.92, wps=16479.8, ups=4.45, wpb=3705.1, bsz=135, num_updates=154200, lr=8.053e-05, gnorm=1.27, loss_scale=4, train_wall=22, gb_free=10.6, wall=47102
2021-05-06 05:42:31 | INFO | train_inner | epoch 003:  33471 / 60421 loss=0.891, ppl=1.85, wps=16539.4, ups=4.4, wpb=3756, bsz=138.7, num_updates=154300, lr=8.05039e-05, gnorm=1.004, loss_scale=4, train_wall=23, gb_free=11.1, wall=47125
2021-05-06 05:42:53 | INFO | train_inner | epoch 003:  33571 / 60421 loss=0.922, ppl=1.89, wps=16330.9, ups=4.47, wpb=3655.2, bsz=132.2, num_updates=154400, lr=8.04778e-05, gnorm=1.27, loss_scale=4, train_wall=22, gb_free=10.9, wall=47147
2021-05-06 05:43:16 | INFO | train_inner | epoch 003:  33671 / 60421 loss=0.932, ppl=1.91, wps=16526.4, ups=4.36, wpb=3787.4, bsz=135.7, num_updates=154500, lr=8.04518e-05, gnorm=1.046, loss_scale=4, train_wall=23, gb_free=10.7, wall=47170
2021-05-06 05:43:39 | INFO | train_inner | epoch 003:  33771 / 60421 loss=0.976, ppl=1.97, wps=16419.1, ups=4.39, wpb=3739.4, bsz=119.4, num_updates=154600, lr=8.04258e-05, gnorm=1.383, loss_scale=4, train_wall=23, gb_free=10.9, wall=47193
2021-05-06 05:44:02 | INFO | train_inner | epoch 003:  33871 / 60421 loss=0.931, ppl=1.91, wps=16324.7, ups=4.38, wpb=3730.9, bsz=126.7, num_updates=154700, lr=8.03998e-05, gnorm=1.04, loss_scale=4, train_wall=23, gb_free=11.1, wall=47216
2021-05-06 05:44:25 | INFO | train_inner | epoch 003:  33971 / 60421 loss=0.944, ppl=1.92, wps=16441.9, ups=4.32, wpb=3804.5, bsz=131.5, num_updates=154800, lr=8.03738e-05, gnorm=1.137, loss_scale=4, train_wall=23, gb_free=10.9, wall=47239
2021-05-06 05:44:48 | INFO | train_inner | epoch 003:  34071 / 60421 loss=0.933, ppl=1.91, wps=16257.9, ups=4.28, wpb=3799.9, bsz=147, num_updates=154900, lr=8.03479e-05, gnorm=1.11, loss_scale=4, train_wall=23, gb_free=10.8, wall=47262
2021-05-06 05:45:11 | INFO | train_inner | epoch 003:  34171 / 60421 loss=0.944, ppl=1.92, wps=15885.6, ups=4.32, wpb=3678.2, bsz=127.2, num_updates=155000, lr=8.03219e-05, gnorm=1.291, loss_scale=4, train_wall=23, gb_free=11.2, wall=47285
2021-05-06 05:45:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-06 05:45:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:45:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:45:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:45:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:45:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:45:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:45:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:45:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:45:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:45:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:45:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:45:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:45:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-06 05:45:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-06 05:45:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-06 05:46:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.178 | ppl 36.21 | bleu 29.22 | wps 2428.8 | wpb 2024.1 | bsz 97.5 | num_updates 155000 | best_bleu 29.47
2021-05-06 05:46:17 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 20 runs
2021-05-06 05:46:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 155000 updates
2021-05-06 05:46:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_155000.pt
2021-05-06 05:46:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_155000.pt
2021-05-06 05:46:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation_da/transformer_da/test1/mixed_zhen/checkpoint_3_155000.pt (epoch 3 @ 155000 updates, score 29.22) (writing took 10.294500060001155 seconds)
2021-05-06 05:46:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-05-06 05:46:27 | INFO | train | epoch 003 | loss 0.947 | ppl 1.93 | wps 12276.6 | ups 3.29 | wpb 3736.2 | bsz 132.6 | num_updates 155000 | lr 8.03219e-05 | gnorm 1.161 | loss_scale 4 | train_wall 7741 | gb_free 11.2 | wall 47361
2021-05-06 05:46:27 | INFO | fairseq_cli.train | done training in 47357.1 seconds
2021-05-06 05:46:27 | INFO | root | Added key: store_based_barrier_key:2 to store for rank: 0

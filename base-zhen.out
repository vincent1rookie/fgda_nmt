nohup: ignoring input
2021-05-01 12:15:14 | INFO | fairseq.distributed.utils | distributed init (rank 0): env://
2021-05-01 12:15:14 | INFO | root | Added key: store_based_barrier_key:1 to store for rank: 0
2021-05-01 12:15:14 | INFO | fairseq.distributed.utils | initialized host ip-172-31-23-127 as rank 0
2021-05-01 12:15:18 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': '/home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/logs', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'no_c10d', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': '/home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 20, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe='sentencepiece', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='/home/ubuntu/fgda_nmt/data-bin/translation/mixed_zhen/data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=True, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='/home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=4, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=20, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen', save_interval=1, save_interval_updates=1000, scoring='bleu', seed=1, sentence_avg=False, sentencepiece_model='???', shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='zh', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir='/home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/logs', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/home/ubuntu/fgda_nmt/data-bin/translation/mixed_zhen/data-bin', 'source_lang': 'zh', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.997)', 'adam_eps': 1e-09, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'sentencepiece', 'sentencepiece_model': '???'}, 'tokenizer': None, 'simul_type': None}
2021-05-01 12:15:18 | INFO | fairseq.tasks.translation | [zh] dictionary: 32001 types
2021-05-01 12:15:18 | INFO | fairseq.tasks.translation | [en] dictionary: 32001 types
2021-05-01 12:15:19 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(32001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(32001, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=32001, bias=False)
  )
)
2021-05-01 12:15:19 | INFO | fairseq_cli.train | task: TranslationTask
2021-05-01 12:15:19 | INFO | fairseq_cli.train | model: TransformerModel
2021-05-01 12:15:19 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2021-05-01 12:15:19 | INFO | fairseq_cli.train | num. shared model params: 93,292,032 (num. trained: 93,292,032)
2021-05-01 12:15:19 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2021-05-01 12:15:19 | INFO | fairseq.data.data_utils | loaded 7,607 examples from: /home/ubuntu/fgda_nmt/data-bin/translation/mixed_zhen/data-bin/valid.zh-en.zh
2021-05-01 12:15:19 | INFO | fairseq.data.data_utils | loaded 7,607 examples from: /home/ubuntu/fgda_nmt/data-bin/translation/mixed_zhen/data-bin/valid.zh-en.en
2021-05-01 12:15:19 | INFO | fairseq.tasks.translation | /home/ubuntu/fgda_nmt/data-bin/translation/mixed_zhen/data-bin valid zh-en 7607 examples
2021-05-01 12:15:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-05-01 12:15:19 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                
2021-05-01 12:15:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-05-01 12:15:19 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-05-01 12:15:19 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None
2021-05-01 12:15:19 | INFO | fairseq.checkpoint_utils | loading pretrained model from /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-05-01 12:15:19 | INFO | fairseq.trainer | Preparing to load checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt
2021-05-01 12:15:20 | INFO | fairseq.trainer | Loaded checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/un_zhen/checkpoint_best.pt (epoch 5 @ 0 updates)
2021-05-01 12:15:20 | INFO | fairseq.trainer | loading train data for epoch 1
2021-05-01 12:15:20 | INFO | fairseq.data.data_utils | loaded 8,000,000 examples from: /home/ubuntu/fgda_nmt/data-bin/translation/mixed_zhen/data-bin/train.zh-en.zh
2021-05-01 12:15:20 | INFO | fairseq.data.data_utils | loaded 8,000,000 examples from: /home/ubuntu/fgda_nmt/data-bin/translation/mixed_zhen/data-bin/train.zh-en.en
2021-05-01 12:15:20 | INFO | fairseq.tasks.translation | /home/ubuntu/fgda_nmt/data-bin/translation/mixed_zhen/data-bin train zh-en 8000000 examples
2021-05-01 12:15:23 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:15:23 | INFO | fairseq.trainer | begin training epoch 1
2021-05-01 12:15:23 | INFO | fairseq_cli.train | Start iterating over samples
2021-05-01 12:15:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2021-05-01 12:15:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2021-05-01 12:15:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2021-05-01 12:15:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2021-05-01 12:15:46 | INFO | train_inner | epoch 001:    104 / 60421 loss=2.64, ppl=6.23, wps=16513.2, ups=4.33, wpb=3812.1, bsz=124.2, num_updates=100, lr=1.25e-05, gnorm=2.319, loss_scale=8, train_wall=23, gb_free=10.8, wall=27
2021-05-01 12:16:09 | INFO | train_inner | epoch 001:    204 / 60421 loss=2.584, ppl=5.99, wps=16592.3, ups=4.44, wpb=3738.9, bsz=128.8, num_updates=200, lr=2.5e-05, gnorm=2.076, loss_scale=8, train_wall=22, gb_free=10.6, wall=50
2021-05-01 12:16:32 | INFO | train_inner | epoch 001:    304 / 60421 loss=2.606, ppl=6.09, wps=16585, ups=4.41, wpb=3760, bsz=109, num_updates=300, lr=3.75e-05, gnorm=2.236, loss_scale=8, train_wall=22, gb_free=10.8, wall=73
2021-05-01 12:16:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-01 12:16:55 | INFO | train_inner | epoch 001:    405 / 60421 loss=2.59, ppl=6.02, wps=16142.3, ups=4.32, wpb=3732.9, bsz=130.6, num_updates=400, lr=5e-05, gnorm=2.375, loss_scale=4, train_wall=23, gb_free=11, wall=96
2021-05-01 12:17:17 | INFO | train_inner | epoch 001:    505 / 60421 loss=2.51, ppl=5.7, wps=16581.9, ups=4.39, wpb=3776.2, bsz=138.7, num_updates=500, lr=6.25e-05, gnorm=2.097, loss_scale=4, train_wall=23, gb_free=10.8, wall=118
2021-05-01 12:17:40 | INFO | train_inner | epoch 001:    605 / 60421 loss=2.482, ppl=5.59, wps=16646.6, ups=4.45, wpb=3738.5, bsz=128.6, num_updates=600, lr=7.5e-05, gnorm=2.392, loss_scale=4, train_wall=22, gb_free=10.8, wall=141
2021-05-01 12:18:02 | INFO | train_inner | epoch 001:    705 / 60421 loss=2.583, ppl=5.99, wps=16588, ups=4.45, wpb=3727.7, bsz=130.9, num_updates=700, lr=8.75e-05, gnorm=2.331, loss_scale=4, train_wall=22, gb_free=11, wall=163
2021-05-01 12:18:25 | INFO | train_inner | epoch 001:    805 / 60421 loss=2.52, ppl=5.74, wps=16742.7, ups=4.38, wpb=3821.7, bsz=135.9, num_updates=800, lr=0.0001, gnorm=2.038, loss_scale=4, train_wall=23, gb_free=11, wall=186
2021-05-01 12:18:48 | INFO | train_inner | epoch 001:    905 / 60421 loss=2.522, ppl=5.74, wps=16534, ups=4.43, wpb=3733.8, bsz=140.6, num_updates=900, lr=0.0001125, gnorm=2.311, loss_scale=4, train_wall=22, gb_free=12, wall=209
2021-05-01 12:19:10 | INFO | train_inner | epoch 001:   1005 / 60421 loss=2.512, ppl=5.7, wps=16489.5, ups=4.47, wpb=3690.9, bsz=127.5, num_updates=1000, lr=0.000125, gnorm=2.43, loss_scale=4, train_wall=22, gb_free=10.8, wall=231
2021-05-01 12:19:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:19:10 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:19:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:19:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:19:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:19:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:19:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:19:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:19:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:19:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:19:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:19:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:19:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:19:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:19:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:19:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:19:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:20:15 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.444 | ppl 21.77 | bleu 16.37 | wps 2449.6 | wpb 2024.1 | bsz 97.5 | num_updates 1000
2021-05-01 12:20:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1000 updates
2021-05-01 12:20:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_1000.pt
2021-05-01 12:20:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_1000.pt
2021-05-01 12:20:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 16.37) (writing took 6.491442115999234 seconds)
2021-05-01 12:20:44 | INFO | train_inner | epoch 001:   1105 / 60421 loss=2.445, ppl=5.44, wps=4031.1, ups=1.06, wpb=3799.8, bsz=157.8, num_updates=1100, lr=0.0001375, gnorm=1.982, loss_scale=4, train_wall=23, gb_free=10.9, wall=325
2021-05-01 12:21:08 | INFO | train_inner | epoch 001:   1205 / 60421 loss=2.435, ppl=5.41, wps=16290.4, ups=4.31, wpb=3777.8, bsz=133.8, num_updates=1200, lr=0.00015, gnorm=2.064, loss_scale=4, train_wall=23, gb_free=11.1, wall=349
2021-05-01 12:21:31 | INFO | train_inner | epoch 001:   1305 / 60421 loss=2.417, ppl=5.34, wps=15861.6, ups=4.33, wpb=3659.7, bsz=126.7, num_updates=1300, lr=0.0001625, gnorm=2.257, loss_scale=4, train_wall=23, gb_free=10.9, wall=372
2021-05-01 12:21:54 | INFO | train_inner | epoch 001:   1405 / 60421 loss=2.46, ppl=5.5, wps=15841.4, ups=4.27, wpb=3710.2, bsz=131, num_updates=1400, lr=0.000175, gnorm=2.215, loss_scale=4, train_wall=23, gb_free=11.1, wall=395
2021-05-01 12:22:17 | INFO | train_inner | epoch 001:   1505 / 60421 loss=2.321, ppl=5, wps=16304.3, ups=4.34, wpb=3759.5, bsz=142.6, num_updates=1500, lr=0.0001875, gnorm=2.331, loss_scale=4, train_wall=23, gb_free=12.1, wall=418
2021-05-01 12:22:40 | INFO | train_inner | epoch 001:   1605 / 60421 loss=2.572, ppl=5.94, wps=16465.7, ups=4.44, wpb=3708.7, bsz=131, num_updates=1600, lr=0.0002, gnorm=2.748, loss_scale=4, train_wall=22, gb_free=11.9, wall=441
2021-05-01 12:23:02 | INFO | train_inner | epoch 001:   1705 / 60421 loss=2.413, ppl=5.33, wps=16527.3, ups=4.4, wpb=3758.2, bsz=130.9, num_updates=1700, lr=0.0002125, gnorm=2.147, loss_scale=4, train_wall=23, gb_free=11.1, wall=463
2021-05-01 12:23:25 | INFO | train_inner | epoch 001:   1805 / 60421 loss=2.385, ppl=5.22, wps=16508.3, ups=4.44, wpb=3716.9, bsz=156.9, num_updates=1800, lr=0.000225, gnorm=2.425, loss_scale=4, train_wall=22, gb_free=10.7, wall=486
2021-05-01 12:23:47 | INFO | train_inner | epoch 001:   1905 / 60421 loss=2.48, ppl=5.58, wps=16584.8, ups=4.46, wpb=3722.6, bsz=119.1, num_updates=1900, lr=0.0002375, gnorm=2.344, loss_scale=4, train_wall=22, gb_free=10.9, wall=508
2021-05-01 12:24:10 | INFO | train_inner | epoch 001:   2005 / 60421 loss=2.372, ppl=5.18, wps=16388.2, ups=4.41, wpb=3714.2, bsz=126.9, num_updates=2000, lr=0.00025, gnorm=2.468, loss_scale=4, train_wall=22, gb_free=10.8, wall=531
2021-05-01 12:24:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:24:10 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:24:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:24:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:24:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:24:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:24:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:24:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:24:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:24:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:24:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:24:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:24:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:24:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:25:19 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.3 | ppl 19.7 | bleu 18.27 | wps 2316.2 | wpb 2024.1 | bsz 97.5 | num_updates 2000 | best_bleu 18.27
2021-05-01 12:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 2000 updates
2021-05-01 12:25:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_2000.pt
2021-05-01 12:25:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_2000.pt
2021-05-01 12:25:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 18.27) (writing took 14.09742524900139 seconds)
2021-05-01 12:25:55 | INFO | train_inner | epoch 001:   2105 / 60421 loss=2.618, ppl=6.14, wps=3512.4, ups=0.95, wpb=3685.1, bsz=136.7, num_updates=2100, lr=0.0002625, gnorm=2.936, loss_scale=4, train_wall=22, gb_free=10.9, wall=636
2021-05-01 12:26:18 | INFO | train_inner | epoch 001:   2205 / 60421 loss=2.223, ppl=4.67, wps=16645.3, ups=4.43, wpb=3754.4, bsz=113.7, num_updates=2200, lr=0.000275, gnorm=2.408, loss_scale=4, train_wall=22, gb_free=10.8, wall=659
2021-05-01 12:26:40 | INFO | train_inner | epoch 001:   2305 / 60421 loss=2.386, ppl=5.23, wps=16744.5, ups=4.44, wpb=3768.3, bsz=141.2, num_updates=2300, lr=0.0002875, gnorm=2.022, loss_scale=4, train_wall=22, gb_free=10.7, wall=681
2021-05-01 12:27:03 | INFO | train_inner | epoch 001:   2405 / 60421 loss=2.466, ppl=5.53, wps=16519.3, ups=4.41, wpb=3744.1, bsz=135.1, num_updates=2400, lr=0.0003, gnorm=2.509, loss_scale=4, train_wall=22, gb_free=10.8, wall=704
2021-05-01 12:27:25 | INFO | train_inner | epoch 001:   2505 / 60421 loss=2.512, ppl=5.7, wps=16688.8, ups=4.43, wpb=3767.5, bsz=122, num_updates=2500, lr=0.0003125, gnorm=2.44, loss_scale=4, train_wall=22, gb_free=10.7, wall=726
2021-05-01 12:27:48 | INFO | train_inner | epoch 001:   2605 / 60421 loss=2.453, ppl=5.48, wps=16569.5, ups=4.43, wpb=3742.8, bsz=138.9, num_updates=2600, lr=0.000325, gnorm=2.319, loss_scale=4, train_wall=22, gb_free=10.7, wall=749
2021-05-01 12:28:10 | INFO | train_inner | epoch 001:   2705 / 60421 loss=2.496, ppl=5.64, wps=16562, ups=4.44, wpb=3734, bsz=134.5, num_updates=2700, lr=0.0003375, gnorm=2.753, loss_scale=4, train_wall=22, gb_free=10.8, wall=771
2021-05-01 12:28:33 | INFO | train_inner | epoch 001:   2805 / 60421 loss=2.478, ppl=5.57, wps=16644.2, ups=4.44, wpb=3748.6, bsz=139.1, num_updates=2800, lr=0.00035, gnorm=2.386, loss_scale=4, train_wall=22, gb_free=10.7, wall=794
2021-05-01 12:28:56 | INFO | train_inner | epoch 001:   2905 / 60421 loss=2.424, ppl=5.37, wps=16507.7, ups=4.42, wpb=3734.8, bsz=147, num_updates=2900, lr=0.0003625, gnorm=2.126, loss_scale=4, train_wall=22, gb_free=10.8, wall=817
2021-05-01 12:29:18 | INFO | train_inner | epoch 001:   3005 / 60421 loss=2.412, ppl=5.32, wps=16495, ups=4.42, wpb=3728.2, bsz=110.3, num_updates=3000, lr=0.000375, gnorm=2.301, loss_scale=4, train_wall=22, gb_free=10.8, wall=839
2021-05-01 12:29:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:29:18 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:29:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:29:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:29:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:29:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:29:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:29:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:29:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:29:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:29:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:30:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.275 | ppl 19.36 | bleu 17.67 | wps 2294.4 | wpb 2024.1 | bsz 97.5 | num_updates 3000 | best_bleu 18.27
2021-05-01 12:30:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3000 updates
2021-05-01 12:30:27 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_3000.pt
2021-05-01 12:30:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_3000.pt
2021-05-01 12:30:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 17.67) (writing took 7.801786414001981 seconds)
2021-05-01 12:30:58 | INFO | train_inner | epoch 001:   3105 / 60421 loss=2.468, ppl=5.53, wps=3752.5, ups=1, wpb=3747.8, bsz=136, num_updates=3100, lr=0.0003875, gnorm=2.204, loss_scale=4, train_wall=23, gb_free=10.9, wall=939
2021-05-01 12:31:21 | INFO | train_inner | epoch 001:   3205 / 60421 loss=2.427, ppl=5.38, wps=16270.6, ups=4.32, wpb=3763.9, bsz=132.2, num_updates=3200, lr=0.0004, gnorm=2.251, loss_scale=4, train_wall=23, gb_free=10.8, wall=962
2021-05-01 12:31:45 | INFO | train_inner | epoch 001:   3305 / 60421 loss=2.469, ppl=5.54, wps=16028.7, ups=4.23, wpb=3790.7, bsz=149.5, num_updates=3300, lr=0.0004125, gnorm=2.225, loss_scale=4, train_wall=23, gb_free=10.8, wall=986
2021-05-01 12:32:08 | INFO | train_inner | epoch 001:   3405 / 60421 loss=2.729, ppl=6.63, wps=16132.1, ups=4.33, wpb=3722.3, bsz=161.1, num_updates=3400, lr=0.000425, gnorm=2.805, loss_scale=4, train_wall=23, gb_free=10.7, wall=1009
2021-05-01 12:32:31 | INFO | train_inner | epoch 001:   3505 / 60421 loss=2.605, ppl=6.09, wps=16442.2, ups=4.39, wpb=3747.6, bsz=139.8, num_updates=3500, lr=0.0004375, gnorm=2.432, loss_scale=4, train_wall=23, gb_free=11.1, wall=1032
2021-05-01 12:32:53 | INFO | train_inner | epoch 001:   3605 / 60421 loss=2.701, ppl=6.5, wps=16512.3, ups=4.43, wpb=3729.4, bsz=134.9, num_updates=3600, lr=0.00045, gnorm=2.404, loss_scale=4, train_wall=22, gb_free=10.8, wall=1054
2021-05-01 12:33:16 | INFO | train_inner | epoch 001:   3705 / 60421 loss=2.611, ppl=6.11, wps=16606.4, ups=4.41, wpb=3767.9, bsz=136.6, num_updates=3700, lr=0.0004625, gnorm=2.606, loss_scale=4, train_wall=23, gb_free=10.6, wall=1077
2021-05-01 12:33:39 | INFO | train_inner | epoch 001:   3805 / 60421 loss=2.649, ppl=6.27, wps=16542.8, ups=4.39, wpb=3768.5, bsz=132.4, num_updates=3800, lr=0.000475, gnorm=2.472, loss_scale=4, train_wall=23, gb_free=10.8, wall=1100
2021-05-01 12:34:01 | INFO | train_inner | epoch 001:   3905 / 60421 loss=2.809, ppl=7.01, wps=16456.7, ups=4.47, wpb=3682.5, bsz=129.7, num_updates=3900, lr=0.0004875, gnorm=3.2, loss_scale=4, train_wall=22, gb_free=10.7, wall=1122
2021-05-01 12:34:24 | INFO | train_inner | epoch 001:   4005 / 60421 loss=2.742, ppl=6.69, wps=16390, ups=4.39, wpb=3733.8, bsz=127.2, num_updates=4000, lr=0.0005, gnorm=3.103, loss_scale=4, train_wall=23, gb_free=10.9, wall=1145
2021-05-01 12:34:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:34:24 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:34:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:34:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:34:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:34:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:34:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:34:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:34:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:34:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:34:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:34:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:34:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:34:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:34:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:34:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:34:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:35:34 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.503 | ppl 22.67 | bleu 16.2 | wps 2265.8 | wpb 2024.1 | bsz 97.5 | num_updates 4000 | best_bleu 18.27
2021-05-01 12:35:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 4000 updates
2021-05-01 12:35:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_4000.pt
2021-05-01 12:35:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_4000.pt
2021-05-01 12:35:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 16.2) (writing took 10.111894304001908 seconds)
2021-05-01 12:36:07 | INFO | train_inner | epoch 001:   4105 / 60421 loss=2.802, ppl=6.97, wps=3599.7, ups=0.97, wpb=3709.2, bsz=105.3, num_updates=4100, lr=0.000493865, gnorm=3.189, loss_scale=4, train_wall=23, gb_free=10.8, wall=1248
2021-05-01 12:36:30 | INFO | train_inner | epoch 001:   4205 / 60421 loss=2.778, ppl=6.86, wps=16143.8, ups=4.33, wpb=3732.2, bsz=128.2, num_updates=4200, lr=0.00048795, gnorm=2.724, loss_scale=4, train_wall=23, gb_free=10.8, wall=1271
2021-05-01 12:36:54 | INFO | train_inner | epoch 001:   4305 / 60421 loss=2.648, ppl=6.27, wps=15995, ups=4.23, wpb=3780.4, bsz=145.8, num_updates=4300, lr=0.000482243, gnorm=2.413, loss_scale=4, train_wall=23, gb_free=11, wall=1295
2021-05-01 12:37:17 | INFO | train_inner | epoch 001:   4405 / 60421 loss=2.479, ppl=5.57, wps=16216.9, ups=4.36, wpb=3720.8, bsz=118.1, num_updates=4400, lr=0.000476731, gnorm=2.239, loss_scale=4, train_wall=23, gb_free=10.8, wall=1318
2021-05-01 12:37:40 | INFO | train_inner | epoch 001:   4505 / 60421 loss=2.634, ppl=6.21, wps=16532.9, ups=4.36, wpb=3788.5, bsz=127.4, num_updates=4500, lr=0.000471405, gnorm=2.555, loss_scale=4, train_wall=23, gb_free=10.8, wall=1341
2021-05-01 12:38:02 | INFO | train_inner | epoch 001:   4605 / 60421 loss=2.601, ppl=6.07, wps=16560.1, ups=4.41, wpb=3755.9, bsz=147.3, num_updates=4600, lr=0.000466252, gnorm=2.388, loss_scale=4, train_wall=23, gb_free=10.8, wall=1363
2021-05-01 12:38:25 | INFO | train_inner | epoch 001:   4705 / 60421 loss=2.675, ppl=6.39, wps=16435.6, ups=4.42, wpb=3720, bsz=123.1, num_updates=4700, lr=0.000461266, gnorm=2.647, loss_scale=4, train_wall=22, gb_free=10.8, wall=1386
2021-05-01 12:38:48 | INFO | train_inner | epoch 001:   4805 / 60421 loss=2.404, ppl=5.29, wps=16686.3, ups=4.4, wpb=3791.7, bsz=134.6, num_updates=4800, lr=0.000456435, gnorm=2.063, loss_scale=4, train_wall=23, gb_free=10.8, wall=1409
2021-05-01 12:39:11 | INFO | train_inner | epoch 001:   4905 / 60421 loss=2.614, ppl=6.12, wps=16437.2, ups=4.35, wpb=3777.8, bsz=140.5, num_updates=4900, lr=0.000451754, gnorm=2.366, loss_scale=4, train_wall=23, gb_free=11, wall=1432
2021-05-01 12:39:33 | INFO | train_inner | epoch 001:   5005 / 60421 loss=2.557, ppl=5.88, wps=16473.2, ups=4.38, wpb=3764.3, bsz=136.2, num_updates=5000, lr=0.000447214, gnorm=2.424, loss_scale=4, train_wall=23, gb_free=11, wall=1454
2021-05-01 12:39:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:39:33 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:39:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:39:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:39:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:39:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:39:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:39:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:39:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:39:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:39:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:40:37 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.255 | ppl 19.09 | bleu 17.9 | wps 2488.4 | wpb 2024.1 | bsz 97.5 | num_updates 5000 | best_bleu 18.27
2021-05-01 12:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 5000 updates
2021-05-01 12:40:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_5000.pt
2021-05-01 12:40:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_5000.pt
2021-05-01 12:40:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 17.9) (writing took 8.174897001998033 seconds)
2021-05-01 12:41:09 | INFO | train_inner | epoch 001:   5105 / 60421 loss=2.548, ppl=5.85, wps=3959.5, ups=1.05, wpb=3762.9, bsz=137.4, num_updates=5100, lr=0.000442807, gnorm=2.432, loss_scale=4, train_wall=23, gb_free=10.9, wall=1549
2021-05-01 12:41:32 | INFO | train_inner | epoch 001:   5205 / 60421 loss=2.66, ppl=6.32, wps=15967.8, ups=4.32, wpb=3700.2, bsz=149, num_updates=5200, lr=0.000438529, gnorm=2.804, loss_scale=4, train_wall=23, gb_free=10.7, wall=1573
2021-05-01 12:41:55 | INFO | train_inner | epoch 001:   5305 / 60421 loss=2.748, ppl=6.72, wps=15815.4, ups=4.32, wpb=3665, bsz=124.6, num_updates=5300, lr=0.000434372, gnorm=2.781, loss_scale=4, train_wall=23, gb_free=10.8, wall=1596
2021-05-01 12:42:18 | INFO | train_inner | epoch 001:   5405 / 60421 loss=2.575, ppl=5.96, wps=16458.1, ups=4.35, wpb=3782.7, bsz=143.4, num_updates=5400, lr=0.000430331, gnorm=2.391, loss_scale=4, train_wall=23, gb_free=10.8, wall=1619
2021-05-01 12:42:41 | INFO | train_inner | epoch 001:   5505 / 60421 loss=2.449, ppl=5.46, wps=16618.6, ups=4.41, wpb=3767, bsz=117.3, num_updates=5500, lr=0.000426401, gnorm=2.092, loss_scale=4, train_wall=22, gb_free=11.1, wall=1641
2021-05-01 12:43:03 | INFO | train_inner | epoch 001:   5605 / 60421 loss=2.587, ppl=6.01, wps=16462.2, ups=4.44, wpb=3705.9, bsz=138.2, num_updates=5600, lr=0.000422577, gnorm=2.562, loss_scale=4, train_wall=22, gb_free=10.7, wall=1664
2021-05-01 12:43:26 | INFO | train_inner | epoch 001:   5705 / 60421 loss=2.622, ppl=6.15, wps=16454.2, ups=4.39, wpb=3745.2, bsz=152.5, num_updates=5700, lr=0.000418854, gnorm=2.473, loss_scale=4, train_wall=23, gb_free=10.7, wall=1687
2021-05-01 12:43:48 | INFO | train_inner | epoch 001:   5805 / 60421 loss=2.414, ppl=5.33, wps=16499.9, ups=4.41, wpb=3741.5, bsz=131.1, num_updates=5800, lr=0.000415227, gnorm=2.144, loss_scale=4, train_wall=22, gb_free=10.9, wall=1709
2021-05-01 12:44:11 | INFO | train_inner | epoch 001:   5905 / 60421 loss=2.347, ppl=5.09, wps=16499.6, ups=4.38, wpb=3768.8, bsz=138.1, num_updates=5900, lr=0.000411693, gnorm=2.059, loss_scale=4, train_wall=23, gb_free=10.8, wall=1732
2021-05-01 12:44:34 | INFO | train_inner | epoch 001:   6005 / 60421 loss=2.624, ppl=6.16, wps=16431.7, ups=4.45, wpb=3689.5, bsz=124.3, num_updates=6000, lr=0.000408248, gnorm=2.6, loss_scale=4, train_wall=22, gb_free=10.8, wall=1755
2021-05-01 12:44:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:44:34 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:44:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:44:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:44:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:44:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:44:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:44:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:44:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:44:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:44:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:44:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:44:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:44:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:45:40 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.109 | ppl 17.26 | bleu 19.54 | wps 2379.8 | wpb 2024.1 | bsz 97.5 | num_updates 6000 | best_bleu 19.54
2021-05-01 12:45:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 6000 updates
2021-05-01 12:45:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_6000.pt
2021-05-01 12:45:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_6000.pt
2021-05-01 12:45:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 19.54) (writing took 13.285750868999457 seconds)
2021-05-01 12:46:17 | INFO | train_inner | epoch 001:   6105 / 60421 loss=2.614, ppl=6.12, wps=3608.5, ups=0.97, wpb=3712.1, bsz=125.8, num_updates=6100, lr=0.000404888, gnorm=2.917, loss_scale=4, train_wall=23, gb_free=10.7, wall=1858
2021-05-01 12:46:40 | INFO | train_inner | epoch 001:   6205 / 60421 loss=2.593, ppl=6.03, wps=16010.8, ups=4.32, wpb=3708.4, bsz=130.6, num_updates=6200, lr=0.00040161, gnorm=2.486, loss_scale=4, train_wall=23, gb_free=11.1, wall=1881
2021-05-01 12:47:03 | INFO | train_inner | epoch 001:   6305 / 60421 loss=2.391, ppl=5.25, wps=16010.6, ups=4.3, wpb=3721.3, bsz=136.5, num_updates=6300, lr=0.00039841, gnorm=2.263, loss_scale=4, train_wall=23, gb_free=10.8, wall=1904
2021-05-01 12:47:26 | INFO | train_inner | epoch 001:   6405 / 60421 loss=2.595, ppl=6.04, wps=16452.5, ups=4.41, wpb=3733.7, bsz=135.9, num_updates=6400, lr=0.000395285, gnorm=2.644, loss_scale=4, train_wall=23, gb_free=10.8, wall=1927
2021-05-01 12:47:49 | INFO | train_inner | epoch 001:   6505 / 60421 loss=2.489, ppl=5.61, wps=16398.2, ups=4.37, wpb=3750.6, bsz=161.5, num_updates=6500, lr=0.000392232, gnorm=2.303, loss_scale=4, train_wall=23, gb_free=10.9, wall=1950
2021-05-01 12:48:11 | INFO | train_inner | epoch 001:   6605 / 60421 loss=2.491, ppl=5.62, wps=16600.1, ups=4.45, wpb=3726.6, bsz=123.8, num_updates=6600, lr=0.000389249, gnorm=2.115, loss_scale=4, train_wall=22, gb_free=10.7, wall=1972
2021-05-01 12:48:34 | INFO | train_inner | epoch 001:   6705 / 60421 loss=2.468, ppl=5.53, wps=16645.4, ups=4.43, wpb=3759.4, bsz=115.4, num_updates=6700, lr=0.000386334, gnorm=2.457, loss_scale=4, train_wall=22, gb_free=10.6, wall=1995
2021-05-01 12:48:56 | INFO | train_inner | epoch 001:   6805 / 60421 loss=2.35, ppl=5.1, wps=16464.4, ups=4.4, wpb=3743, bsz=154.7, num_updates=6800, lr=0.000383482, gnorm=2.109, loss_scale=4, train_wall=23, gb_free=10.9, wall=2017
2021-05-01 12:49:19 | INFO | train_inner | epoch 001:   6905 / 60421 loss=2.5, ppl=5.66, wps=16327.8, ups=4.45, wpb=3668.5, bsz=125, num_updates=6900, lr=0.000380693, gnorm=2.547, loss_scale=4, train_wall=22, gb_free=10.8, wall=2040
2021-05-01 12:49:42 | INFO | train_inner | epoch 001:   7005 / 60421 loss=2.369, ppl=5.17, wps=16384, ups=4.35, wpb=3767.7, bsz=139.4, num_updates=7000, lr=0.000377964, gnorm=2.025, loss_scale=4, train_wall=23, gb_free=10.7, wall=2063
2021-05-01 12:49:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:49:42 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:49:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:49:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:49:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:49:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:49:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:49:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:49:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:49:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:49:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:49:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:49:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:49:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:50:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:50:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:50:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:50:48 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.983 | ppl 15.81 | bleu 20.31 | wps 2381.8 | wpb 2024.1 | bsz 97.5 | num_updates 7000 | best_bleu 20.31
2021-05-01 12:50:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 7000 updates
2021-05-01 12:50:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_7000.pt
2021-05-01 12:50:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_7000.pt
2021-05-01 12:51:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 20.31) (writing took 14.40758455300238 seconds)
2021-05-01 12:51:26 | INFO | train_inner | epoch 001:   7105 / 60421 loss=2.307, ppl=4.95, wps=3612.1, ups=0.96, wpb=3761.7, bsz=120.2, num_updates=7100, lr=0.000375293, gnorm=2.1, loss_scale=4, train_wall=23, gb_free=11, wall=2167
2021-05-01 12:51:49 | INFO | train_inner | epoch 001:   7205 / 60421 loss=2.373, ppl=5.18, wps=15925.6, ups=4.32, wpb=3688.8, bsz=118.5, num_updates=7200, lr=0.000372678, gnorm=2.27, loss_scale=4, train_wall=23, gb_free=10.8, wall=2190
2021-05-01 12:52:12 | INFO | train_inner | epoch 001:   7305 / 60421 loss=2.231, ppl=4.7, wps=16397.9, ups=4.34, wpb=3779.6, bsz=138.4, num_updates=7300, lr=0.000370117, gnorm=1.893, loss_scale=4, train_wall=23, gb_free=10.9, wall=2213
2021-05-01 12:52:35 | INFO | train_inner | epoch 001:   7405 / 60421 loss=2.322, ppl=5, wps=16506.4, ups=4.42, wpb=3734.4, bsz=129.7, num_updates=7400, lr=0.000367607, gnorm=2.101, loss_scale=4, train_wall=22, gb_free=11.1, wall=2236
2021-05-01 12:52:57 | INFO | train_inner | epoch 001:   7505 / 60421 loss=2.403, ppl=5.29, wps=16471.1, ups=4.45, wpb=3705.3, bsz=108.1, num_updates=7500, lr=0.000365148, gnorm=2.54, loss_scale=4, train_wall=22, gb_free=10.7, wall=2258
2021-05-01 12:53:20 | INFO | train_inner | epoch 001:   7605 / 60421 loss=2.232, ppl=4.7, wps=16698.4, ups=4.39, wpb=3805, bsz=136.4, num_updates=7600, lr=0.000362738, gnorm=1.903, loss_scale=4, train_wall=23, gb_free=10.9, wall=2281
2021-05-01 12:53:43 | INFO | train_inner | epoch 001:   7705 / 60421 loss=2.432, ppl=5.4, wps=16539.1, ups=4.46, wpb=3710.5, bsz=122.6, num_updates=7700, lr=0.000360375, gnorm=2.264, loss_scale=4, train_wall=22, gb_free=11.2, wall=2303
2021-05-01 12:54:05 | INFO | train_inner | epoch 001:   7805 / 60421 loss=2.468, ppl=5.53, wps=16390, ups=4.45, wpb=3682.4, bsz=150.1, num_updates=7800, lr=0.000358057, gnorm=2.47, loss_scale=4, train_wall=22, gb_free=11.3, wall=2326
2021-05-01 12:54:28 | INFO | train_inner | epoch 001:   7905 / 60421 loss=2.452, ppl=5.47, wps=16617.6, ups=4.41, wpb=3769.2, bsz=129.2, num_updates=7900, lr=0.000355784, gnorm=2.566, loss_scale=4, train_wall=23, gb_free=10.8, wall=2349
2021-05-01 12:54:50 | INFO | train_inner | epoch 001:   8005 / 60421 loss=2.337, ppl=5.05, wps=16210.9, ups=4.41, wpb=3675.6, bsz=132.7, num_updates=8000, lr=0.000353553, gnorm=2.188, loss_scale=4, train_wall=22, gb_free=10.8, wall=2371
2021-05-01 12:54:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:54:50 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 12:55:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:55:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:55:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:55:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:55:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:55:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:55:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:55:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:55:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:55:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:55:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:55:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:55:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 12:55:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 12:55:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 12:55:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.009 | ppl 16.1 | bleu 20.35 | wps 2427.3 | wpb 2024.1 | bsz 97.5 | num_updates 8000 | best_bleu 20.35
2021-05-01 12:55:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 8000 updates
2021-05-01 12:55:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_8000.pt
2021-05-01 12:55:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_8000.pt
2021-05-01 12:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 20.35) (writing took 14.368984157998057 seconds)
2021-05-01 12:56:33 | INFO | train_inner | epoch 001:   8105 / 60421 loss=2.28, ppl=4.86, wps=3660.4, ups=0.97, wpb=3775.5, bsz=150.6, num_updates=8100, lr=0.000351364, gnorm=2.026, loss_scale=4, train_wall=23, gb_free=10.8, wall=2474
2021-05-01 12:56:57 | INFO | train_inner | epoch 001:   8205 / 60421 loss=2.175, ppl=4.52, wps=16175.6, ups=4.29, wpb=3771.5, bsz=143.4, num_updates=8200, lr=0.000349215, gnorm=1.943, loss_scale=4, train_wall=23, gb_free=10.7, wall=2498
2021-05-01 12:57:20 | INFO | train_inner | epoch 001:   8305 / 60421 loss=2.268, ppl=4.82, wps=16415.5, ups=4.39, wpb=3739.5, bsz=145.2, num_updates=8300, lr=0.000347105, gnorm=2.072, loss_scale=4, train_wall=23, gb_free=10.6, wall=2521
2021-05-01 12:57:42 | INFO | train_inner | epoch 001:   8405 / 60421 loss=2.355, ppl=5.12, wps=16442.5, ups=4.37, wpb=3766.8, bsz=124.5, num_updates=8400, lr=0.000345033, gnorm=2.132, loss_scale=4, train_wall=23, gb_free=10.7, wall=2543
2021-05-01 12:58:05 | INFO | train_inner | epoch 001:   8505 / 60421 loss=2.095, ppl=4.27, wps=16722.2, ups=4.39, wpb=3812.2, bsz=123.8, num_updates=8500, lr=0.000342997, gnorm=1.61, loss_scale=4, train_wall=23, gb_free=10.9, wall=2566
2021-05-01 12:58:28 | INFO | train_inner | epoch 001:   8605 / 60421 loss=2.245, ppl=4.74, wps=16617.8, ups=4.44, wpb=3741.7, bsz=122, num_updates=8600, lr=0.000340997, gnorm=2.05, loss_scale=4, train_wall=22, gb_free=10.6, wall=2589
2021-05-01 12:58:50 | INFO | train_inner | epoch 001:   8705 / 60421 loss=2.355, ppl=5.12, wps=16362.7, ups=4.42, wpb=3701.3, bsz=149.3, num_updates=8700, lr=0.000339032, gnorm=2.522, loss_scale=4, train_wall=22, gb_free=10.8, wall=2611
2021-05-01 12:59:13 | INFO | train_inner | epoch 001:   8805 / 60421 loss=2.24, ppl=4.72, wps=16296.9, ups=4.4, wpb=3704.9, bsz=144.6, num_updates=8800, lr=0.0003371, gnorm=2.075, loss_scale=4, train_wall=23, gb_free=11, wall=2634
2021-05-01 12:59:36 | INFO | train_inner | epoch 001:   8905 / 60421 loss=2.25, ppl=4.76, wps=16454.2, ups=4.43, wpb=3710.2, bsz=125.6, num_updates=8900, lr=0.000335201, gnorm=2.298, loss_scale=4, train_wall=22, gb_free=11, wall=2657
2021-05-01 12:59:59 | INFO | train_inner | epoch 001:   9005 / 60421 loss=2.353, ppl=5.11, wps=16256.7, ups=4.38, wpb=3707.5, bsz=146.1, num_updates=9000, lr=0.000333333, gnorm=2.347, loss_scale=4, train_wall=23, gb_free=11.2, wall=2679
2021-05-01 12:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 12:59:59 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:00:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:00:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:00:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:00:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:00:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:00:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:00:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:00:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:00:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:00:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:00:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:00:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:00:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:00:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:00:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:01:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.922 | ppl 15.16 | bleu 20.66 | wps 2435.1 | wpb 2024.1 | bsz 97.5 | num_updates 9000 | best_bleu 20.66
2021-05-01 13:01:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 9000 updates
2021-05-01 13:01:04 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_9000.pt
2021-05-01 13:01:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_9000.pt
2021-05-01 13:01:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_9000.pt (epoch 1 @ 9000 updates, score 20.66) (writing took 14.340807110995229 seconds)
2021-05-01 13:01:41 | INFO | train_inner | epoch 001:   9105 / 60421 loss=2.413, ppl=5.33, wps=3628.8, ups=0.97, wpb=3728.5, bsz=138, num_updates=9100, lr=0.000331497, gnorm=2.177, loss_scale=4, train_wall=23, gb_free=11, wall=2782
2021-05-01 13:02:04 | INFO | train_inner | epoch 001:   9205 / 60421 loss=2.217, ppl=4.65, wps=16410.1, ups=4.31, wpb=3809.9, bsz=139.7, num_updates=9200, lr=0.00032969, gnorm=1.767, loss_scale=4, train_wall=23, gb_free=11, wall=2805
2021-05-01 13:02:27 | INFO | train_inner | epoch 001:   9305 / 60421 loss=2.095, ppl=4.27, wps=16475.2, ups=4.37, wpb=3769.3, bsz=140.2, num_updates=9300, lr=0.000327913, gnorm=1.788, loss_scale=4, train_wall=23, gb_free=11.2, wall=2828
2021-05-01 13:02:50 | INFO | train_inner | epoch 001:   9405 / 60421 loss=2.127, ppl=4.37, wps=16666.9, ups=4.41, wpb=3782.7, bsz=147.2, num_updates=9400, lr=0.000326164, gnorm=1.986, loss_scale=4, train_wall=23, gb_free=10.9, wall=2851
2021-05-01 13:03:13 | INFO | train_inner | epoch 001:   9505 / 60421 loss=2.302, ppl=4.93, wps=16472.6, ups=4.41, wpb=3733.8, bsz=130.1, num_updates=9500, lr=0.000324443, gnorm=2.606, loss_scale=4, train_wall=22, gb_free=10.8, wall=2874
2021-05-01 13:03:35 | INFO | train_inner | epoch 001:   9605 / 60421 loss=2.296, ppl=4.91, wps=16549.8, ups=4.41, wpb=3748.8, bsz=133, num_updates=9600, lr=0.000322749, gnorm=2.175, loss_scale=4, train_wall=22, gb_free=10.6, wall=2896
2021-05-01 13:03:58 | INFO | train_inner | epoch 001:   9705 / 60421 loss=2.045, ppl=4.13, wps=16775.3, ups=4.36, wpb=3846.3, bsz=115.6, num_updates=9700, lr=0.000321081, gnorm=1.705, loss_scale=4, train_wall=23, gb_free=10.7, wall=2919
2021-05-01 13:04:21 | INFO | train_inner | epoch 001:   9805 / 60421 loss=2.179, ppl=4.53, wps=16564.5, ups=4.38, wpb=3778.7, bsz=124.8, num_updates=9800, lr=0.000319438, gnorm=2.159, loss_scale=4, train_wall=23, gb_free=10.8, wall=2942
2021-05-01 13:04:44 | INFO | train_inner | epoch 001:   9905 / 60421 loss=2.086, ppl=4.25, wps=16464.5, ups=4.41, wpb=3736.2, bsz=113.2, num_updates=9900, lr=0.000317821, gnorm=1.838, loss_scale=4, train_wall=23, gb_free=11.1, wall=2965
2021-05-01 13:05:07 | INFO | train_inner | epoch 001:  10005 / 60421 loss=2.185, ppl=4.55, wps=16373, ups=4.37, wpb=3749.9, bsz=137.3, num_updates=10000, lr=0.000316228, gnorm=1.954, loss_scale=4, train_wall=23, gb_free=10.8, wall=2988
2021-05-01 13:05:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:05:07 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:05:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:05:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:05:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:05:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:05:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:05:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:05:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:05:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:05:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:05:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:05:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:05:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:05:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:05:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:05:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:06:14 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.86 | ppl 14.52 | bleu 22.02 | wps 2338.8 | wpb 2024.1 | bsz 97.5 | num_updates 10000 | best_bleu 22.02
2021-05-01 13:06:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 10000 updates
2021-05-01 13:06:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_10000.pt
2021-05-01 13:06:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_10000.pt
2021-05-01 13:06:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 22.02) (writing took 13.674830952993943 seconds)
2021-05-01 13:06:52 | INFO | train_inner | epoch 001:  10105 / 60421 loss=2.281, ppl=4.86, wps=3559.5, ups=0.95, wpb=3731.6, bsz=141.4, num_updates=10100, lr=0.000314658, gnorm=2.348, loss_scale=4, train_wall=23, gb_free=10.7, wall=3093
2021-05-01 13:07:14 | INFO | train_inner | epoch 001:  10205 / 60421 loss=2.174, ppl=4.51, wps=16204.9, ups=4.37, wpb=3704.6, bsz=143.4, num_updates=10200, lr=0.000313112, gnorm=1.987, loss_scale=4, train_wall=23, gb_free=10.9, wall=3115
2021-05-01 13:07:37 | INFO | train_inner | epoch 001:  10305 / 60421 loss=2.162, ppl=4.47, wps=16565.2, ups=4.43, wpb=3738.5, bsz=132.6, num_updates=10300, lr=0.000311588, gnorm=2.302, loss_scale=4, train_wall=22, gb_free=10.7, wall=3138
2021-05-01 13:08:00 | INFO | train_inner | epoch 001:  10405 / 60421 loss=2.196, ppl=4.58, wps=16663.9, ups=4.44, wpb=3756.1, bsz=142.7, num_updates=10400, lr=0.000310087, gnorm=2.18, loss_scale=4, train_wall=22, gb_free=10.9, wall=3160
2021-05-01 13:08:22 | INFO | train_inner | epoch 001:  10505 / 60421 loss=2.249, ppl=4.75, wps=16466.8, ups=4.38, wpb=3755.4, bsz=136.1, num_updates=10500, lr=0.000308607, gnorm=2.3, loss_scale=4, train_wall=23, gb_free=11, wall=3183
2021-05-01 13:08:45 | INFO | train_inner | epoch 001:  10605 / 60421 loss=2.123, ppl=4.36, wps=16521.3, ups=4.43, wpb=3732.1, bsz=143.4, num_updates=10600, lr=0.000307148, gnorm=2.095, loss_scale=4, train_wall=22, gb_free=10.9, wall=3206
2021-05-01 13:09:08 | INFO | train_inner | epoch 001:  10705 / 60421 loss=2.187, ppl=4.55, wps=16519.3, ups=4.39, wpb=3765.9, bsz=153.8, num_updates=10700, lr=0.000305709, gnorm=2.107, loss_scale=4, train_wall=23, gb_free=10.9, wall=3229
2021-05-01 13:09:30 | INFO | train_inner | epoch 001:  10805 / 60421 loss=2.18, ppl=4.53, wps=16391.2, ups=4.4, wpb=3726.6, bsz=123.3, num_updates=10800, lr=0.00030429, gnorm=2.113, loss_scale=4, train_wall=23, gb_free=10.6, wall=3251
2021-05-01 13:09:53 | INFO | train_inner | epoch 001:  10905 / 60421 loss=2.197, ppl=4.59, wps=16487.3, ups=4.39, wpb=3757.9, bsz=132.9, num_updates=10900, lr=0.000302891, gnorm=2.066, loss_scale=4, train_wall=23, gb_free=10.8, wall=3274
2021-05-01 13:10:16 | INFO | train_inner | epoch 001:  11005 / 60421 loss=2.102, ppl=4.29, wps=16287.6, ups=4.36, wpb=3736.2, bsz=155.8, num_updates=11000, lr=0.000301511, gnorm=2.151, loss_scale=4, train_wall=23, gb_free=10.7, wall=3297
2021-05-01 13:10:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:10:16 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:10:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:10:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:10:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:10:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:10:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:10:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:10:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:10:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:10:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:10:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:10:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:10:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:11:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.891 | ppl 14.83 | bleu 22.19 | wps 2435 | wpb 2024.1 | bsz 97.5 | num_updates 11000 | best_bleu 22.19
2021-05-01 13:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 11000 updates
2021-05-01 13:11:21 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_11000.pt
2021-05-01 13:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_11000.pt
2021-05-01 13:11:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_11000.pt (epoch 1 @ 11000 updates, score 22.19) (writing took 14.354053946997738 seconds)
2021-05-01 13:11:59 | INFO | train_inner | epoch 001:  11105 / 60421 loss=2.153, ppl=4.45, wps=3630.7, ups=0.97, wpb=3729, bsz=133.4, num_updates=11100, lr=0.00030015, gnorm=2.161, loss_scale=4, train_wall=23, gb_free=10.8, wall=3400
2021-05-01 13:12:22 | INFO | train_inner | epoch 001:  11205 / 60421 loss=2.149, ppl=4.44, wps=16591.2, ups=4.42, wpb=3757.7, bsz=113.1, num_updates=11200, lr=0.000298807, gnorm=2.099, loss_scale=4, train_wall=22, gb_free=11, wall=3423
2021-05-01 13:12:44 | INFO | train_inner | epoch 001:  11305 / 60421 loss=2.193, ppl=4.57, wps=16671.1, ups=4.38, wpb=3808.6, bsz=139.3, num_updates=11300, lr=0.000297482, gnorm=2.123, loss_scale=4, train_wall=23, gb_free=10.7, wall=3445
2021-05-01 13:13:07 | INFO | train_inner | epoch 001:  11405 / 60421 loss=2.107, ppl=4.31, wps=16476.7, ups=4.43, wpb=3720, bsz=127.5, num_updates=11400, lr=0.000296174, gnorm=2.052, loss_scale=4, train_wall=22, gb_free=10.8, wall=3468
2021-05-01 13:13:30 | INFO | train_inner | epoch 001:  11505 / 60421 loss=2.139, ppl=4.41, wps=16631.7, ups=4.42, wpb=3759.7, bsz=120.5, num_updates=11500, lr=0.000294884, gnorm=2.028, loss_scale=4, train_wall=22, gb_free=10.7, wall=3491
2021-05-01 13:13:52 | INFO | train_inner | epoch 001:  11605 / 60421 loss=2.105, ppl=4.3, wps=16321.2, ups=4.39, wpb=3718.8, bsz=143, num_updates=11600, lr=0.00029361, gnorm=2.232, loss_scale=4, train_wall=23, gb_free=10.9, wall=3513
2021-05-01 13:14:15 | INFO | train_inner | epoch 001:  11705 / 60421 loss=2.036, ppl=4.1, wps=16356.3, ups=4.41, wpb=3712.5, bsz=131.8, num_updates=11700, lr=0.000292353, gnorm=2.121, loss_scale=4, train_wall=23, gb_free=10.6, wall=3536
2021-05-01 13:14:38 | INFO | train_inner | epoch 001:  11805 / 60421 loss=2.153, ppl=4.45, wps=16507.6, ups=4.38, wpb=3765.4, bsz=142.6, num_updates=11800, lr=0.000291111, gnorm=1.995, loss_scale=4, train_wall=23, gb_free=10.9, wall=3559
2021-05-01 13:15:01 | INFO | train_inner | epoch 001:  11905 / 60421 loss=2.124, ppl=4.36, wps=16470, ups=4.37, wpb=3764.7, bsz=132.5, num_updates=11900, lr=0.000289886, gnorm=2.206, loss_scale=4, train_wall=23, gb_free=10.8, wall=3582
2021-05-01 13:15:24 | INFO | train_inner | epoch 001:  12005 / 60421 loss=2.13, ppl=4.38, wps=16308.4, ups=4.36, wpb=3742.3, bsz=135, num_updates=12000, lr=0.000288675, gnorm=2.036, loss_scale=4, train_wall=23, gb_free=10.9, wall=3605
2021-05-01 13:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:15:24 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:15:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:15:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:15:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:15:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:15:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:15:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:15:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:15:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:15:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:15:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:15:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:15:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:15:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:15:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:15:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:16:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.815 | ppl 14.07 | bleu 22.72 | wps 2410.1 | wpb 2024.1 | bsz 97.5 | num_updates 12000 | best_bleu 22.72
2021-05-01 13:16:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 12000 updates
2021-05-01 13:16:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_12000.pt
2021-05-01 13:16:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_12000.pt
2021-05-01 13:16:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 22.72) (writing took 14.394357648998266 seconds)
2021-05-01 13:17:07 | INFO | train_inner | epoch 001:  12105 / 60421 loss=2.064, ppl=4.18, wps=3641.6, ups=0.97, wpb=3770, bsz=147.3, num_updates=12100, lr=0.00028748, gnorm=1.992, loss_scale=4, train_wall=23, gb_free=10.8, wall=3708
2021-05-01 13:17:30 | INFO | train_inner | epoch 001:  12205 / 60421 loss=2.128, ppl=4.37, wps=16353.3, ups=4.42, wpb=3697.8, bsz=120.2, num_updates=12200, lr=0.000286299, gnorm=2.013, loss_scale=4, train_wall=22, gb_free=10.7, wall=3731
2021-05-01 13:17:52 | INFO | train_inner | epoch 001:  12305 / 60421 loss=2.132, ppl=4.38, wps=16535.5, ups=4.42, wpb=3742.5, bsz=125.4, num_updates=12300, lr=0.000285133, gnorm=2.124, loss_scale=4, train_wall=22, gb_free=10.8, wall=3753
2021-05-01 13:18:15 | INFO | train_inner | epoch 001:  12405 / 60421 loss=2.246, ppl=4.74, wps=16539.3, ups=4.47, wpb=3701.8, bsz=129.7, num_updates=12400, lr=0.000283981, gnorm=2.223, loss_scale=4, train_wall=22, gb_free=11.5, wall=3776
2021-05-01 13:18:37 | INFO | train_inner | epoch 001:  12505 / 60421 loss=2.094, ppl=4.27, wps=16455.9, ups=4.45, wpb=3696.8, bsz=120.8, num_updates=12500, lr=0.000282843, gnorm=2.194, loss_scale=4, train_wall=22, gb_free=11, wall=3798
2021-05-01 13:19:00 | INFO | train_inner | epoch 001:  12605 / 60421 loss=2.09, ppl=4.26, wps=16463.9, ups=4.44, wpb=3712.2, bsz=113.5, num_updates=12600, lr=0.000281718, gnorm=2.031, loss_scale=4, train_wall=22, gb_free=10.8, wall=3821
2021-05-01 13:19:22 | INFO | train_inner | epoch 001:  12705 / 60421 loss=2.087, ppl=4.25, wps=16417.4, ups=4.43, wpb=3703.8, bsz=123.2, num_updates=12700, lr=0.000280607, gnorm=2.279, loss_scale=4, train_wall=22, gb_free=11.1, wall=3843
2021-05-01 13:19:45 | INFO | train_inner | epoch 001:  12805 / 60421 loss=2.1, ppl=4.29, wps=16353.5, ups=4.4, wpb=3720.2, bsz=136.6, num_updates=12800, lr=0.000279508, gnorm=1.967, loss_scale=4, train_wall=23, gb_free=10.9, wall=3866
2021-05-01 13:20:08 | INFO | train_inner | epoch 001:  12905 / 60421 loss=2.067, ppl=4.19, wps=16416.7, ups=4.4, wpb=3731.8, bsz=141.9, num_updates=12900, lr=0.000278423, gnorm=2.081, loss_scale=4, train_wall=23, gb_free=10.9, wall=3889
2021-05-01 13:20:30 | INFO | train_inner | epoch 001:  13005 / 60421 loss=2.164, ppl=4.48, wps=16089.9, ups=4.42, wpb=3639.8, bsz=134, num_updates=13000, lr=0.00027735, gnorm=2.389, loss_scale=4, train_wall=22, gb_free=12.2, wall=3911
2021-05-01 13:20:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:20:31 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:20:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:20:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:20:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:20:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:20:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:20:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:20:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:20:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:20:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:21:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.84 | ppl 14.32 | bleu 22.46 | wps 2420.9 | wpb 2024.1 | bsz 97.5 | num_updates 13000 | best_bleu 22.72
2021-05-01 13:21:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 13000 updates
2021-05-01 13:21:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_13000.pt
2021-05-01 13:21:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_13000.pt
2021-05-01 13:21:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_13000.pt (epoch 1 @ 13000 updates, score 22.46) (writing took 8.153087353995943 seconds)
2021-05-01 13:22:07 | INFO | train_inner | epoch 001:  13105 / 60421 loss=2.103, ppl=4.3, wps=3755.8, ups=1.04, wpb=3622.2, bsz=133.3, num_updates=13100, lr=0.000276289, gnorm=2.136, loss_scale=4, train_wall=23, gb_free=10.8, wall=4008
2021-05-01 13:22:30 | INFO | train_inner | epoch 001:  13205 / 60421 loss=2.051, ppl=4.14, wps=16530.7, ups=4.41, wpb=3748.4, bsz=131.6, num_updates=13200, lr=0.000275241, gnorm=1.916, loss_scale=4, train_wall=22, gb_free=11.2, wall=4031
2021-05-01 13:22:53 | INFO | train_inner | epoch 001:  13305 / 60421 loss=1.977, ppl=3.94, wps=16527.7, ups=4.37, wpb=3783.9, bsz=131, num_updates=13300, lr=0.000274204, gnorm=1.716, loss_scale=4, train_wall=23, gb_free=11, wall=4053
2021-05-01 13:23:15 | INFO | train_inner | epoch 001:  13405 / 60421 loss=2.06, ppl=4.17, wps=16689.2, ups=4.41, wpb=3788.5, bsz=127.6, num_updates=13400, lr=0.000273179, gnorm=1.907, loss_scale=4, train_wall=23, gb_free=10.8, wall=4076
2021-05-01 13:23:38 | INFO | train_inner | epoch 001:  13505 / 60421 loss=2.09, ppl=4.26, wps=16465.8, ups=4.43, wpb=3720.4, bsz=125.1, num_updates=13500, lr=0.000272166, gnorm=1.991, loss_scale=4, train_wall=22, gb_free=10.7, wall=4099
2021-05-01 13:24:01 | INFO | train_inner | epoch 001:  13605 / 60421 loss=2.177, ppl=4.52, wps=16462.5, ups=4.41, wpb=3736.2, bsz=126.6, num_updates=13600, lr=0.000271163, gnorm=2.148, loss_scale=4, train_wall=23, gb_free=10.7, wall=4121
2021-05-01 13:24:23 | INFO | train_inner | epoch 001:  13705 / 60421 loss=2.072, ppl=4.2, wps=16677.2, ups=4.38, wpb=3810, bsz=134.4, num_updates=13700, lr=0.000270172, gnorm=1.894, loss_scale=4, train_wall=23, gb_free=10.5, wall=4144
2021-05-01 13:24:46 | INFO | train_inner | epoch 001:  13805 / 60421 loss=1.974, ppl=3.93, wps=16581.1, ups=4.35, wpb=3812.7, bsz=131.8, num_updates=13800, lr=0.000269191, gnorm=1.782, loss_scale=4, train_wall=23, gb_free=10.8, wall=4167
2021-05-01 13:25:09 | INFO | train_inner | epoch 001:  13905 / 60421 loss=1.996, ppl=3.99, wps=16339.8, ups=4.37, wpb=3740.1, bsz=153.5, num_updates=13900, lr=0.000268221, gnorm=1.971, loss_scale=4, train_wall=23, gb_free=11.1, wall=4190
2021-05-01 13:25:32 | INFO | train_inner | epoch 001:  14005 / 60421 loss=1.92, ppl=3.78, wps=16455.5, ups=4.31, wpb=3818.7, bsz=136.2, num_updates=14000, lr=0.000267261, gnorm=1.786, loss_scale=4, train_wall=23, gb_free=10.9, wall=4213
2021-05-01 13:25:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:25:32 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:25:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:25:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:25:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:25:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:25:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:25:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:26:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.814 | ppl 14.07 | bleu 23.51 | wps 2327.1 | wpb 2024.1 | bsz 97.5 | num_updates 14000 | best_bleu 23.51
2021-05-01 13:26:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 14000 updates
2021-05-01 13:26:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_14000.pt
2021-05-01 13:26:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_14000.pt
2021-05-01 13:26:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_14000.pt (epoch 1 @ 14000 updates, score 23.51) (writing took 14.09552133899706 seconds)
2021-05-01 13:27:18 | INFO | train_inner | epoch 001:  14105 / 60421 loss=1.965, ppl=3.9, wps=3541.1, ups=0.95, wpb=3724.8, bsz=143.4, num_updates=14100, lr=0.000266312, gnorm=1.939, loss_scale=4, train_wall=23, gb_free=10.7, wall=4319
2021-05-01 13:27:40 | INFO | train_inner | epoch 001:  14205 / 60421 loss=2.032, ppl=4.09, wps=16483.1, ups=4.45, wpb=3702.3, bsz=126.6, num_updates=14200, lr=0.000265372, gnorm=2.238, loss_scale=4, train_wall=22, gb_free=10.8, wall=4341
2021-05-01 13:28:03 | INFO | train_inner | epoch 001:  14305 / 60421 loss=2.022, ppl=4.06, wps=16433.9, ups=4.42, wpb=3716.2, bsz=125.2, num_updates=14300, lr=0.000264443, gnorm=2.124, loss_scale=4, train_wall=22, gb_free=10.8, wall=4364
2021-05-01 13:28:25 | INFO | train_inner | epoch 001:  14405 / 60421 loss=2.098, ppl=4.28, wps=16344.3, ups=4.41, wpb=3709.2, bsz=124.2, num_updates=14400, lr=0.000263523, gnorm=2.008, loss_scale=4, train_wall=23, gb_free=11, wall=4386
2021-05-01 13:28:48 | INFO | train_inner | epoch 001:  14505 / 60421 loss=2.077, ppl=4.22, wps=16479.8, ups=4.39, wpb=3752.4, bsz=139.6, num_updates=14500, lr=0.000262613, gnorm=2.161, loss_scale=4, train_wall=23, gb_free=10.9, wall=4409
2021-05-01 13:29:11 | INFO | train_inner | epoch 001:  14605 / 60421 loss=2.013, ppl=4.04, wps=16546.2, ups=4.38, wpb=3781, bsz=137.3, num_updates=14600, lr=0.000261712, gnorm=1.822, loss_scale=4, train_wall=23, gb_free=10.8, wall=4432
2021-05-01 13:29:34 | INFO | train_inner | epoch 001:  14705 / 60421 loss=2.009, ppl=4.03, wps=16434.2, ups=4.38, wpb=3748.9, bsz=121.8, num_updates=14700, lr=0.00026082, gnorm=1.955, loss_scale=4, train_wall=23, gb_free=11, wall=4455
2021-05-01 13:29:57 | INFO | train_inner | epoch 001:  14805 / 60421 loss=2.103, ppl=4.3, wps=16389.8, ups=4.4, wpb=3727.1, bsz=127.6, num_updates=14800, lr=0.000259938, gnorm=2.215, loss_scale=4, train_wall=23, gb_free=11.7, wall=4478
2021-05-01 13:30:20 | INFO | train_inner | epoch 001:  14905 / 60421 loss=2.043, ppl=4.12, wps=16204.3, ups=4.34, wpb=3734.4, bsz=160.2, num_updates=14900, lr=0.000259064, gnorm=2.14, loss_scale=4, train_wall=23, gb_free=11.1, wall=4501
2021-05-01 13:30:42 | INFO | train_inner | epoch 001:  15005 / 60421 loss=1.972, ppl=3.92, wps=16200.8, ups=4.38, wpb=3699.6, bsz=123.9, num_updates=15000, lr=0.000258199, gnorm=2.162, loss_scale=4, train_wall=23, gb_free=10.9, wall=4523
2021-05-01 13:30:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:30:42 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:30:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:30:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:30:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:30:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:30:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:30:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:30:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:30:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:30:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:31:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:31:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:31:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:31:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:31:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:31:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:31:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.861 | ppl 14.53 | bleu 22.88 | wps 2280.7 | wpb 2024.1 | bsz 97.5 | num_updates 15000 | best_bleu 23.51
2021-05-01 13:31:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 15000 updates
2021-05-01 13:31:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_15000.pt
2021-05-01 13:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_15000.pt
2021-05-01 13:32:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_15000.pt (epoch 1 @ 15000 updates, score 22.88) (writing took 7.675982103995921 seconds)
2021-05-01 13:32:23 | INFO | train_inner | epoch 001:  15105 / 60421 loss=2, ppl=4, wps=3737, ups=1, wpb=3741.3, bsz=145.4, num_updates=15100, lr=0.000257343, gnorm=1.871, loss_scale=4, train_wall=23, gb_free=11, wall=4624
2021-05-01 13:32:45 | INFO | train_inner | epoch 001:  15205 / 60421 loss=2.101, ppl=4.29, wps=16579.6, ups=4.41, wpb=3756.9, bsz=138.3, num_updates=15200, lr=0.000256495, gnorm=1.947, loss_scale=4, train_wall=22, gb_free=10.7, wall=4646
2021-05-01 13:33:08 | INFO | train_inner | epoch 001:  15305 / 60421 loss=1.936, ppl=3.83, wps=16736.4, ups=4.39, wpb=3809.9, bsz=152, num_updates=15300, lr=0.000255655, gnorm=1.786, loss_scale=4, train_wall=23, gb_free=10.9, wall=4669
2021-05-01 13:33:31 | INFO | train_inner | epoch 001:  15405 / 60421 loss=1.895, ppl=3.72, wps=16454.2, ups=4.38, wpb=3752.5, bsz=136, num_updates=15400, lr=0.000254824, gnorm=1.74, loss_scale=4, train_wall=23, gb_free=10.8, wall=4692
2021-05-01 13:33:53 | INFO | train_inner | epoch 001:  15505 / 60421 loss=1.882, ppl=3.68, wps=16361.1, ups=4.42, wpb=3699.7, bsz=147.2, num_updates=15500, lr=0.000254, gnorm=1.776, loss_scale=4, train_wall=22, gb_free=10.8, wall=4714
2021-05-01 13:34:16 | INFO | train_inner | epoch 001:  15605 / 60421 loss=1.856, ppl=3.62, wps=16509.1, ups=4.43, wpb=3724.4, bsz=115.9, num_updates=15600, lr=0.000253185, gnorm=1.691, loss_scale=4, train_wall=22, gb_free=10.8, wall=4737
2021-05-01 13:34:39 | INFO | train_inner | epoch 001:  15705 / 60421 loss=1.918, ppl=3.78, wps=16407.5, ups=4.43, wpb=3701.6, bsz=135.8, num_updates=15700, lr=0.000252377, gnorm=1.873, loss_scale=4, train_wall=22, gb_free=10.9, wall=4759
2021-05-01 13:35:01 | INFO | train_inner | epoch 001:  15805 / 60421 loss=1.899, ppl=3.73, wps=16292, ups=4.4, wpb=3701.3, bsz=134.8, num_updates=15800, lr=0.000251577, gnorm=1.749, loss_scale=4, train_wall=23, gb_free=11, wall=4782
2021-05-01 13:35:24 | INFO | train_inner | epoch 001:  15905 / 60421 loss=1.968, ppl=3.91, wps=16003.8, ups=4.35, wpb=3682.3, bsz=158.8, num_updates=15900, lr=0.000250785, gnorm=2.483, loss_scale=4, train_wall=23, gb_free=10.8, wall=4805
2021-05-01 13:35:47 | INFO | train_inner | epoch 001:  16005 / 60421 loss=1.952, ppl=3.87, wps=16247.6, ups=4.34, wpb=3747.1, bsz=120.8, num_updates=16000, lr=0.00025, gnorm=1.848, loss_scale=4, train_wall=23, gb_free=10.8, wall=4828
2021-05-01 13:35:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:35:47 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:36:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:36:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:36:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:36:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:36:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:36:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:36:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.774 | ppl 13.68 | bleu 23.77 | wps 2390.9 | wpb 2024.1 | bsz 97.5 | num_updates 16000 | best_bleu 23.77
2021-05-01 13:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 16000 updates
2021-05-01 13:36:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_16000.pt
2021-05-01 13:36:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_16000.pt
2021-05-01 13:37:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_16000.pt (epoch 1 @ 16000 updates, score 23.77) (writing took 14.354020047998347 seconds)
2021-05-01 13:37:31 | INFO | train_inner | epoch 001:  16105 / 60421 loss=1.895, ppl=3.72, wps=3616.2, ups=0.96, wpb=3751.1, bsz=145.6, num_updates=16100, lr=0.000249222, gnorm=1.985, loss_scale=4, train_wall=23, gb_free=10.8, wall=4932
2021-05-01 13:37:54 | INFO | train_inner | epoch 001:  16205 / 60421 loss=1.816, ppl=3.52, wps=16552.5, ups=4.42, wpb=3748.6, bsz=131.2, num_updates=16200, lr=0.000248452, gnorm=1.765, loss_scale=4, train_wall=22, gb_free=10.9, wall=4955
2021-05-01 13:38:16 | INFO | train_inner | epoch 001:  16305 / 60421 loss=1.979, ppl=3.94, wps=16432.9, ups=4.42, wpb=3718.1, bsz=158.2, num_updates=16300, lr=0.000247689, gnorm=2.254, loss_scale=4, train_wall=22, gb_free=12.2, wall=4977
2021-05-01 13:38:39 | INFO | train_inner | epoch 001:  16405 / 60421 loss=1.937, ppl=3.83, wps=16688.5, ups=4.37, wpb=3818.7, bsz=126.6, num_updates=16400, lr=0.000246932, gnorm=1.893, loss_scale=4, train_wall=23, gb_free=10.9, wall=5000
2021-05-01 13:39:02 | INFO | train_inner | epoch 001:  16505 / 60421 loss=2.076, ppl=4.22, wps=16511.3, ups=4.41, wpb=3742.7, bsz=131, num_updates=16500, lr=0.000246183, gnorm=2.157, loss_scale=4, train_wall=22, gb_free=10.8, wall=5023
2021-05-01 13:39:24 | INFO | train_inner | epoch 001:  16605 / 60421 loss=1.949, ppl=3.86, wps=16205.6, ups=4.46, wpb=3632.9, bsz=139.5, num_updates=16600, lr=0.00024544, gnorm=2.053, loss_scale=4, train_wall=22, gb_free=10.7, wall=5045
2021-05-01 13:39:47 | INFO | train_inner | epoch 001:  16705 / 60421 loss=1.957, ppl=3.88, wps=16458.6, ups=4.38, wpb=3761.7, bsz=124.9, num_updates=16700, lr=0.000244704, gnorm=1.764, loss_scale=4, train_wall=23, gb_free=10.7, wall=5068
2021-05-01 13:40:09 | INFO | train_inner | epoch 001:  16805 / 60421 loss=2.023, ppl=4.06, wps=16239.9, ups=4.5, wpb=3607.9, bsz=120.9, num_updates=16800, lr=0.000243975, gnorm=2.512, loss_scale=8, train_wall=22, gb_free=10.8, wall=5090
2021-05-01 13:40:32 | INFO | train_inner | epoch 001:  16905 / 60421 loss=1.991, ppl=3.98, wps=16292.3, ups=4.41, wpb=3695.1, bsz=123.6, num_updates=16900, lr=0.000243252, gnorm=2.014, loss_scale=8, train_wall=23, gb_free=10.8, wall=5113
2021-05-01 13:40:55 | INFO | train_inner | epoch 001:  17005 / 60421 loss=2.068, ppl=4.19, wps=15882.1, ups=4.39, wpb=3615.7, bsz=120.6, num_updates=17000, lr=0.000242536, gnorm=2.395, loss_scale=8, train_wall=23, gb_free=10.8, wall=5136
2021-05-01 13:40:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:40:55 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:41:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:41:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:41:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:41:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:41:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:41:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:41:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:41:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:41:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:41:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:41:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:41:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:41:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:41:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:41:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:42:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.859 | ppl 14.51 | bleu 23.14 | wps 2314.5 | wpb 2024.1 | bsz 97.5 | num_updates 17000 | best_bleu 23.77
2021-05-01 13:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 17000 updates
2021-05-01 13:42:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_17000.pt
2021-05-01 13:42:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_17000.pt
2021-05-01 13:42:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_17000.pt (epoch 1 @ 17000 updates, score 23.14) (writing took 7.719090195998433 seconds)
2021-05-01 13:42:34 | INFO | train_inner | epoch 001:  17105 / 60421 loss=1.897, ppl=3.72, wps=3812.5, ups=1.01, wpb=3781.9, bsz=133, num_updates=17100, lr=0.000241825, gnorm=1.701, loss_scale=8, train_wall=23, gb_free=10.7, wall=5235
2021-05-01 13:42:56 | INFO | train_inner | epoch 001:  17205 / 60421 loss=2.017, ppl=4.05, wps=16423.4, ups=4.46, wpb=3685.6, bsz=120.9, num_updates=17200, lr=0.000241121, gnorm=2.222, loss_scale=8, train_wall=22, gb_free=11.5, wall=5257
2021-05-01 13:43:19 | INFO | train_inner | epoch 001:  17305 / 60421 loss=1.861, ppl=3.63, wps=16551.3, ups=4.4, wpb=3758.2, bsz=130.1, num_updates=17300, lr=0.000240424, gnorm=2.005, loss_scale=8, train_wall=23, gb_free=10.8, wall=5280
2021-05-01 13:43:42 | INFO | train_inner | epoch 001:  17405 / 60421 loss=2, ppl=4, wps=16285.2, ups=4.43, wpb=3673.2, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=2.333, loss_scale=8, train_wall=22, gb_free=11, wall=5303
2021-05-01 13:44:05 | INFO | train_inner | epoch 001:  17505 / 60421 loss=1.888, ppl=3.7, wps=16403.3, ups=4.38, wpb=3741.1, bsz=127.5, num_updates=17500, lr=0.000239046, gnorm=1.954, loss_scale=8, train_wall=23, gb_free=10.8, wall=5325
2021-05-01 13:44:27 | INFO | train_inner | epoch 001:  17605 / 60421 loss=1.886, ppl=3.7, wps=16550.6, ups=4.42, wpb=3748.3, bsz=133.4, num_updates=17600, lr=0.000238366, gnorm=1.691, loss_scale=8, train_wall=22, gb_free=10.8, wall=5348
2021-05-01 13:44:50 | INFO | train_inner | epoch 001:  17705 / 60421 loss=1.966, ppl=3.91, wps=16267.3, ups=4.41, wpb=3691.7, bsz=151, num_updates=17700, lr=0.000237691, gnorm=2.068, loss_scale=8, train_wall=23, gb_free=10.7, wall=5371
2021-05-01 13:45:13 | INFO | train_inner | epoch 001:  17805 / 60421 loss=1.829, ppl=3.55, wps=16350, ups=4.39, wpb=3722.7, bsz=117, num_updates=17800, lr=0.000237023, gnorm=1.778, loss_scale=8, train_wall=23, gb_free=10.9, wall=5394
2021-05-01 13:45:35 | INFO | train_inner | epoch 001:  17905 / 60421 loss=1.791, ppl=3.46, wps=16395.9, ups=4.39, wpb=3733.4, bsz=140.6, num_updates=17900, lr=0.00023636, gnorm=1.72, loss_scale=8, train_wall=23, gb_free=10.8, wall=5416
2021-05-01 13:45:59 | INFO | train_inner | epoch 001:  18005 / 60421 loss=1.918, ppl=3.78, wps=16168.8, ups=4.31, wpb=3754.8, bsz=136.8, num_updates=18000, lr=0.000235702, gnorm=1.832, loss_scale=8, train_wall=23, gb_free=10.8, wall=5440
2021-05-01 13:45:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:45:59 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:46:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:46:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:46:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:46:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:46:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:46:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:46:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:46:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:46:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:46:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:46:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:46:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:47:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.774 | ppl 13.68 | bleu 24.24 | wps 2388.5 | wpb 2024.1 | bsz 97.5 | num_updates 18000 | best_bleu 24.24
2021-05-01 13:47:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 18000 updates
2021-05-01 13:47:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_18000.pt
2021-05-01 13:47:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_18000.pt
2021-05-01 13:47:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_18000.pt (epoch 1 @ 18000 updates, score 24.24) (writing took 14.362044857 seconds)
2021-05-01 13:47:42 | INFO | train_inner | epoch 001:  18105 / 60421 loss=1.849, ppl=3.6, wps=3590.6, ups=0.97, wpb=3715.2, bsz=146.2, num_updates=18100, lr=0.00023505, gnorm=1.985, loss_scale=8, train_wall=22, gb_free=10.8, wall=5543
2021-05-01 13:48:05 | INFO | train_inner | epoch 001:  18205 / 60421 loss=1.897, ppl=3.72, wps=16572, ups=4.42, wpb=3749.7, bsz=131, num_updates=18200, lr=0.000234404, gnorm=1.883, loss_scale=8, train_wall=22, gb_free=10.8, wall=5566
2021-05-01 13:48:27 | INFO | train_inner | epoch 001:  18305 / 60421 loss=1.905, ppl=3.75, wps=16580.8, ups=4.39, wpb=3775.7, bsz=142.4, num_updates=18300, lr=0.000233762, gnorm=1.808, loss_scale=8, train_wall=23, gb_free=10.8, wall=5588
2021-05-01 13:48:50 | INFO | train_inner | epoch 001:  18405 / 60421 loss=1.909, ppl=3.76, wps=16405.8, ups=4.41, wpb=3721.5, bsz=120.9, num_updates=18400, lr=0.000233126, gnorm=2.026, loss_scale=8, train_wall=23, gb_free=10.6, wall=5611
2021-05-01 13:49:13 | INFO | train_inner | epoch 001:  18505 / 60421 loss=1.742, ppl=3.35, wps=16576.5, ups=4.38, wpb=3784.9, bsz=136.2, num_updates=18500, lr=0.000232495, gnorm=1.581, loss_scale=8, train_wall=23, gb_free=11, wall=5634
2021-05-01 13:49:36 | INFO | train_inner | epoch 001:  18605 / 60421 loss=1.821, ppl=3.53, wps=16400.2, ups=4.4, wpb=3728.7, bsz=153, num_updates=18600, lr=0.000231869, gnorm=1.767, loss_scale=8, train_wall=23, gb_free=11.3, wall=5657
2021-05-01 13:49:59 | INFO | train_inner | epoch 001:  18705 / 60421 loss=1.811, ppl=3.51, wps=16477.1, ups=4.37, wpb=3766.8, bsz=121.6, num_updates=18700, lr=0.000231249, gnorm=1.643, loss_scale=8, train_wall=23, gb_free=10.7, wall=5680
2021-05-01 13:50:21 | INFO | train_inner | epoch 001:  18805 / 60421 loss=1.837, ppl=3.57, wps=16278.1, ups=4.38, wpb=3714.8, bsz=126.9, num_updates=18800, lr=0.000230633, gnorm=1.713, loss_scale=8, train_wall=23, gb_free=11, wall=5702
2021-05-01 13:50:44 | INFO | train_inner | epoch 001:  18905 / 60421 loss=1.806, ppl=3.5, wps=16227.4, ups=4.36, wpb=3721.7, bsz=124.6, num_updates=18900, lr=0.000230022, gnorm=1.928, loss_scale=8, train_wall=23, gb_free=10.8, wall=5725
2021-05-01 13:51:08 | INFO | train_inner | epoch 001:  19005 / 60421 loss=1.751, ppl=3.36, wps=16271.9, ups=4.31, wpb=3775.8, bsz=136.6, num_updates=19000, lr=0.000229416, gnorm=1.777, loss_scale=8, train_wall=23, gb_free=10.8, wall=5749
2021-05-01 13:51:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:51:08 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:51:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:51:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:51:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:51:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:51:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:51:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:51:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:51:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:51:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:51:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:51:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:51:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:51:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:51:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:51:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:52:14 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.795 | ppl 13.88 | bleu 23.48 | wps 2375 | wpb 2024.1 | bsz 97.5 | num_updates 19000 | best_bleu 24.24
2021-05-01 13:52:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 19000 updates
2021-05-01 13:52:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_19000.pt
2021-05-01 13:52:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_19000.pt
2021-05-01 13:52:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_19000.pt (epoch 1 @ 19000 updates, score 23.48) (writing took 7.753330444997118 seconds)
2021-05-01 13:52:45 | INFO | train_inner | epoch 001:  19105 / 60421 loss=1.925, ppl=3.8, wps=3818.1, ups=1.03, wpb=3714.6, bsz=124.7, num_updates=19100, lr=0.000228814, gnorm=2.015, loss_scale=8, train_wall=23, gb_free=10.7, wall=5846
2021-05-01 13:53:07 | INFO | train_inner | epoch 001:  19205 / 60421 loss=1.949, ppl=3.86, wps=16458.3, ups=4.44, wpb=3707.4, bsz=139.6, num_updates=19200, lr=0.000228218, gnorm=2.114, loss_scale=8, train_wall=22, gb_free=10.9, wall=5868
2021-05-01 13:53:30 | INFO | train_inner | epoch 001:  19305 / 60421 loss=1.853, ppl=3.61, wps=16444, ups=4.4, wpb=3736.9, bsz=114.6, num_updates=19300, lr=0.000227626, gnorm=1.677, loss_scale=8, train_wall=23, gb_free=10.8, wall=5891
2021-05-01 13:53:53 | INFO | train_inner | epoch 001:  19405 / 60421 loss=1.858, ppl=3.63, wps=16392.6, ups=4.42, wpb=3706.5, bsz=150.9, num_updates=19400, lr=0.000227038, gnorm=1.873, loss_scale=8, train_wall=22, gb_free=10.9, wall=5914
2021-05-01 13:54:16 | INFO | train_inner | epoch 001:  19505 / 60421 loss=1.78, ppl=3.43, wps=16559, ups=4.31, wpb=3838, bsz=139.1, num_updates=19500, lr=0.000226455, gnorm=1.6, loss_scale=8, train_wall=23, gb_free=10.8, wall=5937
2021-05-01 13:54:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-01 13:54:39 | INFO | train_inner | epoch 001:  19606 / 60421 loss=1.836, ppl=3.57, wps=16232, ups=4.34, wpb=3737.1, bsz=134.5, num_updates=19600, lr=0.000225877, gnorm=1.837, loss_scale=4, train_wall=23, gb_free=10.8, wall=5960
2021-05-01 13:55:02 | INFO | train_inner | epoch 001:  19706 / 60421 loss=1.794, ppl=3.47, wps=16350.2, ups=4.37, wpb=3742.3, bsz=110.8, num_updates=19700, lr=0.000225303, gnorm=1.895, loss_scale=4, train_wall=23, gb_free=11.3, wall=5983
2021-05-01 13:55:25 | INFO | train_inner | epoch 001:  19806 / 60421 loss=1.838, ppl=3.57, wps=16374.5, ups=4.33, wpb=3782.4, bsz=122.3, num_updates=19800, lr=0.000224733, gnorm=1.999, loss_scale=4, train_wall=23, gb_free=10.8, wall=6006
2021-05-01 13:55:48 | INFO | train_inner | epoch 001:  19906 / 60421 loss=1.779, ppl=3.43, wps=16121.7, ups=4.29, wpb=3762.3, bsz=137.3, num_updates=19900, lr=0.000224168, gnorm=1.701, loss_scale=4, train_wall=23, gb_free=10.8, wall=6029
2021-05-01 13:55:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2021-05-01 13:56:12 | INFO | train_inner | epoch 001:  20007 / 60421 loss=1.778, ppl=3.43, wps=16057.9, ups=4.26, wpb=3766.4, bsz=135, num_updates=20000, lr=0.000223607, gnorm=1.676, loss_scale=2, train_wall=23, gb_free=10.9, wall=6053
2021-05-01 13:56:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 13:56:12 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 13:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:56:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:56:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:56:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:56:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:56:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:56:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:56:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:56:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:56:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 13:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 13:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 13:57:18 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.756 | ppl 13.51 | bleu 24.84 | wps 2396.4 | wpb 2024.1 | bsz 97.5 | num_updates 20000 | best_bleu 24.84
2021-05-01 13:57:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 20000 updates
2021-05-01 13:57:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_20000.pt
2021-05-01 13:57:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_20000.pt
2021-05-01 13:57:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_20000.pt (epoch 1 @ 20000 updates, score 24.84) (writing took 14.408110219002992 seconds)
2021-05-01 13:57:55 | INFO | train_inner | epoch 001:  20107 / 60421 loss=1.745, ppl=3.35, wps=3666.6, ups=0.97, wpb=3793.6, bsz=125.1, num_updates=20100, lr=0.00022305, gnorm=1.624, loss_scale=2, train_wall=23, gb_free=10.9, wall=6156
2021-05-01 13:58:18 | INFO | train_inner | epoch 001:  20207 / 60421 loss=1.904, ppl=3.74, wps=16604.4, ups=4.45, wpb=3734.7, bsz=127.8, num_updates=20200, lr=0.000222497, gnorm=1.88, loss_scale=2, train_wall=22, gb_free=10.8, wall=6179
2021-05-01 13:58:40 | INFO | train_inner | epoch 001:  20307 / 60421 loss=1.896, ppl=3.72, wps=16453.8, ups=4.44, wpb=3708.9, bsz=115.1, num_updates=20300, lr=0.000221948, gnorm=1.896, loss_scale=2, train_wall=22, gb_free=10.8, wall=6201
2021-05-01 13:59:03 | INFO | train_inner | epoch 001:  20407 / 60421 loss=1.698, ppl=3.25, wps=16390.4, ups=4.42, wpb=3710.1, bsz=128.6, num_updates=20400, lr=0.000221404, gnorm=1.71, loss_scale=2, train_wall=22, gb_free=11.4, wall=6224
2021-05-01 13:59:26 | INFO | train_inner | epoch 001:  20507 / 60421 loss=1.888, ppl=3.7, wps=16559.4, ups=4.41, wpb=3757.8, bsz=127.1, num_updates=20500, lr=0.000220863, gnorm=1.876, loss_scale=2, train_wall=23, gb_free=11, wall=6246
2021-05-01 13:59:48 | INFO | train_inner | epoch 001:  20607 / 60421 loss=1.894, ppl=3.72, wps=16559.9, ups=4.42, wpb=3749, bsz=130.4, num_updates=20600, lr=0.000220326, gnorm=1.833, loss_scale=2, train_wall=22, gb_free=10.8, wall=6269
2021-05-01 14:00:11 | INFO | train_inner | epoch 001:  20707 / 60421 loss=1.749, ppl=3.36, wps=16601.7, ups=4.35, wpb=3818.9, bsz=126, num_updates=20700, lr=0.000219793, gnorm=1.537, loss_scale=2, train_wall=23, gb_free=10.8, wall=6292
2021-05-01 14:00:34 | INFO | train_inner | epoch 001:  20807 / 60421 loss=1.758, ppl=3.38, wps=16375.1, ups=4.37, wpb=3751, bsz=125.8, num_updates=20800, lr=0.000219265, gnorm=1.817, loss_scale=2, train_wall=23, gb_free=10.9, wall=6315
2021-05-01 14:00:57 | INFO | train_inner | epoch 001:  20907 / 60421 loss=1.769, ppl=3.41, wps=16108.3, ups=4.32, wpb=3732.9, bsz=135.5, num_updates=20900, lr=0.000218739, gnorm=1.836, loss_scale=2, train_wall=23, gb_free=10.7, wall=6338
2021-05-01 14:01:21 | INFO | train_inner | epoch 001:  21007 / 60421 loss=1.709, ppl=3.27, wps=16112, ups=4.26, wpb=3781.5, bsz=138.3, num_updates=21000, lr=0.000218218, gnorm=1.668, loss_scale=2, train_wall=23, gb_free=10.8, wall=6362
2021-05-01 14:01:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:01:21 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:01:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:01:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:01:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:01:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:01:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:01:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:01:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:01:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:01:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:01:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:01:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:01:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:01:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:01:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:01:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:02:28 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.807 | ppl 14 | bleu 25.11 | wps 2354.7 | wpb 2024.1 | bsz 97.5 | num_updates 21000 | best_bleu 25.11
2021-05-01 14:02:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 21000 updates
2021-05-01 14:02:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_21000.pt
2021-05-01 14:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_21000.pt
2021-05-01 14:02:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_21000.pt (epoch 1 @ 21000 updates, score 25.11) (writing took 14.358057491001091 seconds)
2021-05-01 14:03:05 | INFO | train_inner | epoch 001:  21107 / 60421 loss=1.744, ppl=3.35, wps=3582.3, ups=0.96, wpb=3733.1, bsz=134.2, num_updates=21100, lr=0.0002177, gnorm=2.03, loss_scale=2, train_wall=22, gb_free=10.7, wall=6466
2021-05-01 14:03:27 | INFO | train_inner | epoch 001:  21207 / 60421 loss=1.804, ppl=3.49, wps=16638.2, ups=4.46, wpb=3726.8, bsz=131.1, num_updates=21200, lr=0.000217186, gnorm=1.889, loss_scale=2, train_wall=22, gb_free=10.7, wall=6488
2021-05-01 14:03:50 | INFO | train_inner | epoch 001:  21307 / 60421 loss=1.735, ppl=3.33, wps=16685.1, ups=4.43, wpb=3764.2, bsz=134.6, num_updates=21300, lr=0.000216676, gnorm=1.669, loss_scale=2, train_wall=22, gb_free=10.7, wall=6511
2021-05-01 14:04:13 | INFO | train_inner | epoch 001:  21407 / 60421 loss=1.841, ppl=3.58, wps=16629, ups=4.42, wpb=3763.4, bsz=129.7, num_updates=21400, lr=0.000216169, gnorm=1.956, loss_scale=2, train_wall=22, gb_free=10.8, wall=6533
2021-05-01 14:04:35 | INFO | train_inner | epoch 001:  21507 / 60421 loss=1.783, ppl=3.44, wps=16353, ups=4.43, wpb=3692.1, bsz=149.2, num_updates=21500, lr=0.000215666, gnorm=1.841, loss_scale=2, train_wall=22, gb_free=11.2, wall=6556
2021-05-01 14:04:57 | INFO | train_inner | epoch 001:  21607 / 60421 loss=1.872, ppl=3.66, wps=16295, ups=4.49, wpb=3633.2, bsz=127.1, num_updates=21600, lr=0.000215166, gnorm=2.426, loss_scale=2, train_wall=22, gb_free=10.8, wall=6578
2021-05-01 14:05:20 | INFO | train_inner | epoch 001:  21707 / 60421 loss=1.803, ppl=3.49, wps=16253.8, ups=4.37, wpb=3721.9, bsz=142.5, num_updates=21700, lr=0.000214669, gnorm=1.945, loss_scale=2, train_wall=23, gb_free=10.8, wall=6601
2021-05-01 14:05:43 | INFO | train_inner | epoch 001:  21807 / 60421 loss=1.676, ppl=3.2, wps=16340.9, ups=4.34, wpb=3761.3, bsz=138.9, num_updates=21800, lr=0.000214176, gnorm=1.543, loss_scale=2, train_wall=23, gb_free=11.1, wall=6624
2021-05-01 14:06:07 | INFO | train_inner | epoch 001:  21907 / 60421 loss=1.74, ppl=3.34, wps=16296.7, ups=4.31, wpb=3785.2, bsz=119.9, num_updates=21900, lr=0.000213687, gnorm=1.717, loss_scale=2, train_wall=23, gb_free=10.8, wall=6648
2021-05-01 14:06:30 | INFO | train_inner | epoch 001:  22007 / 60421 loss=1.764, ppl=3.4, wps=15868.8, ups=4.28, wpb=3705.8, bsz=124.9, num_updates=22000, lr=0.000213201, gnorm=2.011, loss_scale=2, train_wall=23, gb_free=11.5, wall=6671
2021-05-01 14:06:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:06:30 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:06:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:06:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:06:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:06:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:06:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:06:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:06:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:06:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:06:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:06:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:06:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:06:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:06:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:06:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:06:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:07:35 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.774 | ppl 13.68 | bleu 24.63 | wps 2425.5 | wpb 2024.1 | bsz 97.5 | num_updates 22000 | best_bleu 25.11
2021-05-01 14:07:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 22000 updates
2021-05-01 14:07:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_22000.pt
2021-05-01 14:07:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_22000.pt
2021-05-01 14:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_22000.pt (epoch 1 @ 22000 updates, score 24.63) (writing took 7.670064038000419 seconds)
2021-05-01 14:08:05 | INFO | train_inner | epoch 001:  22107 / 60421 loss=1.851, ppl=3.61, wps=3852, ups=1.05, wpb=3682.3, bsz=137.3, num_updates=22100, lr=0.000212718, gnorm=2.078, loss_scale=2, train_wall=22, gb_free=10.9, wall=6766
2021-05-01 14:08:28 | INFO | train_inner | epoch 001:  22207 / 60421 loss=1.753, ppl=3.37, wps=16642.8, ups=4.42, wpb=3761.7, bsz=146.1, num_updates=22200, lr=0.000212238, gnorm=1.651, loss_scale=2, train_wall=22, gb_free=10.7, wall=6789
2021-05-01 14:08:51 | INFO | train_inner | epoch 001:  22307 / 60421 loss=1.781, ppl=3.44, wps=16565.3, ups=4.44, wpb=3734.5, bsz=130.9, num_updates=22300, lr=0.000211762, gnorm=1.826, loss_scale=2, train_wall=22, gb_free=10.8, wall=6812
2021-05-01 14:09:13 | INFO | train_inner | epoch 001:  22407 / 60421 loss=1.721, ppl=3.3, wps=16536.5, ups=4.49, wpb=3682.3, bsz=126.6, num_updates=22400, lr=0.000211289, gnorm=1.894, loss_scale=2, train_wall=22, gb_free=11, wall=6834
2021-05-01 14:09:36 | INFO | train_inner | epoch 001:  22507 / 60421 loss=1.81, ppl=3.51, wps=16737.6, ups=4.41, wpb=3793.4, bsz=119.4, num_updates=22500, lr=0.000210819, gnorm=1.833, loss_scale=2, train_wall=22, gb_free=10.9, wall=6857
2021-05-01 14:09:58 | INFO | train_inner | epoch 001:  22607 / 60421 loss=1.836, ppl=3.57, wps=16430.1, ups=4.47, wpb=3674.6, bsz=153.3, num_updates=22600, lr=0.000210352, gnorm=1.871, loss_scale=2, train_wall=22, gb_free=10.7, wall=6879
2021-05-01 14:10:20 | INFO | train_inner | epoch 001:  22707 / 60421 loss=1.735, ppl=3.33, wps=16379.6, ups=4.48, wpb=3658.9, bsz=126.6, num_updates=22700, lr=0.000209888, gnorm=1.902, loss_scale=2, train_wall=22, gb_free=10.8, wall=6901
2021-05-01 14:10:43 | INFO | train_inner | epoch 001:  22807 / 60421 loss=1.74, ppl=3.34, wps=16543.5, ups=4.45, wpb=3721.6, bsz=122.3, num_updates=22800, lr=0.000209427, gnorm=1.874, loss_scale=2, train_wall=22, gb_free=11, wall=6924
2021-05-01 14:11:06 | INFO | train_inner | epoch 001:  22907 / 60421 loss=1.601, ppl=3.03, wps=16637.7, ups=4.39, wpb=3788.1, bsz=147.2, num_updates=22900, lr=0.000208969, gnorm=1.565, loss_scale=2, train_wall=23, gb_free=10.9, wall=6947
2021-05-01 14:11:28 | INFO | train_inner | epoch 001:  23007 / 60421 loss=1.849, ppl=3.6, wps=16489.4, ups=4.45, wpb=3706.9, bsz=119.6, num_updates=23000, lr=0.000208514, gnorm=2.23, loss_scale=2, train_wall=22, gb_free=10.8, wall=6969
2021-05-01 14:11:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:11:28 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:11:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:11:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:11:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:11:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:11:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:11:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:11:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:11:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:11:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:11:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:11:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:11:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:12:35 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.833 | ppl 14.25 | bleu 24.49 | wps 2378.4 | wpb 2024.1 | bsz 97.5 | num_updates 23000 | best_bleu 25.11
2021-05-01 14:12:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 23000 updates
2021-05-01 14:12:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_23000.pt
2021-05-01 14:12:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_23000.pt
2021-05-01 14:12:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_23000.pt (epoch 1 @ 23000 updates, score 24.49) (writing took 8.001314913999522 seconds)
2021-05-01 14:13:05 | INFO | train_inner | epoch 001:  23107 / 60421 loss=1.955, ppl=3.88, wps=3811, ups=1.03, wpb=3700.8, bsz=126.1, num_updates=23100, lr=0.000208063, gnorm=2.452, loss_scale=2, train_wall=22, gb_free=10.7, wall=7066
2021-05-01 14:13:28 | INFO | train_inner | epoch 001:  23207 / 60421 loss=1.749, ppl=3.36, wps=16678.2, ups=4.44, wpb=3755.3, bsz=142.3, num_updates=23200, lr=0.000207614, gnorm=1.908, loss_scale=2, train_wall=22, gb_free=10.7, wall=7089
2021-05-01 14:13:51 | INFO | train_inner | epoch 001:  23307 / 60421 loss=1.577, ppl=2.98, wps=16597.5, ups=4.37, wpb=3796.8, bsz=145.7, num_updates=23300, lr=0.000207168, gnorm=1.413, loss_scale=2, train_wall=23, gb_free=11, wall=7111
2021-05-01 14:14:13 | INFO | train_inner | epoch 001:  23407 / 60421 loss=1.576, ppl=2.98, wps=16576.4, ups=4.37, wpb=3790.9, bsz=149.8, num_updates=23400, lr=0.000206725, gnorm=1.397, loss_scale=2, train_wall=23, gb_free=10.8, wall=7134
2021-05-01 14:14:36 | INFO | train_inner | epoch 001:  23507 / 60421 loss=1.624, ppl=3.08, wps=16676.4, ups=4.41, wpb=3780.8, bsz=132.8, num_updates=23500, lr=0.000206284, gnorm=1.452, loss_scale=2, train_wall=22, gb_free=10.9, wall=7157
2021-05-01 14:14:59 | INFO | train_inner | epoch 001:  23607 / 60421 loss=1.624, ppl=3.08, wps=16511.7, ups=4.37, wpb=3782.1, bsz=147.8, num_updates=23600, lr=0.000205847, gnorm=1.652, loss_scale=2, train_wall=23, gb_free=10.8, wall=7180
2021-05-01 14:15:22 | INFO | train_inner | epoch 001:  23707 / 60421 loss=1.863, ppl=3.64, wps=16348.5, ups=4.39, wpb=3726.7, bsz=127.9, num_updates=23700, lr=0.000205412, gnorm=2.313, loss_scale=2, train_wall=23, gb_free=10.8, wall=7203
2021-05-01 14:15:44 | INFO | train_inner | epoch 001:  23807 / 60421 loss=1.829, ppl=3.55, wps=16187.9, ups=4.41, wpb=3667.9, bsz=136.1, num_updates=23800, lr=0.00020498, gnorm=2.515, loss_scale=2, train_wall=22, gb_free=11.9, wall=7225
2021-05-01 14:16:08 | INFO | train_inner | epoch 001:  23907 / 60421 loss=1.81, ppl=3.51, wps=16061.5, ups=4.33, wpb=3709, bsz=143, num_updates=23900, lr=0.000204551, gnorm=2.26, loss_scale=2, train_wall=23, gb_free=11.1, wall=7248
2021-05-01 14:16:31 | INFO | train_inner | epoch 001:  24007 / 60421 loss=1.842, ppl=3.58, wps=16052.8, ups=4.31, wpb=3726.4, bsz=124, num_updates=24000, lr=0.000204124, gnorm=1.827, loss_scale=2, train_wall=23, gb_free=10.6, wall=7272
2021-05-01 14:16:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:16:31 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:16:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:16:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:16:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:16:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:16:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:16:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:16:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:16:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:16:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:16:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:16:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:16:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:17:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.755 | ppl 13.5 | bleu 24.94 | wps 2429.8 | wpb 2024.1 | bsz 97.5 | num_updates 24000 | best_bleu 25.11
2021-05-01 14:17:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 24000 updates
2021-05-01 14:17:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_24000.pt
2021-05-01 14:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_24000.pt
2021-05-01 14:17:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_24000.pt (epoch 1 @ 24000 updates, score 24.94) (writing took 8.255117014996358 seconds)
2021-05-01 14:18:07 | INFO | train_inner | epoch 001:  24107 / 60421 loss=1.683, ppl=3.21, wps=3896.3, ups=1.04, wpb=3752.5, bsz=132.6, num_updates=24100, lr=0.0002037, gnorm=1.716, loss_scale=2, train_wall=23, gb_free=10.9, wall=7368
2021-05-01 14:18:30 | INFO | train_inner | epoch 001:  24207 / 60421 loss=1.748, ppl=3.36, wps=16595.9, ups=4.43, wpb=3750.4, bsz=137.9, num_updates=24200, lr=0.000203279, gnorm=1.894, loss_scale=2, train_wall=22, gb_free=10.8, wall=7391
2021-05-01 14:18:52 | INFO | train_inner | epoch 001:  24307 / 60421 loss=1.724, ppl=3.3, wps=16630.8, ups=4.4, wpb=3775.8, bsz=149.9, num_updates=24300, lr=0.00020286, gnorm=2.005, loss_scale=2, train_wall=23, gb_free=10.6, wall=7413
2021-05-01 14:19:15 | INFO | train_inner | epoch 001:  24407 / 60421 loss=1.673, ppl=3.19, wps=16659.6, ups=4.37, wpb=3810.5, bsz=118.3, num_updates=24400, lr=0.000202444, gnorm=1.56, loss_scale=2, train_wall=23, gb_free=10.8, wall=7436
2021-05-01 14:19:38 | INFO | train_inner | epoch 001:  24507 / 60421 loss=1.651, ppl=3.14, wps=16474.4, ups=4.4, wpb=3746.2, bsz=136.3, num_updates=24500, lr=0.000202031, gnorm=1.798, loss_scale=2, train_wall=23, gb_free=10.7, wall=7459
2021-05-01 14:20:01 | INFO | train_inner | epoch 001:  24607 / 60421 loss=1.698, ppl=3.24, wps=16475.6, ups=4.38, wpb=3764.7, bsz=123, num_updates=24600, lr=0.000201619, gnorm=1.774, loss_scale=2, train_wall=23, gb_free=10.9, wall=7482
2021-05-01 14:20:24 | INFO | train_inner | epoch 001:  24707 / 60421 loss=1.609, ppl=3.05, wps=16266.8, ups=4.35, wpb=3742.8, bsz=125.9, num_updates=24700, lr=0.000201211, gnorm=1.608, loss_scale=2, train_wall=23, gb_free=10.8, wall=7505
2021-05-01 14:20:47 | INFO | train_inner | epoch 001:  24807 / 60421 loss=1.528, ppl=2.88, wps=16266.9, ups=4.28, wpb=3800.8, bsz=149.3, num_updates=24800, lr=0.000200805, gnorm=1.383, loss_scale=2, train_wall=23, gb_free=10.8, wall=7528
2021-05-01 14:21:11 | INFO | train_inner | epoch 001:  24907 / 60421 loss=1.759, ppl=3.38, wps=16102.1, ups=4.29, wpb=3757.2, bsz=132.6, num_updates=24900, lr=0.000200401, gnorm=2.001, loss_scale=2, train_wall=23, gb_free=10.8, wall=7551
2021-05-01 14:21:34 | INFO | train_inner | epoch 001:  25007 / 60421 loss=1.692, ppl=3.23, wps=15864.2, ups=4.27, wpb=3712.2, bsz=153.2, num_updates=25000, lr=0.0002, gnorm=1.77, loss_scale=2, train_wall=23, gb_free=11.5, wall=7575
2021-05-01 14:21:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:21:34 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:21:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:21:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:21:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:21:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:21:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:21:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:21:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:21:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:21:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:21:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:21:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:21:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:21:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:21:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:21:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:22:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.748 | ppl 13.44 | bleu 25.2 | wps 2436.5 | wpb 2024.1 | bsz 97.5 | num_updates 25000 | best_bleu 25.2
2021-05-01 14:22:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 25000 updates
2021-05-01 14:22:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_25000.pt
2021-05-01 14:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_25000.pt
2021-05-01 14:22:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_25000.pt (epoch 1 @ 25000 updates, score 25.2) (writing took 14.332155270996736 seconds)
2021-05-01 14:23:16 | INFO | train_inner | epoch 001:  25107 / 60421 loss=1.616, ppl=3.07, wps=3647.5, ups=0.98, wpb=3720.6, bsz=119.4, num_updates=25100, lr=0.000199601, gnorm=1.589, loss_scale=2, train_wall=22, gb_free=10.7, wall=7677
2021-05-01 14:23:38 | INFO | train_inner | epoch 001:  25207 / 60421 loss=1.77, ppl=3.41, wps=16685.1, ups=4.45, wpb=3746.4, bsz=118.4, num_updates=25200, lr=0.000199205, gnorm=1.867, loss_scale=2, train_wall=22, gb_free=11.6, wall=7699
2021-05-01 14:24:01 | INFO | train_inner | epoch 001:  25307 / 60421 loss=1.721, ppl=3.3, wps=16585.5, ups=4.42, wpb=3751.1, bsz=134.6, num_updates=25300, lr=0.000198811, gnorm=1.895, loss_scale=2, train_wall=22, gb_free=10.9, wall=7722
2021-05-01 14:24:24 | INFO | train_inner | epoch 001:  25407 / 60421 loss=1.699, ppl=3.25, wps=16589.6, ups=4.38, wpb=3789.2, bsz=119.9, num_updates=25400, lr=0.000198419, gnorm=1.688, loss_scale=2, train_wall=23, gb_free=10.8, wall=7745
2021-05-01 14:24:46 | INFO | train_inner | epoch 001:  25507 / 60421 loss=1.752, ppl=3.37, wps=16384.1, ups=4.46, wpb=3675.6, bsz=115.3, num_updates=25500, lr=0.00019803, gnorm=2.025, loss_scale=2, train_wall=22, gb_free=10.8, wall=7767
2021-05-01 14:25:09 | INFO | train_inner | epoch 001:  25607 / 60421 loss=1.737, ppl=3.33, wps=16123.4, ups=4.43, wpb=3642.8, bsz=143, num_updates=25600, lr=0.000197642, gnorm=1.928, loss_scale=2, train_wall=22, gb_free=10.9, wall=7790
2021-05-01 14:25:32 | INFO | train_inner | epoch 001:  25707 / 60421 loss=1.718, ppl=3.29, wps=16094, ups=4.41, wpb=3652.3, bsz=116.5, num_updates=25700, lr=0.000197257, gnorm=1.992, loss_scale=2, train_wall=23, gb_free=11.2, wall=7813
2021-05-01 14:25:55 | INFO | train_inner | epoch 001:  25807 / 60421 loss=1.697, ppl=3.24, wps=16093.9, ups=4.32, wpb=3723.6, bsz=128.3, num_updates=25800, lr=0.000196875, gnorm=1.838, loss_scale=2, train_wall=23, gb_free=10.7, wall=7836
2021-05-01 14:26:18 | INFO | train_inner | epoch 001:  25907 / 60421 loss=1.782, ppl=3.44, wps=16106.2, ups=4.33, wpb=3717.8, bsz=111.8, num_updates=25900, lr=0.000196494, gnorm=2.108, loss_scale=2, train_wall=23, gb_free=10.9, wall=7859
2021-05-01 14:26:42 | INFO | train_inner | epoch 001:  26007 / 60421 loss=1.658, ppl=3.15, wps=15839.6, ups=4.2, wpb=3774.7, bsz=155.8, num_updates=26000, lr=0.000196116, gnorm=1.642, loss_scale=2, train_wall=24, gb_free=10.8, wall=7883
2021-05-01 14:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:26:42 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:26:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:26:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:26:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:26:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:26:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:26:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:26:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:26:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:26:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:26:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:26:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:26:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:27:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:27:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:27:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:27:46 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.795 | ppl 13.88 | bleu 25.07 | wps 2450.7 | wpb 2024.1 | bsz 97.5 | num_updates 26000 | best_bleu 25.2
2021-05-01 14:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 26000 updates
2021-05-01 14:27:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_26000.pt
2021-05-01 14:27:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_26000.pt
2021-05-01 14:27:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_26000.pt (epoch 1 @ 26000 updates, score 25.07) (writing took 10.149798261001706 seconds)
2021-05-01 14:28:19 | INFO | train_inner | epoch 001:  26107 / 60421 loss=1.684, ppl=3.21, wps=3803.1, ups=1.03, wpb=3708.6, bsz=120.2, num_updates=26100, lr=0.00019574, gnorm=1.798, loss_scale=2, train_wall=22, gb_free=10.7, wall=7980
2021-05-01 14:28:42 | INFO | train_inner | epoch 001:  26207 / 60421 loss=1.616, ppl=3.07, wps=16481.8, ups=4.45, wpb=3703, bsz=132.1, num_updates=26200, lr=0.000195366, gnorm=1.938, loss_scale=2, train_wall=22, gb_free=11, wall=8003
2021-05-01 14:29:04 | INFO | train_inner | epoch 001:  26307 / 60421 loss=1.835, ppl=3.57, wps=16509.9, ups=4.44, wpb=3714.3, bsz=125.1, num_updates=26300, lr=0.000194994, gnorm=2.284, loss_scale=2, train_wall=22, gb_free=10.8, wall=8025
2021-05-01 14:29:27 | INFO | train_inner | epoch 001:  26407 / 60421 loss=1.771, ppl=3.41, wps=16559.5, ups=4.39, wpb=3767.8, bsz=132.5, num_updates=26400, lr=0.000194625, gnorm=1.951, loss_scale=2, train_wall=23, gb_free=10.9, wall=8048
2021-05-01 14:29:50 | INFO | train_inner | epoch 001:  26507 / 60421 loss=1.761, ppl=3.39, wps=16431.6, ups=4.38, wpb=3748.1, bsz=131.8, num_updates=26500, lr=0.000194257, gnorm=2.049, loss_scale=2, train_wall=23, gb_free=10.8, wall=8071
2021-05-01 14:30:13 | INFO | train_inner | epoch 001:  26607 / 60421 loss=1.592, ppl=3.01, wps=16443.1, ups=4.37, wpb=3761.2, bsz=125.5, num_updates=26600, lr=0.000193892, gnorm=1.576, loss_scale=2, train_wall=23, gb_free=11.1, wall=8093
2021-05-01 14:30:35 | INFO | train_inner | epoch 001:  26707 / 60421 loss=1.653, ppl=3.14, wps=16137.2, ups=4.39, wpb=3674.3, bsz=133.3, num_updates=26700, lr=0.000193528, gnorm=1.839, loss_scale=2, train_wall=23, gb_free=11.4, wall=8116
2021-05-01 14:30:59 | INFO | train_inner | epoch 001:  26807 / 60421 loss=1.631, ppl=3.1, wps=16218.7, ups=4.3, wpb=3771.8, bsz=124.4, num_updates=26800, lr=0.000193167, gnorm=1.588, loss_scale=2, train_wall=23, gb_free=10.7, wall=8140
2021-05-01 14:31:22 | INFO | train_inner | epoch 001:  26907 / 60421 loss=1.646, ppl=3.13, wps=16294.6, ups=4.23, wpb=3856.7, bsz=117.9, num_updates=26900, lr=0.000192807, gnorm=1.514, loss_scale=2, train_wall=23, gb_free=10.8, wall=8163
2021-05-01 14:31:46 | INFO | train_inner | epoch 001:  27007 / 60421 loss=1.68, ppl=3.2, wps=15610.5, ups=4.18, wpb=3730.6, bsz=128.7, num_updates=27000, lr=0.00019245, gnorm=1.766, loss_scale=2, train_wall=24, gb_free=10.9, wall=8187
2021-05-01 14:31:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:31:46 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:31:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:31:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:31:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:32:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:32:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:32:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:32:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.8 | ppl 13.93 | bleu 25.79 | wps 2414.3 | wpb 2024.1 | bsz 97.5 | num_updates 27000 | best_bleu 25.79
2021-05-01 14:32:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 27000 updates
2021-05-01 14:32:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_27000.pt
2021-05-01 14:32:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_27000.pt
2021-05-01 14:33:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_27000.pt (epoch 1 @ 27000 updates, score 25.79) (writing took 14.358053755000583 seconds)
2021-05-01 14:33:29 | INFO | train_inner | epoch 001:  27107 / 60421 loss=1.644, ppl=3.12, wps=3667.5, ups=0.97, wpb=3778, bsz=145.4, num_updates=27100, lr=0.000192095, gnorm=1.858, loss_scale=2, train_wall=23, gb_free=10.8, wall=8290
2021-05-01 14:33:51 | INFO | train_inner | epoch 001:  27207 / 60421 loss=1.744, ppl=3.35, wps=16684, ups=4.49, wpb=3711.8, bsz=117.4, num_updates=27200, lr=0.000191741, gnorm=2, loss_scale=2, train_wall=22, gb_free=10.8, wall=8312
2021-05-01 14:34:14 | INFO | train_inner | epoch 001:  27307 / 60421 loss=1.601, ppl=3.03, wps=16346.6, ups=4.44, wpb=3681.2, bsz=138.2, num_updates=27300, lr=0.00019139, gnorm=1.827, loss_scale=2, train_wall=22, gb_free=10.7, wall=8335
2021-05-01 14:34:37 | INFO | train_inner | epoch 001:  27407 / 60421 loss=1.552, ppl=2.93, wps=16314.7, ups=4.39, wpb=3715.1, bsz=152.9, num_updates=27400, lr=0.00019104, gnorm=1.881, loss_scale=2, train_wall=23, gb_free=10.8, wall=8358
2021-05-01 14:34:59 | INFO | train_inner | epoch 001:  27507 / 60421 loss=1.619, ppl=3.07, wps=16439.2, ups=4.4, wpb=3738.4, bsz=135.7, num_updates=27500, lr=0.000190693, gnorm=1.733, loss_scale=2, train_wall=23, gb_free=11.5, wall=8380
2021-05-01 14:35:22 | INFO | train_inner | epoch 001:  27607 / 60421 loss=1.845, ppl=3.59, wps=16242.2, ups=4.38, wpb=3705.7, bsz=116.3, num_updates=27600, lr=0.000190347, gnorm=2.183, loss_scale=2, train_wall=23, gb_free=11.3, wall=8403
2021-05-01 14:35:45 | INFO | train_inner | epoch 001:  27707 / 60421 loss=1.851, ppl=3.61, wps=16159.7, ups=4.35, wpb=3718.5, bsz=130.2, num_updates=27700, lr=0.000190003, gnorm=2.15, loss_scale=2, train_wall=23, gb_free=11, wall=8426
2021-05-01 14:36:09 | INFO | train_inner | epoch 001:  27807 / 60421 loss=1.644, ppl=3.12, wps=16220.1, ups=4.28, wpb=3791.1, bsz=148.8, num_updates=27800, lr=0.000189661, gnorm=1.688, loss_scale=2, train_wall=23, gb_free=10.9, wall=8450
2021-05-01 14:36:32 | INFO | train_inner | epoch 001:  27907 / 60421 loss=1.63, ppl=3.09, wps=16076.7, ups=4.24, wpb=3790.3, bsz=115.7, num_updates=27900, lr=0.000189321, gnorm=1.542, loss_scale=2, train_wall=23, gb_free=10.9, wall=8473
2021-05-01 14:36:56 | INFO | train_inner | epoch 001:  28007 / 60421 loss=1.606, ppl=3.05, wps=15805.1, ups=4.24, wpb=3730.3, bsz=124.6, num_updates=28000, lr=0.000188982, gnorm=1.702, loss_scale=2, train_wall=23, gb_free=10.7, wall=8497
2021-05-01 14:36:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:36:56 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:37:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:37:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:37:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:37:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:37:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:37:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:38:01 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.836 | ppl 14.28 | bleu 25.54 | wps 2437.1 | wpb 2024.1 | bsz 97.5 | num_updates 28000 | best_bleu 25.79
2021-05-01 14:38:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 28000 updates
2021-05-01 14:38:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_28000.pt
2021-05-01 14:38:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_28000.pt
2021-05-01 14:38:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_28000.pt (epoch 1 @ 28000 updates, score 25.54) (writing took 8.29593959099293 seconds)
2021-05-01 14:38:32 | INFO | train_inner | epoch 001:  28107 / 60421 loss=1.721, ppl=3.3, wps=3921.8, ups=1.04, wpb=3773.8, bsz=118.2, num_updates=28100, lr=0.000188646, gnorm=1.814, loss_scale=2, train_wall=23, gb_free=10.7, wall=8593
2021-05-01 14:38:55 | INFO | train_inner | epoch 001:  28207 / 60421 loss=1.655, ppl=3.15, wps=16371.9, ups=4.38, wpb=3739.5, bsz=143.9, num_updates=28200, lr=0.000188311, gnorm=1.709, loss_scale=2, train_wall=23, gb_free=10.8, wall=8616
2021-05-01 14:39:17 | INFO | train_inner | epoch 001:  28307 / 60421 loss=1.692, ppl=3.23, wps=16296.8, ups=4.45, wpb=3659.2, bsz=127, num_updates=28300, lr=0.000187978, gnorm=2.253, loss_scale=2, train_wall=22, gb_free=10.8, wall=8638
2021-05-01 14:39:40 | INFO | train_inner | epoch 001:  28407 / 60421 loss=1.656, ppl=3.15, wps=16551.3, ups=4.42, wpb=3740.7, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.708, loss_scale=2, train_wall=22, gb_free=10.9, wall=8661
2021-05-01 14:40:03 | INFO | train_inner | epoch 001:  28507 / 60421 loss=1.595, ppl=3.02, wps=16441.5, ups=4.41, wpb=3732.1, bsz=130.7, num_updates=28500, lr=0.000187317, gnorm=1.959, loss_scale=2, train_wall=23, gb_free=10.6, wall=8684
2021-05-01 14:40:26 | INFO | train_inner | epoch 001:  28607 / 60421 loss=1.627, ppl=3.09, wps=16445.9, ups=4.34, wpb=3789, bsz=112, num_updates=28600, lr=0.000186989, gnorm=1.724, loss_scale=2, train_wall=23, gb_free=10.7, wall=8707
2021-05-01 14:40:49 | INFO | train_inner | epoch 001:  28707 / 60421 loss=1.528, ppl=2.88, wps=16277.1, ups=4.34, wpb=3753.6, bsz=139.5, num_updates=28700, lr=0.000186663, gnorm=1.556, loss_scale=2, train_wall=23, gb_free=10.9, wall=8730
2021-05-01 14:41:12 | INFO | train_inner | epoch 001:  28807 / 60421 loss=1.641, ppl=3.12, wps=15904, ups=4.34, wpb=3661.7, bsz=121.3, num_updates=28800, lr=0.000186339, gnorm=1.953, loss_scale=2, train_wall=23, gb_free=11.1, wall=8753
2021-05-01 14:41:35 | INFO | train_inner | epoch 001:  28907 / 60421 loss=1.622, ppl=3.08, wps=16004.6, ups=4.27, wpb=3749, bsz=134.3, num_updates=28900, lr=0.000186016, gnorm=1.826, loss_scale=2, train_wall=23, gb_free=10.8, wall=8776
2021-05-01 14:41:59 | INFO | train_inner | epoch 001:  29007 / 60421 loss=1.645, ppl=3.13, wps=15862.2, ups=4.27, wpb=3712.8, bsz=123.6, num_updates=29000, lr=0.000185695, gnorm=1.787, loss_scale=2, train_wall=23, gb_free=10.9, wall=8800
2021-05-01 14:41:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:41:59 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:42:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:42:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:42:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:42:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:42:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:42:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:42:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:42:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:42:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:42:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:42:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:42:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:42:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:42:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:42:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:43:05 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.811 | ppl 14.04 | bleu 25.57 | wps 2396 | wpb 2024.1 | bsz 97.5 | num_updates 29000 | best_bleu 25.79
2021-05-01 14:43:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 29000 updates
2021-05-01 14:43:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_29000.pt
2021-05-01 14:43:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_29000.pt
2021-05-01 14:43:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_29000.pt (epoch 1 @ 29000 updates, score 25.57) (writing took 10.106148304003 seconds)
2021-05-01 14:43:38 | INFO | train_inner | epoch 001:  29107 / 60421 loss=1.548, ppl=2.92, wps=3813.2, ups=1.01, wpb=3779.8, bsz=122.4, num_updates=29100, lr=0.000185376, gnorm=1.582, loss_scale=2, train_wall=23, gb_free=10.8, wall=8899
2021-05-01 14:44:00 | INFO | train_inner | epoch 001:  29207 / 60421 loss=1.557, ppl=2.94, wps=16693.3, ups=4.39, wpb=3800.8, bsz=146.1, num_updates=29200, lr=0.000185058, gnorm=1.558, loss_scale=2, train_wall=23, gb_free=10.8, wall=8921
2021-05-01 14:44:23 | INFO | train_inner | epoch 001:  29307 / 60421 loss=1.532, ppl=2.89, wps=16524.2, ups=4.37, wpb=3779.9, bsz=131.7, num_updates=29300, lr=0.000184742, gnorm=1.605, loss_scale=2, train_wall=23, gb_free=10.7, wall=8944
2021-05-01 14:44:46 | INFO | train_inner | epoch 001:  29407 / 60421 loss=1.652, ppl=3.14, wps=16221.8, ups=4.4, wpb=3683.8, bsz=152.5, num_updates=29400, lr=0.000184428, gnorm=1.934, loss_scale=2, train_wall=23, gb_free=10.9, wall=8967
2021-05-01 14:45:09 | INFO | train_inner | epoch 001:  29507 / 60421 loss=1.6, ppl=3.03, wps=16454.7, ups=4.39, wpb=3744.4, bsz=130.9, num_updates=29500, lr=0.000184115, gnorm=1.75, loss_scale=2, train_wall=23, gb_free=10.7, wall=8990
2021-05-01 14:45:32 | INFO | train_inner | epoch 001:  29607 / 60421 loss=1.543, ppl=2.91, wps=16432.5, ups=4.38, wpb=3753.8, bsz=127, num_updates=29600, lr=0.000183804, gnorm=1.655, loss_scale=2, train_wall=23, gb_free=10.8, wall=9013
2021-05-01 14:45:54 | INFO | train_inner | epoch 001:  29707 / 60421 loss=1.676, ppl=3.2, wps=16144.9, ups=4.39, wpb=3679, bsz=125.2, num_updates=29700, lr=0.000183494, gnorm=1.923, loss_scale=2, train_wall=23, gb_free=10.8, wall=9035
2021-05-01 14:46:18 | INFO | train_inner | epoch 001:  29807 / 60421 loss=1.549, ppl=2.93, wps=16033.2, ups=4.33, wpb=3705.4, bsz=128.3, num_updates=29800, lr=0.000183186, gnorm=1.867, loss_scale=2, train_wall=23, gb_free=10.8, wall=9059
2021-05-01 14:46:41 | INFO | train_inner | epoch 001:  29907 / 60421 loss=1.677, ppl=3.2, wps=15970.9, ups=4.2, wpb=3800.4, bsz=140.7, num_updates=29900, lr=0.000182879, gnorm=1.821, loss_scale=2, train_wall=24, gb_free=10.8, wall=9082
2021-05-01 14:47:04 | INFO | train_inner | epoch 001:  30007 / 60421 loss=1.634, ppl=3.1, wps=15920.5, ups=4.33, wpb=3676.2, bsz=142.5, num_updates=30000, lr=0.000182574, gnorm=2.099, loss_scale=2, train_wall=23, gb_free=10.8, wall=9105
2021-05-01 14:47:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:47:04 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:47:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:47:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:47:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:47:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:47:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:47:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:47:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:47:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:47:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:47:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:47:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:47:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:47:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:47:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:47:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:48:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.82 | ppl 14.13 | bleu 24.97 | wps 2448.2 | wpb 2024.1 | bsz 97.5 | num_updates 30000 | best_bleu 25.79
2021-05-01 14:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 30000 updates
2021-05-01 14:48:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_30000.pt
2021-05-01 14:48:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_30000.pt
2021-05-01 14:48:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_30000.pt (epoch 1 @ 30000 updates, score 24.97) (writing took 7.653300580997893 seconds)
2021-05-01 14:48:40 | INFO | train_inner | epoch 001:  30107 / 60421 loss=1.542, ppl=2.91, wps=3931.9, ups=1.05, wpb=3745.4, bsz=138.6, num_updates=30100, lr=0.000182271, gnorm=1.961, loss_scale=2, train_wall=23, gb_free=11, wall=9201
2021-05-01 14:49:02 | INFO | train_inner | epoch 001:  30207 / 60421 loss=1.603, ppl=3.04, wps=16542.4, ups=4.45, wpb=3719.3, bsz=131.2, num_updates=30200, lr=0.000181969, gnorm=1.775, loss_scale=2, train_wall=22, gb_free=10.9, wall=9223
2021-05-01 14:49:25 | INFO | train_inner | epoch 001:  30307 / 60421 loss=1.583, ppl=3, wps=16345.5, ups=4.43, wpb=3685.9, bsz=134.1, num_updates=30300, lr=0.000181668, gnorm=1.864, loss_scale=2, train_wall=22, gb_free=10.7, wall=9246
2021-05-01 14:49:47 | INFO | train_inner | epoch 001:  30407 / 60421 loss=1.71, ppl=3.27, wps=16451.6, ups=4.42, wpb=3723.4, bsz=123, num_updates=30400, lr=0.000181369, gnorm=1.898, loss_scale=2, train_wall=22, gb_free=10.9, wall=9268
2021-05-01 14:50:10 | INFO | train_inner | epoch 001:  30507 / 60421 loss=1.6, ppl=3.03, wps=16391.4, ups=4.41, wpb=3717.5, bsz=126.6, num_updates=30500, lr=0.000181071, gnorm=1.842, loss_scale=2, train_wall=23, gb_free=10.7, wall=9291
2021-05-01 14:50:33 | INFO | train_inner | epoch 001:  30607 / 60421 loss=1.577, ppl=2.98, wps=16446, ups=4.39, wpb=3745.5, bsz=119.8, num_updates=30600, lr=0.000180775, gnorm=1.619, loss_scale=2, train_wall=23, gb_free=10.9, wall=9314
2021-05-01 14:50:56 | INFO | train_inner | epoch 001:  30707 / 60421 loss=1.719, ppl=3.29, wps=16032.3, ups=4.36, wpb=3679.9, bsz=134, num_updates=30700, lr=0.000180481, gnorm=2.103, loss_scale=2, train_wall=23, gb_free=10.8, wall=9337
2021-05-01 14:51:19 | INFO | train_inner | epoch 001:  30807 / 60421 loss=1.629, ppl=3.09, wps=15794.1, ups=4.37, wpb=3615.2, bsz=149.8, num_updates=30800, lr=0.000180187, gnorm=2.392, loss_scale=2, train_wall=23, gb_free=10.8, wall=9360
2021-05-01 14:51:42 | INFO | train_inner | epoch 001:  30907 / 60421 loss=1.626, ppl=3.09, wps=15719.8, ups=4.26, wpb=3692.7, bsz=139.6, num_updates=30900, lr=0.000179896, gnorm=1.987, loss_scale=2, train_wall=23, gb_free=10.8, wall=9383
2021-05-01 14:52:05 | INFO | train_inner | epoch 001:  31007 / 60421 loss=1.603, ppl=3.04, wps=16082.9, ups=4.31, wpb=3733.9, bsz=139.7, num_updates=31000, lr=0.000179605, gnorm=1.685, loss_scale=2, train_wall=23, gb_free=10.7, wall=9406
2021-05-01 14:52:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:52:05 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:52:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:52:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:52:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:52:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:52:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:52:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:52:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:52:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:52:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:53:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.865 | ppl 14.57 | bleu 25.48 | wps 2482.7 | wpb 2024.1 | bsz 97.5 | num_updates 31000 | best_bleu 25.79
2021-05-01 14:53:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 31000 updates
2021-05-01 14:53:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_31000.pt
2021-05-01 14:53:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_31000.pt
2021-05-01 14:53:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_31000.pt (epoch 1 @ 31000 updates, score 25.48) (writing took 7.710272226999223 seconds)
2021-05-01 14:53:40 | INFO | train_inner | epoch 001:  31107 / 60421 loss=1.562, ppl=2.95, wps=3967.5, ups=1.06, wpb=3752.5, bsz=133.1, num_updates=31100, lr=0.000179316, gnorm=1.663, loss_scale=2, train_wall=23, gb_free=10.7, wall=9501
2021-05-01 14:54:03 | INFO | train_inner | epoch 001:  31207 / 60421 loss=1.489, ppl=2.81, wps=16574.2, ups=4.4, wpb=3762.8, bsz=123.5, num_updates=31200, lr=0.000179029, gnorm=1.417, loss_scale=2, train_wall=23, gb_free=11, wall=9524
2021-05-01 14:54:25 | INFO | train_inner | epoch 001:  31307 / 60421 loss=1.555, ppl=2.94, wps=16600.6, ups=4.43, wpb=3747.9, bsz=113, num_updates=31300, lr=0.000178743, gnorm=1.593, loss_scale=2, train_wall=22, gb_free=10.9, wall=9546
2021-05-01 14:54:48 | INFO | train_inner | epoch 001:  31407 / 60421 loss=1.529, ppl=2.89, wps=16628.3, ups=4.39, wpb=3790, bsz=138.2, num_updates=31400, lr=0.000178458, gnorm=1.564, loss_scale=2, train_wall=23, gb_free=11.2, wall=9569
2021-05-01 14:55:11 | INFO | train_inner | epoch 001:  31507 / 60421 loss=1.549, ppl=2.93, wps=16476.5, ups=4.37, wpb=3769.2, bsz=136.9, num_updates=31500, lr=0.000178174, gnorm=1.553, loss_scale=2, train_wall=23, gb_free=10.7, wall=9592
2021-05-01 14:55:34 | INFO | train_inner | epoch 001:  31607 / 60421 loss=1.401, ppl=2.64, wps=16232.6, ups=4.36, wpb=3723.1, bsz=144.4, num_updates=31600, lr=0.000177892, gnorm=1.455, loss_scale=2, train_wall=23, gb_free=10.8, wall=9615
2021-05-01 14:55:57 | INFO | train_inner | epoch 001:  31707 / 60421 loss=1.589, ppl=3.01, wps=16283, ups=4.28, wpb=3803.8, bsz=151, num_updates=31700, lr=0.000177611, gnorm=1.566, loss_scale=2, train_wall=23, gb_free=11.2, wall=9638
2021-05-01 14:56:21 | INFO | train_inner | epoch 001:  31807 / 60421 loss=1.531, ppl=2.89, wps=16239.6, ups=4.21, wpb=3855.8, bsz=140.8, num_updates=31800, lr=0.000177332, gnorm=1.418, loss_scale=2, train_wall=24, gb_free=10.8, wall=9662
2021-05-01 14:56:45 | INFO | train_inner | epoch 001:  31907 / 60421 loss=1.521, ppl=2.87, wps=15720.4, ups=4.22, wpb=3727.7, bsz=130.4, num_updates=31900, lr=0.000177054, gnorm=1.57, loss_scale=2, train_wall=24, gb_free=10.8, wall=9686
2021-05-01 14:57:08 | INFO | train_inner | epoch 001:  32007 / 60421 loss=1.582, ppl=2.99, wps=16068.2, ups=4.36, wpb=3685.4, bsz=125.4, num_updates=32000, lr=0.000176777, gnorm=1.998, loss_scale=2, train_wall=23, gb_free=10.7, wall=9709
2021-05-01 14:57:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 14:57:08 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 14:57:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:57:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:57:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:57:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:57:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:57:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:57:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:57:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:57:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:57:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:57:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:57:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:57:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:57:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:57:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:57:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 14:57:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 14:57:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 14:58:12 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.851 | ppl 14.43 | bleu 25.15 | wps 2458 | wpb 2024.1 | bsz 97.5 | num_updates 32000 | best_bleu 25.79
2021-05-01 14:58:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 32000 updates
2021-05-01 14:58:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_32000.pt
2021-05-01 14:58:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_32000.pt
2021-05-01 14:58:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_32000.pt (epoch 1 @ 32000 updates, score 25.15) (writing took 7.743600089997926 seconds)
2021-05-01 14:58:42 | INFO | train_inner | epoch 001:  32107 / 60421 loss=1.667, ppl=3.18, wps=3845.3, ups=1.06, wpb=3640.5, bsz=145, num_updates=32100, lr=0.000176501, gnorm=2.225, loss_scale=2, train_wall=22, gb_free=10.8, wall=9803
2021-05-01 14:59:05 | INFO | train_inner | epoch 001:  32207 / 60421 loss=1.606, ppl=3.04, wps=16505.7, ups=4.42, wpb=3733.5, bsz=137.6, num_updates=32200, lr=0.000176227, gnorm=2.027, loss_scale=2, train_wall=22, gb_free=10.7, wall=9826
2021-05-01 14:59:28 | INFO | train_inner | epoch 001:  32307 / 60421 loss=1.576, ppl=2.98, wps=16567.2, ups=4.41, wpb=3754.8, bsz=134.6, num_updates=32300, lr=0.000175954, gnorm=1.721, loss_scale=2, train_wall=22, gb_free=11.1, wall=9848
2021-05-01 14:59:50 | INFO | train_inner | epoch 001:  32407 / 60421 loss=1.507, ppl=2.84, wps=16586.4, ups=4.41, wpb=3758.8, bsz=139.8, num_updates=32400, lr=0.000175682, gnorm=1.707, loss_scale=2, train_wall=22, gb_free=10.7, wall=9871
2021-05-01 15:00:13 | INFO | train_inner | epoch 001:  32507 / 60421 loss=1.535, ppl=2.9, wps=16357.5, ups=4.33, wpb=3774.8, bsz=149.6, num_updates=32500, lr=0.000175412, gnorm=1.685, loss_scale=2, train_wall=23, gb_free=10.8, wall=9894
2021-05-01 15:00:36 | INFO | train_inner | epoch 001:  32607 / 60421 loss=1.566, ppl=2.96, wps=16118.4, ups=4.39, wpb=3670.7, bsz=140, num_updates=32600, lr=0.000175142, gnorm=1.969, loss_scale=2, train_wall=23, gb_free=11, wall=9917
2021-05-01 15:00:59 | INFO | train_inner | epoch 001:  32707 / 60421 loss=1.587, ppl=3, wps=16221.7, ups=4.31, wpb=3763.2, bsz=119.4, num_updates=32700, lr=0.000174874, gnorm=1.775, loss_scale=2, train_wall=23, gb_free=10.8, wall=9940
2021-05-01 15:01:22 | INFO | train_inner | epoch 001:  32807 / 60421 loss=1.657, ppl=3.15, wps=15829.7, ups=4.31, wpb=3673.8, bsz=129.4, num_updates=32800, lr=0.000174608, gnorm=2.113, loss_scale=2, train_wall=23, gb_free=10.8, wall=9963
2021-05-01 15:01:46 | INFO | train_inner | epoch 001:  32907 / 60421 loss=1.56, ppl=2.95, wps=15690.9, ups=4.26, wpb=3685.1, bsz=125.6, num_updates=32900, lr=0.000174342, gnorm=1.842, loss_scale=2, train_wall=23, gb_free=11.2, wall=9987
2021-05-01 15:02:09 | INFO | train_inner | epoch 001:  33007 / 60421 loss=1.694, ppl=3.23, wps=16218.4, ups=4.37, wpb=3711.2, bsz=131.9, num_updates=33000, lr=0.000174078, gnorm=2.082, loss_scale=2, train_wall=23, gb_free=10.9, wall=10010
2021-05-01 15:02:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:02:09 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:02:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:02:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:02:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:02:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:02:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:02:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:02:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:02:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:02:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:03:15 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.879 | ppl 14.71 | bleu 25.67 | wps 2407.3 | wpb 2024.1 | bsz 97.5 | num_updates 33000 | best_bleu 25.79
2021-05-01 15:03:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 33000 updates
2021-05-01 15:03:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_33000.pt
2021-05-01 15:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_33000.pt
2021-05-01 15:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_33000.pt (epoch 1 @ 33000 updates, score 25.67) (writing took 7.796759293996729 seconds)
2021-05-01 15:03:45 | INFO | train_inner | epoch 001:  33107 / 60421 loss=1.543, ppl=2.91, wps=3841.8, ups=1.04, wpb=3702.5, bsz=138.2, num_updates=33100, lr=0.000173814, gnorm=1.725, loss_scale=2, train_wall=22, gb_free=10.8, wall=10106
2021-05-01 15:04:08 | INFO | train_inner | epoch 001:  33207 / 60421 loss=1.526, ppl=2.88, wps=16182.4, ups=4.46, wpb=3627.9, bsz=139, num_updates=33200, lr=0.000173553, gnorm=1.881, loss_scale=2, train_wall=22, gb_free=10.8, wall=10129
2021-05-01 15:04:30 | INFO | train_inner | epoch 001:  33307 / 60421 loss=1.593, ppl=3.02, wps=16488.4, ups=4.38, wpb=3763.7, bsz=137.5, num_updates=33300, lr=0.000173292, gnorm=1.93, loss_scale=2, train_wall=23, gb_free=10.9, wall=10151
2021-05-01 15:04:53 | INFO | train_inner | epoch 001:  33407 / 60421 loss=1.616, ppl=3.07, wps=16650.3, ups=4.41, wpb=3776.2, bsz=111.8, num_updates=33400, lr=0.000173032, gnorm=1.593, loss_scale=2, train_wall=23, gb_free=10.8, wall=10174
2021-05-01 15:05:16 | INFO | train_inner | epoch 001:  33507 / 60421 loss=1.508, ppl=2.84, wps=16354.2, ups=4.4, wpb=3718.7, bsz=112.2, num_updates=33500, lr=0.000172774, gnorm=1.715, loss_scale=2, train_wall=23, gb_free=11, wall=10197
2021-05-01 15:05:39 | INFO | train_inner | epoch 001:  33607 / 60421 loss=1.542, ppl=2.91, wps=16365.2, ups=4.39, wpb=3727.8, bsz=123.9, num_updates=33600, lr=0.000172516, gnorm=1.597, loss_scale=2, train_wall=23, gb_free=10.9, wall=10220
2021-05-01 15:06:02 | INFO | train_inner | epoch 001:  33707 / 60421 loss=1.57, ppl=2.97, wps=16161.6, ups=4.29, wpb=3763.8, bsz=127.4, num_updates=33700, lr=0.00017226, gnorm=1.818, loss_scale=2, train_wall=23, gb_free=10.8, wall=10243
2021-05-01 15:06:25 | INFO | train_inner | epoch 001:  33807 / 60421 loss=1.456, ppl=2.74, wps=16040.5, ups=4.25, wpb=3773.2, bsz=149.5, num_updates=33800, lr=0.000172005, gnorm=1.52, loss_scale=2, train_wall=23, gb_free=10.8, wall=10266
2021-05-01 15:06:49 | INFO | train_inner | epoch 001:  33907 / 60421 loss=1.499, ppl=2.83, wps=15741.5, ups=4.19, wpb=3758.4, bsz=121.5, num_updates=33900, lr=0.000171751, gnorm=1.64, loss_scale=2, train_wall=24, gb_free=10.8, wall=10290
2021-05-01 15:07:13 | INFO | train_inner | epoch 001:  34007 / 60421 loss=1.54, ppl=2.91, wps=16372.3, ups=4.29, wpb=3812.3, bsz=131, num_updates=34000, lr=0.000171499, gnorm=1.553, loss_scale=2, train_wall=23, gb_free=10.8, wall=10314
2021-05-01 15:07:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:07:13 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:07:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:07:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:07:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:07:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:07:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:07:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:07:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:07:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:07:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:07:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:07:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:07:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:08:19 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.865 | ppl 14.57 | bleu 26.54 | wps 2393.8 | wpb 2024.1 | bsz 97.5 | num_updates 34000 | best_bleu 26.54
2021-05-01 15:08:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 34000 updates
2021-05-01 15:08:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_34000.pt
2021-05-01 15:08:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_34000.pt
2021-05-01 15:08:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_34000.pt (epoch 1 @ 34000 updates, score 26.54) (writing took 14.350875590003852 seconds)
2021-05-01 15:08:56 | INFO | train_inner | epoch 001:  34107 / 60421 loss=1.502, ppl=2.83, wps=3594.6, ups=0.97, wpb=3709.8, bsz=128, num_updates=34100, lr=0.000171247, gnorm=1.629, loss_scale=2, train_wall=22, gb_free=10.7, wall=10417
2021-05-01 15:09:18 | INFO | train_inner | epoch 001:  34207 / 60421 loss=1.544, ppl=2.92, wps=16506.3, ups=4.45, wpb=3707.3, bsz=126.6, num_updates=34200, lr=0.000170996, gnorm=1.95, loss_scale=2, train_wall=22, gb_free=10.8, wall=10439
2021-05-01 15:09:41 | INFO | train_inner | epoch 001:  34307 / 60421 loss=1.67, ppl=3.18, wps=16485.7, ups=4.46, wpb=3695.4, bsz=126.8, num_updates=34300, lr=0.000170747, gnorm=2.172, loss_scale=2, train_wall=22, gb_free=10.9, wall=10462
2021-05-01 15:10:04 | INFO | train_inner | epoch 001:  34407 / 60421 loss=1.477, ppl=2.78, wps=16358.3, ups=4.37, wpb=3746, bsz=130.6, num_updates=34400, lr=0.000170499, gnorm=1.662, loss_scale=2, train_wall=23, gb_free=10.8, wall=10485
2021-05-01 15:10:26 | INFO | train_inner | epoch 001:  34507 / 60421 loss=1.566, ppl=2.96, wps=16373.8, ups=4.37, wpb=3747.3, bsz=138.3, num_updates=34500, lr=0.000170251, gnorm=1.83, loss_scale=2, train_wall=23, gb_free=10.6, wall=10507
2021-05-01 15:10:50 | INFO | train_inner | epoch 001:  34607 / 60421 loss=1.605, ppl=3.04, wps=16171.6, ups=4.34, wpb=3726, bsz=125.5, num_updates=34600, lr=0.000170005, gnorm=1.874, loss_scale=2, train_wall=23, gb_free=10.8, wall=10530
2021-05-01 15:11:13 | INFO | train_inner | epoch 001:  34707 / 60421 loss=1.515, ppl=2.86, wps=16142.4, ups=4.32, wpb=3736.1, bsz=127.4, num_updates=34700, lr=0.00016976, gnorm=1.65, loss_scale=2, train_wall=23, gb_free=11, wall=10554
2021-05-01 15:11:36 | INFO | train_inner | epoch 001:  34807 / 60421 loss=1.53, ppl=2.89, wps=15942.5, ups=4.26, wpb=3746.1, bsz=113.5, num_updates=34800, lr=0.000169516, gnorm=1.769, loss_scale=2, train_wall=23, gb_free=10.7, wall=10577
2021-05-01 15:11:59 | INFO | train_inner | epoch 001:  34907 / 60421 loss=1.554, ppl=2.94, wps=15826.7, ups=4.29, wpb=3692.9, bsz=147.8, num_updates=34900, lr=0.000169273, gnorm=1.933, loss_scale=2, train_wall=23, gb_free=10.9, wall=10600
2021-05-01 15:12:22 | INFO | train_inner | epoch 001:  35007 / 60421 loss=1.573, ppl=2.97, wps=16196.9, ups=4.38, wpb=3695.6, bsz=130.3, num_updates=35000, lr=0.000169031, gnorm=1.874, loss_scale=2, train_wall=23, gb_free=10.8, wall=10623
2021-05-01 15:12:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:12:22 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:12:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:12:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:12:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:12:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:12:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:12:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:12:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:12:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:12:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:12:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:12:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:12:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:13:30 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.868 | ppl 14.6 | bleu 25.98 | wps 2333.4 | wpb 2024.1 | bsz 97.5 | num_updates 35000 | best_bleu 26.54
2021-05-01 15:13:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 35000 updates
2021-05-01 15:13:30 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_35000.pt
2021-05-01 15:13:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_35000.pt
2021-05-01 15:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_35000.pt (epoch 1 @ 35000 updates, score 25.98) (writing took 9.978763271996286 seconds)
2021-05-01 15:14:03 | INFO | train_inner | epoch 001:  35107 / 60421 loss=1.579, ppl=2.99, wps=3763.1, ups=0.99, wpb=3785.2, bsz=132.5, num_updates=35100, lr=0.00016879, gnorm=1.833, loss_scale=2, train_wall=22, gb_free=10.9, wall=10724
2021-05-01 15:14:26 | INFO | train_inner | epoch 001:  35207 / 60421 loss=1.607, ppl=3.05, wps=16458, ups=4.42, wpb=3720.4, bsz=158.7, num_updates=35200, lr=0.00016855, gnorm=1.925, loss_scale=2, train_wall=22, gb_free=10.7, wall=10746
2021-05-01 15:14:48 | INFO | train_inner | epoch 001:  35307 / 60421 loss=1.61, ppl=3.05, wps=16517.9, ups=4.41, wpb=3744.6, bsz=126.2, num_updates=35300, lr=0.000168311, gnorm=1.769, loss_scale=2, train_wall=22, gb_free=10.7, wall=10769
2021-05-01 15:15:11 | INFO | train_inner | epoch 001:  35407 / 60421 loss=1.59, ppl=3.01, wps=16331, ups=4.43, wpb=3683.8, bsz=128.4, num_updates=35400, lr=0.000168073, gnorm=1.965, loss_scale=2, train_wall=22, gb_free=10.7, wall=10792
2021-05-01 15:15:34 | INFO | train_inner | epoch 001:  35507 / 60421 loss=1.456, ppl=2.74, wps=16334.6, ups=4.38, wpb=3730.1, bsz=132.5, num_updates=35500, lr=0.000167836, gnorm=1.841, loss_scale=2, train_wall=23, gb_free=11.8, wall=10815
2021-05-01 15:15:56 | INFO | train_inner | epoch 001:  35607 / 60421 loss=1.589, ppl=3.01, wps=15980.5, ups=4.4, wpb=3628.2, bsz=132.6, num_updates=35600, lr=0.0001676, gnorm=2.429, loss_scale=2, train_wall=23, gb_free=11.2, wall=10837
2021-05-01 15:16:19 | INFO | train_inner | epoch 001:  35707 / 60421 loss=1.583, ppl=3, wps=15943.1, ups=4.31, wpb=3697.8, bsz=124.2, num_updates=35700, lr=0.000167365, gnorm=1.779, loss_scale=2, train_wall=23, gb_free=10.8, wall=10860
2021-05-01 15:16:43 | INFO | train_inner | epoch 001:  35807 / 60421 loss=1.518, ppl=2.86, wps=15862.7, ups=4.22, wpb=3761.7, bsz=131.1, num_updates=35800, lr=0.000167132, gnorm=1.596, loss_scale=2, train_wall=24, gb_free=10.9, wall=10884
2021-05-01 15:17:06 | INFO | train_inner | epoch 001:  35907 / 60421 loss=1.49, ppl=2.81, wps=16058.8, ups=4.29, wpb=3739.7, bsz=134.6, num_updates=35900, lr=0.000166899, gnorm=1.497, loss_scale=2, train_wall=23, gb_free=10.7, wall=10907
2021-05-01 15:17:29 | INFO | train_inner | epoch 001:  36007 / 60421 loss=1.517, ppl=2.86, wps=16302.3, ups=4.43, wpb=3679.4, bsz=135.9, num_updates=36000, lr=0.000166667, gnorm=2.086, loss_scale=2, train_wall=22, gb_free=11, wall=10930
2021-05-01 15:17:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:17:29 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:17:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:17:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:17:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:17:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:17:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:17:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:17:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:17:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:17:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:18:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.907 | ppl 15 | bleu 22.67 | wps 2647.2 | wpb 2024.1 | bsz 97.5 | num_updates 36000 | best_bleu 26.54
2021-05-01 15:18:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 36000 updates
2021-05-01 15:18:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_36000.pt
2021-05-01 15:18:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_36000.pt
2021-05-01 15:18:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_36000.pt (epoch 1 @ 36000 updates, score 22.67) (writing took 8.229209022996656 seconds)
2021-05-01 15:19:00 | INFO | train_inner | epoch 001:  36107 / 60421 loss=1.512, ppl=2.85, wps=4137.8, ups=1.1, wpb=3763.7, bsz=132.5, num_updates=36100, lr=0.000166436, gnorm=1.607, loss_scale=2, train_wall=23, gb_free=10.7, wall=11021
2021-05-01 15:19:23 | INFO | train_inner | epoch 001:  36207 / 60421 loss=1.503, ppl=2.83, wps=16620.1, ups=4.43, wpb=3752, bsz=118, num_updates=36200, lr=0.000166206, gnorm=1.697, loss_scale=2, train_wall=22, gb_free=10.7, wall=11044
2021-05-01 15:19:45 | INFO | train_inner | epoch 001:  36307 / 60421 loss=1.504, ppl=2.84, wps=16435.9, ups=4.45, wpb=3690, bsz=130.9, num_updates=36300, lr=0.000165977, gnorm=1.804, loss_scale=2, train_wall=22, gb_free=11.2, wall=11066
2021-05-01 15:20:08 | INFO | train_inner | epoch 001:  36407 / 60421 loss=1.469, ppl=2.77, wps=16375.8, ups=4.43, wpb=3693.5, bsz=125.8, num_updates=36400, lr=0.000165748, gnorm=1.704, loss_scale=4, train_wall=22, gb_free=10.8, wall=11089
2021-05-01 15:20:30 | INFO | train_inner | epoch 001:  36507 / 60421 loss=1.491, ppl=2.81, wps=16383.3, ups=4.37, wpb=3746.8, bsz=135.8, num_updates=36500, lr=0.000165521, gnorm=1.664, loss_scale=4, train_wall=23, gb_free=11.1, wall=11111
2021-05-01 15:20:54 | INFO | train_inner | epoch 001:  36607 / 60421 loss=1.636, ppl=3.11, wps=16249.5, ups=4.31, wpb=3766.7, bsz=134.4, num_updates=36600, lr=0.000165295, gnorm=1.858, loss_scale=4, train_wall=23, gb_free=10.8, wall=11135
2021-05-01 15:21:17 | INFO | train_inner | epoch 001:  36707 / 60421 loss=1.576, ppl=2.98, wps=16013.3, ups=4.3, wpb=3722.6, bsz=119.2, num_updates=36700, lr=0.00016507, gnorm=1.853, loss_scale=4, train_wall=23, gb_free=10.7, wall=11158
2021-05-01 15:21:41 | INFO | train_inner | epoch 001:  36807 / 60421 loss=1.538, ppl=2.9, wps=15627.6, ups=4.21, wpb=3712.8, bsz=132.2, num_updates=36800, lr=0.000164845, gnorm=1.907, loss_scale=4, train_wall=24, gb_free=11, wall=11182
2021-05-01 15:22:04 | INFO | train_inner | epoch 001:  36907 / 60421 loss=1.559, ppl=2.95, wps=16038.2, ups=4.28, wpb=3751.6, bsz=135.6, num_updates=36900, lr=0.000164622, gnorm=1.766, loss_scale=4, train_wall=23, gb_free=10.6, wall=11205
2021-05-01 15:22:27 | INFO | train_inner | epoch 001:  37007 / 60421 loss=1.443, ppl=2.72, wps=16551, ups=4.38, wpb=3782.6, bsz=128.2, num_updates=37000, lr=0.000164399, gnorm=1.387, loss_scale=4, train_wall=23, gb_free=10.8, wall=11228
2021-05-01 15:22:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:22:27 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:22:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:22:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:22:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:22:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:22:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:22:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:22:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:22:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:22:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:22:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:22:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:22:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:23:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.892 | ppl 14.85 | bleu 26.65 | wps 2429.7 | wpb 2024.1 | bsz 97.5 | num_updates 37000 | best_bleu 26.65
2021-05-01 15:23:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 37000 updates
2021-05-01 15:23:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_37000.pt
2021-05-01 15:23:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_37000.pt
2021-05-01 15:23:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_37000.pt (epoch 1 @ 37000 updates, score 26.65) (writing took 14.3441565930043 seconds)
2021-05-01 15:24:09 | INFO | train_inner | epoch 001:  37107 / 60421 loss=1.655, ppl=3.15, wps=3647.1, ups=0.98, wpb=3719.1, bsz=113.4, num_updates=37100, lr=0.000164177, gnorm=2.145, loss_scale=4, train_wall=22, gb_free=10.9, wall=11330
2021-05-01 15:24:31 | INFO | train_inner | epoch 001:  37207 / 60421 loss=1.529, ppl=2.89, wps=16711, ups=4.43, wpb=3776.3, bsz=144.8, num_updates=37200, lr=0.000163956, gnorm=1.792, loss_scale=4, train_wall=22, gb_free=10.7, wall=11352
2021-05-01 15:24:54 | INFO | train_inner | epoch 001:  37307 / 60421 loss=1.531, ppl=2.89, wps=16712.1, ups=4.48, wpb=3731, bsz=117.4, num_updates=37300, lr=0.000163737, gnorm=1.678, loss_scale=4, train_wall=22, gb_free=10.9, wall=11375
2021-05-01 15:25:16 | INFO | train_inner | epoch 001:  37407 / 60421 loss=1.45, ppl=2.73, wps=16627.6, ups=4.45, wpb=3736.8, bsz=134.6, num_updates=37400, lr=0.000163517, gnorm=1.631, loss_scale=4, train_wall=22, gb_free=10.7, wall=11397
2021-05-01 15:25:39 | INFO | train_inner | epoch 001:  37507 / 60421 loss=1.396, ppl=2.63, wps=16692.1, ups=4.42, wpb=3779.9, bsz=153.3, num_updates=37500, lr=0.000163299, gnorm=1.47, loss_scale=4, train_wall=22, gb_free=10.8, wall=11420
2021-05-01 15:26:01 | INFO | train_inner | epoch 001:  37607 / 60421 loss=1.554, ppl=2.94, wps=16580, ups=4.49, wpb=3688.7, bsz=125.7, num_updates=37600, lr=0.000163082, gnorm=1.973, loss_scale=4, train_wall=22, gb_free=10.7, wall=11442
2021-05-01 15:26:24 | INFO | train_inner | epoch 001:  37707 / 60421 loss=1.51, ppl=2.85, wps=16639.9, ups=4.46, wpb=3732.2, bsz=119.8, num_updates=37700, lr=0.000162866, gnorm=1.618, loss_scale=4, train_wall=22, gb_free=10.8, wall=11465
2021-05-01 15:26:46 | INFO | train_inner | epoch 001:  37807 / 60421 loss=1.48, ppl=2.79, wps=16469.2, ups=4.43, wpb=3716.1, bsz=144.6, num_updates=37800, lr=0.00016265, gnorm=1.758, loss_scale=4, train_wall=22, gb_free=10.8, wall=11487
2021-05-01 15:27:09 | INFO | train_inner | epoch 001:  37907 / 60421 loss=1.541, ppl=2.91, wps=16553, ups=4.41, wpb=3751, bsz=119.2, num_updates=37900, lr=0.000162435, gnorm=1.841, loss_scale=4, train_wall=22, gb_free=10.8, wall=11510
2021-05-01 15:27:31 | INFO | train_inner | epoch 001:  38007 / 60421 loss=1.514, ppl=2.86, wps=16698.2, ups=4.44, wpb=3764.1, bsz=132.6, num_updates=38000, lr=0.000162221, gnorm=1.665, loss_scale=4, train_wall=22, gb_free=10.9, wall=11532
2021-05-01 15:27:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:27:31 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:27:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:27:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:27:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:27:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:27:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:27:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:27:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:27:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:27:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:27:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:27:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:27:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:28:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.836 | ppl 14.28 | bleu 26.33 | wps 2354.5 | wpb 2024.1 | bsz 97.5 | num_updates 38000 | best_bleu 26.65
2021-05-01 15:28:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 38000 updates
2021-05-01 15:28:39 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_38000.pt
2021-05-01 15:28:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_38000.pt
2021-05-01 15:28:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_38000.pt (epoch 1 @ 38000 updates, score 26.33) (writing took 8.25378480800282 seconds)
2021-05-01 15:29:10 | INFO | train_inner | epoch 001:  38107 / 60421 loss=1.502, ppl=2.83, wps=3778.9, ups=1.02, wpb=3714.1, bsz=133, num_updates=38100, lr=0.000162008, gnorm=1.688, loss_scale=4, train_wall=22, gb_free=10.9, wall=11631
2021-05-01 15:29:32 | INFO | train_inner | epoch 001:  38207 / 60421 loss=1.496, ppl=2.82, wps=16628.4, ups=4.39, wpb=3786.3, bsz=104.1, num_updates=38200, lr=0.000161796, gnorm=1.437, loss_scale=4, train_wall=23, gb_free=10.8, wall=11653
2021-05-01 15:29:55 | INFO | train_inner | epoch 001:  38307 / 60421 loss=1.67, ppl=3.18, wps=16469.4, ups=4.42, wpb=3727.1, bsz=118.1, num_updates=38300, lr=0.000161585, gnorm=1.971, loss_scale=4, train_wall=22, gb_free=10.9, wall=11676
2021-05-01 15:30:18 | INFO | train_inner | epoch 001:  38407 / 60421 loss=1.515, ppl=2.86, wps=16231.7, ups=4.43, wpb=3665, bsz=114.4, num_updates=38400, lr=0.000161374, gnorm=1.757, loss_scale=4, train_wall=22, gb_free=11, wall=11699
2021-05-01 15:30:41 | INFO | train_inner | epoch 001:  38507 / 60421 loss=1.505, ppl=2.84, wps=16430.9, ups=4.32, wpb=3802.5, bsz=151.2, num_updates=38500, lr=0.000161165, gnorm=1.618, loss_scale=4, train_wall=23, gb_free=10.7, wall=11722
2021-05-01 15:31:04 | INFO | train_inner | epoch 001:  38607 / 60421 loss=1.468, ppl=2.77, wps=16001.9, ups=4.33, wpb=3696, bsz=159, num_updates=38600, lr=0.000160956, gnorm=1.971, loss_scale=4, train_wall=23, gb_free=10.9, wall=11745
2021-05-01 15:31:27 | INFO | train_inner | epoch 001:  38707 / 60421 loss=1.434, ppl=2.7, wps=16028.9, ups=4.25, wpb=3775.2, bsz=128.4, num_updates=38700, lr=0.000160748, gnorm=1.693, loss_scale=4, train_wall=23, gb_free=10.7, wall=11768
2021-05-01 15:31:51 | INFO | train_inner | epoch 001:  38807 / 60421 loss=1.518, ppl=2.86, wps=15745.4, ups=4.21, wpb=3737.6, bsz=140.8, num_updates=38800, lr=0.00016054, gnorm=1.812, loss_scale=4, train_wall=24, gb_free=10.9, wall=11792
2021-05-01 15:32:14 | INFO | train_inner | epoch 001:  38907 / 60421 loss=1.46, ppl=2.75, wps=16010.8, ups=4.38, wpb=3658.9, bsz=137.8, num_updates=38900, lr=0.000160334, gnorm=1.823, loss_scale=4, train_wall=23, gb_free=11, wall=11815
2021-05-01 15:32:37 | INFO | train_inner | epoch 001:  39007 / 60421 loss=1.514, ppl=2.86, wps=16371.4, ups=4.4, wpb=3724.5, bsz=131.4, num_updates=39000, lr=0.000160128, gnorm=1.981, loss_scale=4, train_wall=23, gb_free=10.7, wall=11838
2021-05-01 15:32:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:32:37 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:32:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:32:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:32:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:32:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:32:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:32:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:32:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:32:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:32:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:32:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:32:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:32:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:33:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.939 | ppl 15.33 | bleu 26.37 | wps 2428.1 | wpb 2024.1 | bsz 97.5 | num_updates 39000 | best_bleu 26.65
2021-05-01 15:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 39000 updates
2021-05-01 15:33:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_39000.pt
2021-05-01 15:33:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_39000.pt
2021-05-01 15:33:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_39000.pt (epoch 1 @ 39000 updates, score 26.37) (writing took 7.711908919001871 seconds)
2021-05-01 15:34:13 | INFO | train_inner | epoch 001:  39107 / 60421 loss=1.427, ppl=2.69, wps=3920.1, ups=1.04, wpb=3760.1, bsz=148.6, num_updates=39100, lr=0.000159923, gnorm=1.664, loss_scale=4, train_wall=23, gb_free=10.8, wall=11934
2021-05-01 15:34:35 | INFO | train_inner | epoch 001:  39207 / 60421 loss=1.418, ppl=2.67, wps=16564.3, ups=4.44, wpb=3728, bsz=123.8, num_updates=39200, lr=0.000159719, gnorm=1.647, loss_scale=4, train_wall=22, gb_free=10.8, wall=11956
2021-05-01 15:34:58 | INFO | train_inner | epoch 001:  39307 / 60421 loss=1.455, ppl=2.74, wps=16493.6, ups=4.42, wpb=3730.6, bsz=129.3, num_updates=39300, lr=0.000159516, gnorm=1.686, loss_scale=4, train_wall=22, gb_free=11.2, wall=11979
2021-05-01 15:35:21 | INFO | train_inner | epoch 001:  39407 / 60421 loss=1.452, ppl=2.74, wps=16585.9, ups=4.33, wpb=3829.3, bsz=135.8, num_updates=39400, lr=0.000159313, gnorm=1.531, loss_scale=4, train_wall=23, gb_free=10.8, wall=12002
2021-05-01 15:35:43 | INFO | train_inner | epoch 001:  39507 / 60421 loss=1.542, ppl=2.91, wps=16081.9, ups=4.43, wpb=3627.4, bsz=118, num_updates=39500, lr=0.000159111, gnorm=2.085, loss_scale=4, train_wall=22, gb_free=10.8, wall=12024
2021-05-01 15:36:07 | INFO | train_inner | epoch 001:  39607 / 60421 loss=1.554, ppl=2.94, wps=15965.8, ups=4.33, wpb=3689.2, bsz=127.9, num_updates=39600, lr=0.00015891, gnorm=1.85, loss_scale=4, train_wall=23, gb_free=10.8, wall=12048
2021-05-01 15:36:30 | INFO | train_inner | epoch 001:  39707 / 60421 loss=1.576, ppl=2.98, wps=15944, ups=4.27, wpb=3733.1, bsz=133.3, num_updates=39700, lr=0.00015871, gnorm=1.994, loss_scale=4, train_wall=23, gb_free=10.6, wall=12071
2021-05-01 15:36:54 | INFO | train_inner | epoch 001:  39807 / 60421 loss=1.434, ppl=2.7, wps=16049.3, ups=4.23, wpb=3789.8, bsz=122.2, num_updates=39800, lr=0.000158511, gnorm=1.414, loss_scale=4, train_wall=23, gb_free=11.4, wall=12095
2021-05-01 15:37:17 | INFO | train_inner | epoch 001:  39907 / 60421 loss=1.515, ppl=2.86, wps=16289.1, ups=4.34, wpb=3754.6, bsz=138, num_updates=39900, lr=0.000158312, gnorm=1.859, loss_scale=4, train_wall=23, gb_free=10.8, wall=12118
2021-05-01 15:37:39 | INFO | train_inner | epoch 001:  40007 / 60421 loss=1.487, ppl=2.8, wps=16510.6, ups=4.43, wpb=3726.3, bsz=122.2, num_updates=40000, lr=0.000158114, gnorm=1.831, loss_scale=4, train_wall=22, gb_free=10.8, wall=12140
2021-05-01 15:37:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:37:39 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:37:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:37:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:37:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:37:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:37:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:37:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:37:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:37:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:37:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:37:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:37:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:37:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:37:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:37:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:37:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:38:44 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.921 | ppl 15.14 | bleu 26.59 | wps 2442.6 | wpb 2024.1 | bsz 97.5 | num_updates 40000 | best_bleu 26.65
2021-05-01 15:38:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 40000 updates
2021-05-01 15:38:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_40000.pt
2021-05-01 15:38:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_40000.pt
2021-05-01 15:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_40000.pt (epoch 1 @ 40000 updates, score 26.59) (writing took 7.691694496003038 seconds)
2021-05-01 15:39:14 | INFO | train_inner | epoch 001:  40107 / 60421 loss=1.556, ppl=2.94, wps=3834, ups=1.05, wpb=3643, bsz=114.2, num_updates=40100, lr=0.000157917, gnorm=2.329, loss_scale=4, train_wall=22, gb_free=10.9, wall=12235
2021-05-01 15:39:37 | INFO | train_inner | epoch 001:  40207 / 60421 loss=1.464, ppl=2.76, wps=16603, ups=4.43, wpb=3747.9, bsz=131.4, num_updates=40200, lr=0.00015772, gnorm=1.589, loss_scale=4, train_wall=22, gb_free=10.8, wall=12258
2021-05-01 15:39:59 | INFO | train_inner | epoch 001:  40307 / 60421 loss=1.461, ppl=2.75, wps=16322.3, ups=4.45, wpb=3667.5, bsz=127.5, num_updates=40300, lr=0.000157524, gnorm=1.773, loss_scale=4, train_wall=22, gb_free=10.8, wall=12280
2021-05-01 15:40:22 | INFO | train_inner | epoch 001:  40407 / 60421 loss=1.43, ppl=2.69, wps=16400.2, ups=4.35, wpb=3774.2, bsz=124.2, num_updates=40400, lr=0.000157329, gnorm=1.545, loss_scale=4, train_wall=23, gb_free=11, wall=12303
2021-05-01 15:40:45 | INFO | train_inner | epoch 001:  40507 / 60421 loss=1.497, ppl=2.82, wps=16325.1, ups=4.33, wpb=3768.8, bsz=146.7, num_updates=40500, lr=0.000157135, gnorm=1.838, loss_scale=4, train_wall=23, gb_free=10.7, wall=12326
2021-05-01 15:41:09 | INFO | train_inner | epoch 001:  40607 / 60421 loss=1.402, ppl=2.64, wps=15947.4, ups=4.31, wpb=3701.7, bsz=144.2, num_updates=40600, lr=0.000156941, gnorm=1.862, loss_scale=4, train_wall=23, gb_free=10.8, wall=12350
2021-05-01 15:41:32 | INFO | train_inner | epoch 001:  40707 / 60421 loss=1.488, ppl=2.8, wps=15876.1, ups=4.28, wpb=3708.7, bsz=139.6, num_updates=40700, lr=0.000156748, gnorm=1.878, loss_scale=4, train_wall=23, gb_free=10.8, wall=12373
2021-05-01 15:41:56 | INFO | train_inner | epoch 001:  40807 / 60421 loss=1.53, ppl=2.89, wps=15918.8, ups=4.23, wpb=3763.1, bsz=128.8, num_updates=40800, lr=0.000156556, gnorm=2.037, loss_scale=4, train_wall=23, gb_free=10.8, wall=12397
2021-05-01 15:42:19 | INFO | train_inner | epoch 001:  40907 / 60421 loss=1.389, ppl=2.62, wps=16104.6, ups=4.35, wpb=3702.1, bsz=138.2, num_updates=40900, lr=0.000156365, gnorm=1.498, loss_scale=4, train_wall=23, gb_free=10.8, wall=12420
2021-05-01 15:42:41 | INFO | train_inner | epoch 001:  41007 / 60421 loss=1.478, ppl=2.79, wps=16406, ups=4.41, wpb=3718.1, bsz=116.4, num_updates=41000, lr=0.000156174, gnorm=1.77, loss_scale=4, train_wall=22, gb_free=10.9, wall=12442
2021-05-01 15:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:42:41 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:42:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:42:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:42:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:42:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:42:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:42:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:42:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:42:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:42:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:42:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:42:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:42:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:43:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:43:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:43:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:43:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.96 | ppl 15.56 | bleu 26.64 | wps 2397.6 | wpb 2024.1 | bsz 97.5 | num_updates 41000 | best_bleu 26.65
2021-05-01 15:43:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 41000 updates
2021-05-01 15:43:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_41000.pt
2021-05-01 15:43:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_41000.pt
2021-05-01 15:43:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_41000.pt (epoch 1 @ 41000 updates, score 26.64) (writing took 10.060299983000732 seconds)
2021-05-01 15:44:20 | INFO | train_inner | epoch 001:  41107 / 60421 loss=1.447, ppl=2.73, wps=3756.2, ups=1.01, wpb=3709.3, bsz=137.9, num_updates=41100, lr=0.000155984, gnorm=1.994, loss_scale=4, train_wall=22, gb_free=10.9, wall=12541
2021-05-01 15:44:42 | INFO | train_inner | epoch 001:  41207 / 60421 loss=1.359, ppl=2.57, wps=16339.9, ups=4.46, wpb=3666.4, bsz=137.4, num_updates=41200, lr=0.000155794, gnorm=1.688, loss_scale=4, train_wall=22, gb_free=10.9, wall=12563
2021-05-01 15:45:05 | INFO | train_inner | epoch 001:  41307 / 60421 loss=1.59, ppl=3.01, wps=16521.6, ups=4.38, wpb=3775, bsz=130.2, num_updates=41300, lr=0.000155606, gnorm=1.947, loss_scale=4, train_wall=23, gb_free=11.1, wall=12586
2021-05-01 15:45:28 | INFO | train_inner | epoch 001:  41407 / 60421 loss=1.39, ppl=2.62, wps=16246.5, ups=4.36, wpb=3726.9, bsz=152.1, num_updates=41400, lr=0.000155417, gnorm=1.536, loss_scale=4, train_wall=23, gb_free=10.9, wall=12609
2021-05-01 15:45:51 | INFO | train_inner | epoch 001:  41507 / 60421 loss=1.449, ppl=2.73, wps=16285.6, ups=4.32, wpb=3770.5, bsz=123, num_updates=41500, lr=0.00015523, gnorm=1.921, loss_scale=4, train_wall=23, gb_free=10.8, wall=12632
2021-05-01 15:46:15 | INFO | train_inner | epoch 001:  41607 / 60421 loss=1.454, ppl=2.74, wps=16105.3, ups=4.28, wpb=3760.1, bsz=139.3, num_updates=41600, lr=0.000155043, gnorm=1.68, loss_scale=4, train_wall=23, gb_free=10.9, wall=12656
2021-05-01 15:46:38 | INFO | train_inner | epoch 001:  41707 / 60421 loss=1.348, ppl=2.55, wps=15791.6, ups=4.23, wpb=3736.6, bsz=139.6, num_updates=41700, lr=0.000154857, gnorm=1.394, loss_scale=4, train_wall=23, gb_free=10.8, wall=12679
2021-05-01 15:47:02 | INFO | train_inner | epoch 001:  41807 / 60421 loss=1.455, ppl=2.74, wps=16084.6, ups=4.25, wpb=3783.2, bsz=145.3, num_updates=41800, lr=0.000154672, gnorm=1.609, loss_scale=4, train_wall=23, gb_free=10.9, wall=12703
2021-05-01 15:47:25 | INFO | train_inner | epoch 001:  41907 / 60421 loss=1.36, ppl=2.57, wps=16273.6, ups=4.37, wpb=3723.1, bsz=144.8, num_updates=41900, lr=0.000154487, gnorm=1.563, loss_scale=4, train_wall=23, gb_free=10.8, wall=12726
2021-05-01 15:47:48 | INFO | train_inner | epoch 001:  42007 / 60421 loss=1.393, ppl=2.63, wps=16581.4, ups=4.36, wpb=3803.8, bsz=137.4, num_updates=42000, lr=0.000154303, gnorm=1.454, loss_scale=4, train_wall=23, gb_free=10.9, wall=12749
2021-05-01 15:47:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:47:48 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:47:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:47:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:47:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:48:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:48:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:48:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:48:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:48:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:48:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:48:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:48:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:48:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:48:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.906 | ppl 14.99 | bleu 26.67 | wps 2454.4 | wpb 2024.1 | bsz 97.5 | num_updates 42000 | best_bleu 26.67
2021-05-01 15:48:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 42000 updates
2021-05-01 15:48:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_42000.pt
2021-05-01 15:48:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_42000.pt
2021-05-01 15:49:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_42000.pt (epoch 1 @ 42000 updates, score 26.67) (writing took 14.033905488999153 seconds)
2021-05-01 15:49:29 | INFO | train_inner | epoch 001:  42107 / 60421 loss=1.454, ppl=2.74, wps=3676.8, ups=0.99, wpb=3725.1, bsz=130.6, num_updates=42100, lr=0.00015412, gnorm=1.783, loss_scale=4, train_wall=22, gb_free=11, wall=12850
2021-05-01 15:49:52 | INFO | train_inner | epoch 001:  42207 / 60421 loss=1.367, ppl=2.58, wps=16582.5, ups=4.37, wpb=3790.5, bsz=165.4, num_updates=42200, lr=0.000153937, gnorm=1.546, loss_scale=4, train_wall=23, gb_free=10.9, wall=12873
2021-05-01 15:50:15 | INFO | train_inner | epoch 001:  42307 / 60421 loss=1.416, ppl=2.67, wps=16433.3, ups=4.38, wpb=3749.6, bsz=122.1, num_updates=42300, lr=0.000153755, gnorm=1.598, loss_scale=4, train_wall=23, gb_free=10.8, wall=12896
2021-05-01 15:50:38 | INFO | train_inner | epoch 001:  42407 / 60421 loss=1.377, ppl=2.6, wps=16352.6, ups=4.37, wpb=3739.7, bsz=139.3, num_updates=42400, lr=0.000153574, gnorm=1.674, loss_scale=4, train_wall=23, gb_free=10.7, wall=12919
2021-05-01 15:51:01 | INFO | train_inner | epoch 001:  42507 / 60421 loss=1.397, ppl=2.63, wps=16275.3, ups=4.33, wpb=3758.4, bsz=122.2, num_updates=42500, lr=0.000153393, gnorm=1.512, loss_scale=4, train_wall=23, gb_free=11.1, wall=12942
2021-05-01 15:51:24 | INFO | train_inner | epoch 001:  42607 / 60421 loss=1.39, ppl=2.62, wps=16099.4, ups=4.3, wpb=3746.2, bsz=108.5, num_updates=42600, lr=0.000153213, gnorm=1.396, loss_scale=4, train_wall=23, gb_free=10.7, wall=12965
2021-05-01 15:51:47 | INFO | train_inner | epoch 001:  42707 / 60421 loss=1.329, ppl=2.51, wps=15746.2, ups=4.26, wpb=3699.8, bsz=135.8, num_updates=42700, lr=0.000153033, gnorm=1.598, loss_scale=4, train_wall=23, gb_free=10.7, wall=12988
2021-05-01 15:52:10 | INFO | train_inner | epoch 001:  42807 / 60421 loss=1.358, ppl=2.56, wps=16140.4, ups=4.38, wpb=3684.8, bsz=129.9, num_updates=42800, lr=0.000152854, gnorm=1.672, loss_scale=4, train_wall=23, gb_free=10.8, wall=13011
2021-05-01 15:52:33 | INFO | train_inner | epoch 001:  42907 / 60421 loss=1.422, ppl=2.68, wps=16414.8, ups=4.39, wpb=3739.7, bsz=122.3, num_updates=42900, lr=0.000152676, gnorm=1.588, loss_scale=4, train_wall=23, gb_free=10.7, wall=13034
2021-05-01 15:52:56 | INFO | train_inner | epoch 001:  43007 / 60421 loss=1.373, ppl=2.59, wps=16489.7, ups=4.37, wpb=3772.1, bsz=135.6, num_updates=43000, lr=0.000152499, gnorm=1.51, loss_scale=4, train_wall=23, gb_free=11.3, wall=13057
2021-05-01 15:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:52:56 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:53:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:53:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:53:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:53:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:53:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:53:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:53:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:53:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:53:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:53:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:53:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:53:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:53:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:53:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:53:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:54:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.014 | ppl 16.15 | bleu 26.94 | wps 2363.7 | wpb 2024.1 | bsz 97.5 | num_updates 43000 | best_bleu 26.94
2021-05-01 15:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 43000 updates
2021-05-01 15:54:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_43000.pt
2021-05-01 15:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_43000.pt
2021-05-01 15:54:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_43000.pt (epoch 1 @ 43000 updates, score 26.94) (writing took 14.362954035997973 seconds)
2021-05-01 15:54:40 | INFO | train_inner | epoch 001:  43107 / 60421 loss=1.401, ppl=2.64, wps=3528.3, ups=0.96, wpb=3668.9, bsz=126.9, num_updates=43100, lr=0.000152322, gnorm=1.679, loss_scale=4, train_wall=22, gb_free=11.2, wall=13161
2021-05-01 15:55:02 | INFO | train_inner | epoch 001:  43207 / 60421 loss=1.445, ppl=2.72, wps=16383.9, ups=4.42, wpb=3704.3, bsz=129.1, num_updates=43200, lr=0.000152145, gnorm=1.763, loss_scale=4, train_wall=22, gb_free=11.6, wall=13183
2021-05-01 15:55:25 | INFO | train_inner | epoch 001:  43307 / 60421 loss=1.454, ppl=2.74, wps=16509.1, ups=4.38, wpb=3767.7, bsz=138.8, num_updates=43300, lr=0.000151969, gnorm=1.637, loss_scale=4, train_wall=23, gb_free=10.8, wall=13206
2021-05-01 15:55:48 | INFO | train_inner | epoch 001:  43407 / 60421 loss=1.393, ppl=2.63, wps=16087.1, ups=4.37, wpb=3680.9, bsz=141.1, num_updates=43400, lr=0.000151794, gnorm=2.135, loss_scale=4, train_wall=23, gb_free=11.1, wall=13229
2021-05-01 15:56:11 | INFO | train_inner | epoch 001:  43507 / 60421 loss=1.345, ppl=2.54, wps=16143.3, ups=4.3, wpb=3750.5, bsz=125.3, num_updates=43500, lr=0.00015162, gnorm=1.477, loss_scale=4, train_wall=23, gb_free=10.7, wall=13252
2021-05-01 15:56:35 | INFO | train_inner | epoch 001:  43607 / 60421 loss=1.352, ppl=2.55, wps=15888.9, ups=4.24, wpb=3743.3, bsz=160.4, num_updates=43600, lr=0.000151446, gnorm=1.592, loss_scale=4, train_wall=23, gb_free=10.7, wall=13276
2021-05-01 15:56:59 | INFO | train_inner | epoch 001:  43707 / 60421 loss=1.372, ppl=2.59, wps=15907.6, ups=4.24, wpb=3750.2, bsz=153.6, num_updates=43700, lr=0.000151272, gnorm=1.642, loss_scale=4, train_wall=23, gb_free=10.8, wall=13300
2021-05-01 15:57:22 | INFO | train_inner | epoch 001:  43807 / 60421 loss=1.39, ppl=2.62, wps=16291, ups=4.35, wpb=3748.7, bsz=129.6, num_updates=43800, lr=0.000151099, gnorm=1.635, loss_scale=4, train_wall=23, gb_free=10.9, wall=13323
2021-05-01 15:57:44 | INFO | train_inner | epoch 001:  43907 / 60421 loss=1.45, ppl=2.73, wps=16504.2, ups=4.45, wpb=3711.4, bsz=118.4, num_updates=43900, lr=0.000150927, gnorm=1.68, loss_scale=4, train_wall=22, gb_free=10.7, wall=13345
2021-05-01 15:58:07 | INFO | train_inner | epoch 001:  44007 / 60421 loss=1.385, ppl=2.61, wps=16457.8, ups=4.36, wpb=3771.8, bsz=129.9, num_updates=44000, lr=0.000150756, gnorm=1.533, loss_scale=4, train_wall=23, gb_free=10.7, wall=13368
2021-05-01 15:58:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 15:58:07 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 15:58:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:58:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:58:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:58:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 15:58:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 15:58:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 15:59:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.979 | ppl 15.77 | bleu 27.03 | wps 2402.3 | wpb 2024.1 | bsz 97.5 | num_updates 44000 | best_bleu 27.03
2021-05-01 15:59:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 44000 updates
2021-05-01 15:59:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_44000.pt
2021-05-01 15:59:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_44000.pt
2021-05-01 15:59:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_44000.pt (epoch 1 @ 44000 updates, score 27.03) (writing took 14.352953373003402 seconds)
2021-05-01 15:59:50 | INFO | train_inner | epoch 001:  44107 / 60421 loss=1.316, ppl=2.49, wps=3601.2, ups=0.97, wpb=3716, bsz=156.3, num_updates=44100, lr=0.000150585, gnorm=1.546, loss_scale=4, train_wall=23, gb_free=11, wall=13471
2021-05-01 16:00:13 | INFO | train_inner | epoch 001:  44207 / 60421 loss=1.365, ppl=2.58, wps=16437.6, ups=4.4, wpb=3737.3, bsz=131.9, num_updates=44200, lr=0.000150414, gnorm=1.401, loss_scale=4, train_wall=23, gb_free=11.1, wall=13494
2021-05-01 16:00:36 | INFO | train_inner | epoch 001:  44307 / 60421 loss=1.381, ppl=2.6, wps=16365.7, ups=4.36, wpb=3752.4, bsz=125.4, num_updates=44300, lr=0.000150244, gnorm=1.581, loss_scale=4, train_wall=23, gb_free=10.7, wall=13517
2021-05-01 16:00:59 | INFO | train_inner | epoch 001:  44407 / 60421 loss=1.361, ppl=2.57, wps=16190.4, ups=4.36, wpb=3712.7, bsz=125.8, num_updates=44400, lr=0.000150075, gnorm=1.591, loss_scale=4, train_wall=23, gb_free=11.1, wall=13540
2021-05-01 16:01:22 | INFO | train_inner | epoch 001:  44507 / 60421 loss=1.532, ppl=2.89, wps=16232.9, ups=4.32, wpb=3760.2, bsz=125.5, num_updates=44500, lr=0.000149906, gnorm=1.865, loss_scale=4, train_wall=23, gb_free=11.6, wall=13563
2021-05-01 16:01:45 | INFO | train_inner | epoch 001:  44607 / 60421 loss=1.448, ppl=2.73, wps=15925.6, ups=4.26, wpb=3734.2, bsz=128.5, num_updates=44600, lr=0.000149738, gnorm=1.761, loss_scale=4, train_wall=23, gb_free=10.9, wall=13586
2021-05-01 16:02:08 | INFO | train_inner | epoch 001:  44707 / 60421 loss=1.43, ppl=2.69, wps=16256.1, ups=4.35, wpb=3737.8, bsz=123.5, num_updates=44700, lr=0.000149571, gnorm=1.684, loss_scale=4, train_wall=23, gb_free=10.8, wall=13609
2021-05-01 16:02:31 | INFO | train_inner | epoch 001:  44807 / 60421 loss=1.28, ppl=2.43, wps=16204.5, ups=4.37, wpb=3712.1, bsz=173.1, num_updates=44800, lr=0.000149404, gnorm=1.695, loss_scale=4, train_wall=23, gb_free=10.8, wall=13632
2021-05-01 16:02:54 | INFO | train_inner | epoch 001:  44907 / 60421 loss=1.437, ppl=2.71, wps=16802.7, ups=4.35, wpb=3866.9, bsz=128.6, num_updates=44900, lr=0.000149237, gnorm=1.506, loss_scale=4, train_wall=23, gb_free=10.9, wall=13655
2021-05-01 16:03:17 | INFO | train_inner | epoch 001:  45007 / 60421 loss=1.371, ppl=2.59, wps=16540.7, ups=4.42, wpb=3743.6, bsz=121.3, num_updates=45000, lr=0.000149071, gnorm=1.61, loss_scale=4, train_wall=22, gb_free=11, wall=13678
2021-05-01 16:03:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:03:17 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:03:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:03:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:03:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:03:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:03:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:03:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:03:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:03:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:03:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:04:22 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.015 | ppl 16.17 | bleu 26.82 | wps 2446.8 | wpb 2024.1 | bsz 97.5 | num_updates 45000 | best_bleu 27.03
2021-05-01 16:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 45000 updates
2021-05-01 16:04:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_45000.pt
2021-05-01 16:04:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_45000.pt
2021-05-01 16:04:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_45000.pt (epoch 1 @ 45000 updates, score 26.82) (writing took 7.655389237996133 seconds)
2021-05-01 16:04:52 | INFO | train_inner | epoch 001:  45107 / 60421 loss=1.384, ppl=2.61, wps=3953.2, ups=1.05, wpb=3774, bsz=141.2, num_updates=45100, lr=0.000148906, gnorm=1.653, loss_scale=4, train_wall=23, gb_free=10.7, wall=13773
2021-05-01 16:05:15 | INFO | train_inner | epoch 001:  45207 / 60421 loss=1.311, ppl=2.48, wps=16452.9, ups=4.37, wpb=3768.9, bsz=153.2, num_updates=45200, lr=0.000148741, gnorm=1.521, loss_scale=4, train_wall=23, gb_free=11.1, wall=13796
2021-05-01 16:05:38 | INFO | train_inner | epoch 001:  45307 / 60421 loss=1.414, ppl=2.67, wps=16192.5, ups=4.39, wpb=3690.5, bsz=114.2, num_updates=45300, lr=0.000148577, gnorm=1.863, loss_scale=4, train_wall=23, gb_free=11, wall=13819
2021-05-01 16:06:01 | INFO | train_inner | epoch 001:  45407 / 60421 loss=1.407, ppl=2.65, wps=16071.9, ups=4.33, wpb=3715.8, bsz=152.9, num_updates=45400, lr=0.000148413, gnorm=1.724, loss_scale=4, train_wall=23, gb_free=11, wall=13842
2021-05-01 16:06:24 | INFO | train_inner | epoch 001:  45507 / 60421 loss=1.449, ppl=2.73, wps=16047.9, ups=4.37, wpb=3672.1, bsz=119.2, num_updates=45500, lr=0.00014825, gnorm=1.962, loss_scale=4, train_wall=23, gb_free=10.7, wall=13865
2021-05-01 16:06:48 | INFO | train_inner | epoch 001:  45607 / 60421 loss=1.321, ppl=2.5, wps=15658.3, ups=4.21, wpb=3715, bsz=121.9, num_updates=45600, lr=0.000148087, gnorm=1.458, loss_scale=4, train_wall=24, gb_free=10.8, wall=13889
2021-05-01 16:07:11 | INFO | train_inner | epoch 001:  45707 / 60421 loss=1.386, ppl=2.61, wps=16234.4, ups=4.31, wpb=3770, bsz=115.8, num_updates=45700, lr=0.000147925, gnorm=1.609, loss_scale=4, train_wall=23, gb_free=11.6, wall=13912
2021-05-01 16:07:34 | INFO | train_inner | epoch 001:  45807 / 60421 loss=1.514, ppl=2.86, wps=16489.3, ups=4.4, wpb=3746.2, bsz=145.8, num_updates=45800, lr=0.000147764, gnorm=1.99, loss_scale=4, train_wall=23, gb_free=10.7, wall=13935
2021-05-01 16:07:56 | INFO | train_inner | epoch 001:  45907 / 60421 loss=1.396, ppl=2.63, wps=16464.8, ups=4.41, wpb=3731.7, bsz=121.1, num_updates=45900, lr=0.000147602, gnorm=1.597, loss_scale=4, train_wall=22, gb_free=10.9, wall=13957
2021-05-01 16:08:19 | INFO | train_inner | epoch 001:  46007 / 60421 loss=1.388, ppl=2.62, wps=16591.6, ups=4.42, wpb=3754.4, bsz=138.4, num_updates=46000, lr=0.000147442, gnorm=1.474, loss_scale=4, train_wall=22, gb_free=10.6, wall=13980
2021-05-01 16:08:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:08:19 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:08:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:08:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:08:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:08:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:08:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:08:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:08:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:08:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:08:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:08:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:08:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:08:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:08:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:08:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:08:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:08:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:08:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:08:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:09:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.029 | ppl 16.33 | bleu 27.26 | wps 2402.3 | wpb 2024.1 | bsz 97.5 | num_updates 46000 | best_bleu 27.26
2021-05-01 16:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 46000 updates
2021-05-01 16:09:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_46000.pt
2021-05-01 16:09:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_46000.pt
2021-05-01 16:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_46000.pt (epoch 1 @ 46000 updates, score 27.26) (writing took 14.346990217003622 seconds)
2021-05-01 16:10:02 | INFO | train_inner | epoch 001:  46107 / 60421 loss=1.363, ppl=2.57, wps=3593.4, ups=0.97, wpb=3701.1, bsz=125.6, num_updates=46100, lr=0.000147282, gnorm=1.582, loss_scale=4, train_wall=22, gb_free=10.8, wall=14083
2021-05-01 16:10:25 | INFO | train_inner | epoch 001:  46207 / 60421 loss=1.349, ppl=2.55, wps=16456.7, ups=4.41, wpb=3728, bsz=136, num_updates=46200, lr=0.000147122, gnorm=1.5, loss_scale=4, train_wall=22, gb_free=10.8, wall=14106
2021-05-01 16:10:48 | INFO | train_inner | epoch 001:  46307 / 60421 loss=1.39, ppl=2.62, wps=16347.9, ups=4.34, wpb=3762.5, bsz=145.1, num_updates=46300, lr=0.000146964, gnorm=1.692, loss_scale=4, train_wall=23, gb_free=10.7, wall=14129
2021-05-01 16:11:11 | INFO | train_inner | epoch 001:  46407 / 60421 loss=1.379, ppl=2.6, wps=16105.2, ups=4.31, wpb=3736.5, bsz=140.4, num_updates=46400, lr=0.000146805, gnorm=1.617, loss_scale=4, train_wall=23, gb_free=10.6, wall=14152
2021-05-01 16:11:34 | INFO | train_inner | epoch 001:  46507 / 60421 loss=1.359, ppl=2.56, wps=15929.9, ups=4.28, wpb=3722.2, bsz=137.5, num_updates=46500, lr=0.000146647, gnorm=1.589, loss_scale=4, train_wall=23, gb_free=11, wall=14175
2021-05-01 16:11:58 | INFO | train_inner | epoch 001:  46607 / 60421 loss=1.345, ppl=2.54, wps=15875, ups=4.22, wpb=3762.4, bsz=147, num_updates=46600, lr=0.00014649, gnorm=1.555, loss_scale=4, train_wall=24, gb_free=10.8, wall=14199
2021-05-01 16:12:21 | INFO | train_inner | epoch 001:  46707 / 60421 loss=1.344, ppl=2.54, wps=16301, ups=4.34, wpb=3754.8, bsz=132.8, num_updates=46700, lr=0.000146333, gnorm=1.475, loss_scale=4, train_wall=23, gb_free=10.7, wall=14222
2021-05-01 16:12:44 | INFO | train_inner | epoch 001:  46807 / 60421 loss=1.394, ppl=2.63, wps=16447.2, ups=4.43, wpb=3713.1, bsz=138.4, num_updates=46800, lr=0.000146176, gnorm=1.699, loss_scale=4, train_wall=22, gb_free=10.7, wall=14245
2021-05-01 16:13:06 | INFO | train_inner | epoch 001:  46907 / 60421 loss=1.443, ppl=2.72, wps=16685.6, ups=4.42, wpb=3775.6, bsz=124.3, num_updates=46900, lr=0.00014602, gnorm=1.675, loss_scale=4, train_wall=22, gb_free=10.8, wall=14267
2021-05-01 16:13:29 | INFO | train_inner | epoch 001:  47007 / 60421 loss=1.328, ppl=2.51, wps=16643.9, ups=4.41, wpb=3770, bsz=147.3, num_updates=47000, lr=0.000145865, gnorm=1.503, loss_scale=4, train_wall=22, gb_free=10.9, wall=14290
2021-05-01 16:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:13:29 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:13:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:13:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:13:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:13:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:13:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:13:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:13:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:13:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:13:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:13:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:13:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:13:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:13:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:13:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:13:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:14:33 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.029 | ppl 16.32 | bleu 27.09 | wps 2454.7 | wpb 2024.1 | bsz 97.5 | num_updates 47000 | best_bleu 27.26
2021-05-01 16:14:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 47000 updates
2021-05-01 16:14:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_47000.pt
2021-05-01 16:14:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_47000.pt
2021-05-01 16:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_47000.pt (epoch 1 @ 47000 updates, score 27.09) (writing took 7.691381200005708 seconds)
2021-05-01 16:15:04 | INFO | train_inner | epoch 001:  47107 / 60421 loss=1.518, ppl=2.86, wps=3909.7, ups=1.05, wpb=3711.2, bsz=120, num_updates=47100, lr=0.00014571, gnorm=1.913, loss_scale=4, train_wall=22, gb_free=11, wall=14385
2021-05-01 16:15:27 | INFO | train_inner | epoch 001:  47207 / 60421 loss=1.482, ppl=2.79, wps=16703.1, ups=4.34, wpb=3845.4, bsz=131.3, num_updates=47200, lr=0.000145556, gnorm=1.579, loss_scale=4, train_wall=23, gb_free=10.8, wall=14408
2021-05-01 16:15:50 | INFO | train_inner | epoch 001:  47307 / 60421 loss=1.36, ppl=2.57, wps=16371.7, ups=4.3, wpb=3809.5, bsz=135.4, num_updates=47300, lr=0.000145402, gnorm=1.539, loss_scale=4, train_wall=23, gb_free=11, wall=14431
2021-05-01 16:16:13 | INFO | train_inner | epoch 001:  47407 / 60421 loss=1.436, ppl=2.71, wps=15910.7, ups=4.34, wpb=3663.2, bsz=122.3, num_updates=47400, lr=0.000145248, gnorm=2.002, loss_scale=4, train_wall=23, gb_free=10.7, wall=14454
2021-05-01 16:16:37 | INFO | train_inner | epoch 001:  47507 / 60421 loss=1.314, ppl=2.49, wps=16079.5, ups=4.26, wpb=3772.8, bsz=123.7, num_updates=47500, lr=0.000145095, gnorm=1.474, loss_scale=4, train_wall=23, gb_free=10.7, wall=14478
2021-05-01 16:17:00 | INFO | train_inner | epoch 001:  47607 / 60421 loss=1.284, ppl=2.44, wps=16025.2, ups=4.27, wpb=3756.1, bsz=132.6, num_updates=47600, lr=0.000144943, gnorm=1.403, loss_scale=4, train_wall=23, gb_free=10.8, wall=14501
2021-05-01 16:17:23 | INFO | train_inner | epoch 001:  47707 / 60421 loss=1.417, ppl=2.67, wps=16292, ups=4.42, wpb=3689.3, bsz=119.3, num_updates=47700, lr=0.000144791, gnorm=1.791, loss_scale=4, train_wall=22, gb_free=10.8, wall=14524
2021-05-01 16:17:45 | INFO | train_inner | epoch 001:  47807 / 60421 loss=1.413, ppl=2.66, wps=16340.5, ups=4.45, wpb=3670.7, bsz=136.6, num_updates=47800, lr=0.000144639, gnorm=1.95, loss_scale=4, train_wall=22, gb_free=10.7, wall=14546
2021-05-01 16:18:08 | INFO | train_inner | epoch 001:  47907 / 60421 loss=1.353, ppl=2.55, wps=16447.4, ups=4.46, wpb=3684.1, bsz=120.5, num_updates=47900, lr=0.000144488, gnorm=1.788, loss_scale=4, train_wall=22, gb_free=10.8, wall=14569
2021-05-01 16:18:30 | INFO | train_inner | epoch 001:  48007 / 60421 loss=1.36, ppl=2.57, wps=16614.1, ups=4.4, wpb=3776.4, bsz=138.5, num_updates=48000, lr=0.000144338, gnorm=1.718, loss_scale=4, train_wall=23, gb_free=10.7, wall=14591
2021-05-01 16:18:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:18:30 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:18:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:18:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:18:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:18:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:18:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:18:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:18:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:18:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:18:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:18:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:18:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:18:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:18:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:18:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:18:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:19:37 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.048 | ppl 16.54 | bleu 26.77 | wps 2385.2 | wpb 2024.1 | bsz 97.5 | num_updates 48000 | best_bleu 27.26
2021-05-01 16:19:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 48000 updates
2021-05-01 16:19:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_48000.pt
2021-05-01 16:19:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_48000.pt
2021-05-01 16:19:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_48000.pt (epoch 1 @ 48000 updates, score 26.77) (writing took 10.056497591998777 seconds)
2021-05-01 16:20:10 | INFO | train_inner | epoch 001:  48107 / 60421 loss=1.412, ppl=2.66, wps=3810.5, ups=1, wpb=3803.6, bsz=129, num_updates=48100, lr=0.000144187, gnorm=1.667, loss_scale=4, train_wall=23, gb_free=10.8, wall=14691
2021-05-01 16:20:33 | INFO | train_inner | epoch 001:  48207 / 60421 loss=1.381, ppl=2.6, wps=16320.8, ups=4.35, wpb=3752.9, bsz=114.4, num_updates=48200, lr=0.000144038, gnorm=1.643, loss_scale=4, train_wall=23, gb_free=10.9, wall=14714
2021-05-01 16:20:56 | INFO | train_inner | epoch 001:  48307 / 60421 loss=1.386, ppl=2.61, wps=16384.2, ups=4.34, wpb=3771.6, bsz=128.2, num_updates=48300, lr=0.000143889, gnorm=1.758, loss_scale=4, train_wall=23, gb_free=10.9, wall=14737
2021-05-01 16:21:19 | INFO | train_inner | epoch 001:  48407 / 60421 loss=1.351, ppl=2.55, wps=16056.9, ups=4.31, wpb=3727.7, bsz=152.2, num_updates=48400, lr=0.00014374, gnorm=1.577, loss_scale=4, train_wall=23, gb_free=10.9, wall=14760
2021-05-01 16:21:43 | INFO | train_inner | epoch 001:  48507 / 60421 loss=1.402, ppl=2.64, wps=15805.1, ups=4.26, wpb=3707.3, bsz=116.3, num_updates=48500, lr=0.000143592, gnorm=1.747, loss_scale=4, train_wall=23, gb_free=10.9, wall=14784
2021-05-01 16:22:06 | INFO | train_inner | epoch 001:  48607 / 60421 loss=1.356, ppl=2.56, wps=16179.1, ups=4.33, wpb=3734, bsz=133.4, num_updates=48600, lr=0.000143444, gnorm=1.943, loss_scale=4, train_wall=23, gb_free=10.8, wall=14807
2021-05-01 16:22:29 | INFO | train_inner | epoch 001:  48707 / 60421 loss=1.376, ppl=2.6, wps=16363.2, ups=4.34, wpb=3773, bsz=117, num_updates=48700, lr=0.000143296, gnorm=1.59, loss_scale=4, train_wall=23, gb_free=11.1, wall=14830
2021-05-01 16:22:52 | INFO | train_inner | epoch 001:  48807 / 60421 loss=1.369, ppl=2.58, wps=16535.9, ups=4.39, wpb=3764.6, bsz=132, num_updates=48800, lr=0.00014315, gnorm=1.588, loss_scale=4, train_wall=23, gb_free=10.7, wall=14853
2021-05-01 16:23:14 | INFO | train_inner | epoch 001:  48907 / 60421 loss=1.36, ppl=2.57, wps=16471.7, ups=4.4, wpb=3746.7, bsz=126.8, num_updates=48900, lr=0.000143003, gnorm=1.62, loss_scale=4, train_wall=23, gb_free=10.8, wall=14875
2021-05-01 16:23:37 | INFO | train_inner | epoch 001:  49007 / 60421 loss=1.385, ppl=2.61, wps=16221.8, ups=4.48, wpb=3621, bsz=136.4, num_updates=49000, lr=0.000142857, gnorm=2.003, loss_scale=4, train_wall=22, gb_free=11.1, wall=14898
2021-05-01 16:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:23:37 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:23:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:23:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:23:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:23:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:23:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:23:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:23:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:23:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:23:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:23:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:23:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:23:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:23:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:23:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:23:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:24:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.048 | ppl 16.54 | bleu 26.93 | wps 2432.8 | wpb 2024.1 | bsz 97.5 | num_updates 49000 | best_bleu 27.26
2021-05-01 16:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 49000 updates
2021-05-01 16:24:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_49000.pt
2021-05-01 16:24:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_49000.pt
2021-05-01 16:24:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_49000.pt (epoch 1 @ 49000 updates, score 26.93) (writing took 10.153369585998007 seconds)
2021-05-01 16:25:15 | INFO | train_inner | epoch 001:  49107 / 60421 loss=1.377, ppl=2.6, wps=3845.8, ups=1.02, wpb=3785.4, bsz=142.6, num_updates=49100, lr=0.000142712, gnorm=1.615, loss_scale=4, train_wall=23, gb_free=10.8, wall=14996
2021-05-01 16:25:38 | INFO | train_inner | epoch 001:  49207 / 60421 loss=1.405, ppl=2.65, wps=16374.1, ups=4.41, wpb=3713.2, bsz=116.7, num_updates=49200, lr=0.000142566, gnorm=1.871, loss_scale=4, train_wall=22, gb_free=10.7, wall=15019
2021-05-01 16:26:01 | INFO | train_inner | epoch 001:  49307 / 60421 loss=1.275, ppl=2.42, wps=16241.2, ups=4.29, wpb=3783.2, bsz=134.5, num_updates=49300, lr=0.000142422, gnorm=1.424, loss_scale=4, train_wall=23, gb_free=10.8, wall=15042
2021-05-01 16:26:25 | INFO | train_inner | epoch 001:  49407 / 60421 loss=1.306, ppl=2.47, wps=16141.6, ups=4.27, wpb=3780.7, bsz=143.9, num_updates=49400, lr=0.000142278, gnorm=1.525, loss_scale=4, train_wall=23, gb_free=10.7, wall=15066
2021-05-01 16:26:48 | INFO | train_inner | epoch 001:  49507 / 60421 loss=1.337, ppl=2.53, wps=15943.2, ups=4.24, wpb=3764, bsz=120.6, num_updates=49500, lr=0.000142134, gnorm=1.417, loss_scale=4, train_wall=23, gb_free=10.8, wall=15089
2021-05-01 16:27:11 | INFO | train_inner | epoch 001:  49607 / 60421 loss=1.435, ppl=2.7, wps=16222.8, ups=4.34, wpb=3734.6, bsz=137.7, num_updates=49600, lr=0.00014199, gnorm=1.708, loss_scale=4, train_wall=23, gb_free=10.8, wall=15112
2021-05-01 16:27:34 | INFO | train_inner | epoch 001:  49707 / 60421 loss=1.368, ppl=2.58, wps=16475.3, ups=4.37, wpb=3770.4, bsz=115.5, num_updates=49700, lr=0.000141848, gnorm=1.741, loss_scale=4, train_wall=23, gb_free=10.9, wall=15135
2021-05-01 16:27:57 | INFO | train_inner | epoch 001:  49807 / 60421 loss=1.292, ppl=2.45, wps=16485, ups=4.4, wpb=3746, bsz=146.7, num_updates=49800, lr=0.000141705, gnorm=1.442, loss_scale=4, train_wall=23, gb_free=10.9, wall=15158
2021-05-01 16:28:19 | INFO | train_inner | epoch 001:  49907 / 60421 loss=1.313, ppl=2.48, wps=16484.7, ups=4.45, wpb=3704.4, bsz=139.7, num_updates=49900, lr=0.000141563, gnorm=1.891, loss_scale=4, train_wall=22, gb_free=10.6, wall=15180
2021-05-01 16:28:42 | INFO | train_inner | epoch 001:  50007 / 60421 loss=1.332, ppl=2.52, wps=16498.9, ups=4.4, wpb=3748.6, bsz=136.5, num_updates=50000, lr=0.000141421, gnorm=1.569, loss_scale=4, train_wall=23, gb_free=10.7, wall=15203
2021-05-01 16:28:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:28:42 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:28:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:28:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:28:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:28:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:28:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:28:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:28:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:28:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:28:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:28:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:28:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:28:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:29:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:29:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:29:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:29:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.112 | ppl 17.29 | bleu 26.64 | wps 2442 | wpb 2024.1 | bsz 97.5 | num_updates 50000 | best_bleu 27.26
2021-05-01 16:29:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 50000 updates
2021-05-01 16:29:47 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_50000.pt
2021-05-01 16:29:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_50000.pt
2021-05-01 16:29:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_50000.pt (epoch 1 @ 50000 updates, score 26.64) (writing took 8.21793161000096 seconds)
2021-05-01 16:30:18 | INFO | train_inner | epoch 001:  50107 / 60421 loss=1.308, ppl=2.48, wps=3884.5, ups=1.04, wpb=3727.4, bsz=131.6, num_updates=50100, lr=0.00014128, gnorm=1.559, loss_scale=4, train_wall=23, gb_free=10.7, wall=15299
2021-05-01 16:30:41 | INFO | train_inner | epoch 001:  50207 / 60421 loss=1.401, ppl=2.64, wps=16436.2, ups=4.42, wpb=3719.2, bsz=122.2, num_updates=50200, lr=0.000141139, gnorm=1.657, loss_scale=4, train_wall=22, gb_free=10.8, wall=15322
2021-05-01 16:31:04 | INFO | train_inner | epoch 001:  50307 / 60421 loss=1.318, ppl=2.49, wps=16289.4, ups=4.31, wpb=3782.4, bsz=131.8, num_updates=50300, lr=0.000140999, gnorm=1.468, loss_scale=4, train_wall=23, gb_free=11.1, wall=15345
2021-05-01 16:31:27 | INFO | train_inner | epoch 001:  50407 / 60421 loss=1.378, ppl=2.6, wps=16271.4, ups=4.29, wpb=3790.8, bsz=149.9, num_updates=50400, lr=0.000140859, gnorm=1.63, loss_scale=4, train_wall=23, gb_free=10.8, wall=15368
2021-05-01 16:31:50 | INFO | train_inner | epoch 001:  50507 / 60421 loss=1.429, ppl=2.69, wps=15749.8, ups=4.3, wpb=3666.2, bsz=130.2, num_updates=50500, lr=0.00014072, gnorm=2.075, loss_scale=4, train_wall=23, gb_free=10.8, wall=15391
2021-05-01 16:32:14 | INFO | train_inner | epoch 001:  50607 / 60421 loss=1.347, ppl=2.54, wps=16385.2, ups=4.31, wpb=3802.1, bsz=120, num_updates=50600, lr=0.00014058, gnorm=1.415, loss_scale=4, train_wall=23, gb_free=10.9, wall=15415
2021-05-01 16:32:37 | INFO | train_inner | epoch 001:  50707 / 60421 loss=1.241, ppl=2.36, wps=16492.9, ups=4.36, wpb=3780.8, bsz=135.2, num_updates=50700, lr=0.000140442, gnorm=1.356, loss_scale=4, train_wall=23, gb_free=10.8, wall=15437
2021-05-01 16:32:59 | INFO | train_inner | epoch 001:  50807 / 60421 loss=1.32, ppl=2.5, wps=16452.5, ups=4.37, wpb=3764.4, bsz=125.7, num_updates=50800, lr=0.000140303, gnorm=1.642, loss_scale=4, train_wall=23, gb_free=10.8, wall=15460
2021-05-01 16:33:22 | INFO | train_inner | epoch 001:  50907 / 60421 loss=1.45, ppl=2.73, wps=16414.6, ups=4.39, wpb=3737.9, bsz=149, num_updates=50900, lr=0.000140165, gnorm=2.016, loss_scale=4, train_wall=23, gb_free=10.8, wall=15483
2021-05-01 16:33:45 | INFO | train_inner | epoch 001:  51007 / 60421 loss=1.317, ppl=2.49, wps=16729.2, ups=4.38, wpb=3823.1, bsz=133.6, num_updates=51000, lr=0.000140028, gnorm=1.379, loss_scale=4, train_wall=23, gb_free=10.9, wall=15506
2021-05-01 16:33:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:33:45 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:34:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:34:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:34:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:34:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:34:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:34:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:34:51 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.092 | ppl 17.06 | bleu 27.59 | wps 2385.5 | wpb 2024.1 | bsz 97.5 | num_updates 51000 | best_bleu 27.59
2021-05-01 16:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 51000 updates
2021-05-01 16:34:51 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_51000.pt
2021-05-01 16:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_51000.pt
2021-05-01 16:35:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_51000.pt (epoch 1 @ 51000 updates, score 27.59) (writing took 14.409471258004487 seconds)
2021-05-01 16:35:28 | INFO | train_inner | epoch 001:  51107 / 60421 loss=1.453, ppl=2.74, wps=3552.4, ups=0.97, wpb=3673.9, bsz=131, num_updates=51100, lr=0.000139891, gnorm=2.045, loss_scale=4, train_wall=22, gb_free=10.8, wall=15609
2021-05-01 16:35:52 | INFO | train_inner | epoch 001:  51207 / 60421 loss=1.334, ppl=2.52, wps=16187.1, ups=4.32, wpb=3749.9, bsz=129.4, num_updates=51200, lr=0.000139754, gnorm=1.653, loss_scale=4, train_wall=23, gb_free=11, wall=15633
2021-05-01 16:36:15 | INFO | train_inner | epoch 001:  51307 / 60421 loss=1.309, ppl=2.48, wps=16120, ups=4.36, wpb=3698.2, bsz=131, num_updates=51300, lr=0.000139618, gnorm=1.867, loss_scale=4, train_wall=23, gb_free=10.8, wall=15656
2021-05-01 16:36:38 | INFO | train_inner | epoch 001:  51407 / 60421 loss=1.349, ppl=2.55, wps=15940.4, ups=4.21, wpb=3785, bsz=138.1, num_updates=51400, lr=0.000139482, gnorm=1.591, loss_scale=4, train_wall=24, gb_free=10.8, wall=15679
2021-05-01 16:37:01 | INFO | train_inner | epoch 001:  51507 / 60421 loss=1.399, ppl=2.64, wps=15945.3, ups=4.31, wpb=3700.7, bsz=145.5, num_updates=51500, lr=0.000139347, gnorm=1.971, loss_scale=4, train_wall=23, gb_free=10.7, wall=15702
2021-05-01 16:37:24 | INFO | train_inner | epoch 001:  51607 / 60421 loss=1.241, ppl=2.36, wps=16397.6, ups=4.35, wpb=3767.8, bsz=141.9, num_updates=51600, lr=0.000139212, gnorm=1.351, loss_scale=4, train_wall=23, gb_free=10.7, wall=15725
2021-05-01 16:37:47 | INFO | train_inner | epoch 001:  51707 / 60421 loss=1.321, ppl=2.5, wps=16392, ups=4.36, wpb=3758.2, bsz=147.2, num_updates=51700, lr=0.000139077, gnorm=1.776, loss_scale=4, train_wall=23, gb_free=10.7, wall=15748
2021-05-01 16:38:10 | INFO | train_inner | epoch 001:  51807 / 60421 loss=1.315, ppl=2.49, wps=16487.3, ups=4.4, wpb=3744.2, bsz=129.3, num_updates=51800, lr=0.000138943, gnorm=1.591, loss_scale=4, train_wall=23, gb_free=10.6, wall=15771
2021-05-01 16:38:32 | INFO | train_inner | epoch 001:  51907 / 60421 loss=1.365, ppl=2.58, wps=16314.2, ups=4.49, wpb=3637.4, bsz=142.5, num_updates=51900, lr=0.000138809, gnorm=2.008, loss_scale=4, train_wall=22, gb_free=10.9, wall=15793
2021-05-01 16:38:55 | INFO | train_inner | epoch 001:  52007 / 60421 loss=1.275, ppl=2.42, wps=16475.4, ups=4.38, wpb=3760.3, bsz=132.2, num_updates=52000, lr=0.000138675, gnorm=1.489, loss_scale=4, train_wall=23, gb_free=11.1, wall=15816
2021-05-01 16:38:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:38:55 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:39:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:39:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:39:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:39:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:39:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:39:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:39:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:39:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:39:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:39:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:39:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:39:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:39:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:39:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:39:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:40:00 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.089 | ppl 17.02 | bleu 26.69 | wps 2433.8 | wpb 2024.1 | bsz 97.5 | num_updates 52000 | best_bleu 27.59
2021-05-01 16:40:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 52000 updates
2021-05-01 16:40:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_52000.pt
2021-05-01 16:40:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_52000.pt
2021-05-01 16:40:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_52000.pt (epoch 1 @ 52000 updates, score 26.69) (writing took 8.164468233997468 seconds)
2021-05-01 16:40:31 | INFO | train_inner | epoch 001:  52107 / 60421 loss=1.411, ppl=2.66, wps=3819.6, ups=1.04, wpb=3664.1, bsz=131.2, num_updates=52100, lr=0.000138542, gnorm=1.976, loss_scale=4, train_wall=22, gb_free=10.8, wall=15912
2021-05-01 16:40:54 | INFO | train_inner | epoch 001:  52207 / 60421 loss=1.229, ppl=2.34, wps=16319.7, ups=4.3, wpb=3793.2, bsz=151.5, num_updates=52200, lr=0.000138409, gnorm=1.451, loss_scale=4, train_wall=23, gb_free=10.8, wall=15935
2021-05-01 16:41:18 | INFO | train_inner | epoch 001:  52307 / 60421 loss=1.255, ppl=2.39, wps=16004.7, ups=4.32, wpb=3700.7, bsz=134.4, num_updates=52300, lr=0.000138277, gnorm=1.482, loss_scale=4, train_wall=23, gb_free=10.9, wall=15958
2021-05-01 16:41:41 | INFO | train_inner | epoch 001:  52407 / 60421 loss=1.317, ppl=2.49, wps=15972.1, ups=4.29, wpb=3726.9, bsz=134.4, num_updates=52400, lr=0.000138145, gnorm=1.589, loss_scale=4, train_wall=23, gb_free=11, wall=15982
2021-05-01 16:42:04 | INFO | train_inner | epoch 001:  52507 / 60421 loss=1.277, ppl=2.42, wps=16089.6, ups=4.28, wpb=3759.6, bsz=133.8, num_updates=52500, lr=0.000138013, gnorm=1.457, loss_scale=4, train_wall=23, gb_free=10.7, wall=16005
2021-05-01 16:42:27 | INFO | train_inner | epoch 001:  52607 / 60421 loss=1.339, ppl=2.53, wps=16425.3, ups=4.4, wpb=3734.6, bsz=110.6, num_updates=52600, lr=0.000137882, gnorm=1.47, loss_scale=4, train_wall=23, gb_free=10.7, wall=16028
2021-05-01 16:42:49 | INFO | train_inner | epoch 001:  52707 / 60421 loss=1.385, ppl=2.61, wps=16474.1, ups=4.46, wpb=3696.9, bsz=118.6, num_updates=52700, lr=0.000137751, gnorm=1.798, loss_scale=4, train_wall=22, gb_free=10.8, wall=16050
2021-05-01 16:43:12 | INFO | train_inner | epoch 001:  52807 / 60421 loss=1.287, ppl=2.44, wps=16577.4, ups=4.38, wpb=3785.2, bsz=139.3, num_updates=52800, lr=0.00013762, gnorm=1.547, loss_scale=8, train_wall=23, gb_free=10.8, wall=16073
2021-05-01 16:43:35 | INFO | train_inner | epoch 001:  52907 / 60421 loss=1.322, ppl=2.5, wps=16483.9, ups=4.4, wpb=3742.2, bsz=114.4, num_updates=52900, lr=0.00013749, gnorm=1.64, loss_scale=8, train_wall=23, gb_free=11.3, wall=16096
2021-05-01 16:43:57 | INFO | train_inner | epoch 001:  53007 / 60421 loss=1.31, ppl=2.48, wps=16348.8, ups=4.5, wpb=3633.1, bsz=133.3, num_updates=53000, lr=0.000137361, gnorm=1.956, loss_scale=8, train_wall=22, gb_free=11, wall=16118
2021-05-01 16:43:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:43:57 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:44:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:44:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:44:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:44:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:44:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:44:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:44:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:44:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:44:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:44:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:44:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:44:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:45:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.135 | ppl 17.57 | bleu 27.13 | wps 2407.1 | wpb 2024.1 | bsz 97.5 | num_updates 53000 | best_bleu 27.59
2021-05-01 16:45:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 53000 updates
2021-05-01 16:45:03 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_53000.pt
2021-05-01 16:45:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_53000.pt
2021-05-01 16:45:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_53000.pt (epoch 1 @ 53000 updates, score 27.13) (writing took 7.675266225000087 seconds)
2021-05-01 16:45:34 | INFO | train_inner | epoch 001:  53107 / 60421 loss=1.473, ppl=2.78, wps=3901.4, ups=1.04, wpb=3767.4, bsz=116.6, num_updates=53100, lr=0.000137231, gnorm=1.852, loss_scale=8, train_wall=23, gb_free=10.7, wall=16215
2021-05-01 16:45:57 | INFO | train_inner | epoch 001:  53207 / 60421 loss=1.216, ppl=2.32, wps=16062.3, ups=4.34, wpb=3701.9, bsz=165.9, num_updates=53200, lr=0.000137102, gnorm=1.612, loss_scale=8, train_wall=23, gb_free=10.8, wall=16238
2021-05-01 16:46:20 | INFO | train_inner | epoch 001:  53307 / 60421 loss=1.289, ppl=2.44, wps=16174.5, ups=4.3, wpb=3757.6, bsz=140, num_updates=53300, lr=0.000136973, gnorm=1.639, loss_scale=8, train_wall=23, gb_free=10.8, wall=16261
2021-05-01 16:46:43 | INFO | train_inner | epoch 001:  53407 / 60421 loss=1.268, ppl=2.41, wps=15714.7, ups=4.29, wpb=3660.1, bsz=145.5, num_updates=53400, lr=0.000136845, gnorm=2.168, loss_scale=8, train_wall=23, gb_free=10.8, wall=16284
2021-05-01 16:47:07 | INFO | train_inner | epoch 001:  53507 / 60421 loss=1.3, ppl=2.46, wps=16206.7, ups=4.28, wpb=3789.1, bsz=118.2, num_updates=53500, lr=0.000136717, gnorm=1.359, loss_scale=8, train_wall=23, gb_free=10.8, wall=16308
2021-05-01 16:47:30 | INFO | train_inner | epoch 001:  53607 / 60421 loss=1.308, ppl=2.48, wps=16582, ups=4.32, wpb=3842.8, bsz=117, num_updates=53600, lr=0.00013659, gnorm=1.351, loss_scale=8, train_wall=23, gb_free=11, wall=16331
2021-05-01 16:47:53 | INFO | train_inner | epoch 001:  53707 / 60421 loss=1.328, ppl=2.51, wps=16409.2, ups=4.41, wpb=3724.8, bsz=127.9, num_updates=53700, lr=0.000136462, gnorm=1.821, loss_scale=8, train_wall=23, gb_free=11.1, wall=16354
2021-05-01 16:48:15 | INFO | train_inner | epoch 001:  53807 / 60421 loss=1.37, ppl=2.59, wps=16445.5, ups=4.44, wpb=3706.8, bsz=130.5, num_updates=53800, lr=0.000136335, gnorm=1.946, loss_scale=8, train_wall=22, gb_free=10.7, wall=16376
2021-05-01 16:48:38 | INFO | train_inner | epoch 001:  53907 / 60421 loss=1.289, ppl=2.44, wps=16727.4, ups=4.35, wpb=3844.6, bsz=140.5, num_updates=53900, lr=0.000136209, gnorm=1.37, loss_scale=8, train_wall=23, gb_free=10.8, wall=16399
2021-05-01 16:49:01 | INFO | train_inner | epoch 001:  54007 / 60421 loss=1.251, ppl=2.38, wps=16362.1, ups=4.41, wpb=3707.5, bsz=131, num_updates=54000, lr=0.000136083, gnorm=1.723, loss_scale=8, train_wall=22, gb_free=11.3, wall=16422
2021-05-01 16:49:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:49:01 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:49:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:49:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:49:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:49:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:49:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:49:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:49:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:49:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:49:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:49:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:49:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:49:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:49:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:49:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:49:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:50:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.099 | ppl 17.14 | bleu 26.93 | wps 2410.3 | wpb 2024.1 | bsz 97.5 | num_updates 54000 | best_bleu 27.59
2021-05-01 16:50:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 54000 updates
2021-05-01 16:50:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_54000.pt
2021-05-01 16:50:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_54000.pt
2021-05-01 16:50:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_54000.pt (epoch 1 @ 54000 updates, score 26.93) (writing took 7.781871955994575 seconds)
2021-05-01 16:50:37 | INFO | train_inner | epoch 001:  54107 / 60421 loss=1.312, ppl=2.48, wps=3879.6, ups=1.04, wpb=3748.1, bsz=115.6, num_updates=54100, lr=0.000135957, gnorm=1.526, loss_scale=8, train_wall=23, gb_free=10.7, wall=16518
2021-05-01 16:51:00 | INFO | train_inner | epoch 001:  54207 / 60421 loss=1.321, ppl=2.5, wps=16395.6, ups=4.33, wpb=3787.9, bsz=121.8, num_updates=54200, lr=0.000135831, gnorm=1.463, loss_scale=8, train_wall=23, gb_free=11.2, wall=16541
2021-05-01 16:51:24 | INFO | train_inner | epoch 001:  54307 / 60421 loss=1.269, ppl=2.41, wps=16137.3, ups=4.27, wpb=3776.5, bsz=141.5, num_updates=54300, lr=0.000135706, gnorm=1.441, loss_scale=8, train_wall=23, gb_free=10.7, wall=16565
2021-05-01 16:51:47 | INFO | train_inner | epoch 001:  54407 / 60421 loss=1.234, ppl=2.35, wps=15757.9, ups=4.24, wpb=3713.5, bsz=135.7, num_updates=54400, lr=0.000135582, gnorm=1.419, loss_scale=8, train_wall=23, gb_free=10.8, wall=16588
2021-05-01 16:52:11 | INFO | train_inner | epoch 001:  54507 / 60421 loss=1.22, ppl=2.33, wps=16222.7, ups=4.3, wpb=3771.9, bsz=136.6, num_updates=54500, lr=0.000135457, gnorm=1.357, loss_scale=8, train_wall=23, gb_free=10.9, wall=16612
2021-05-01 16:52:33 | INFO | train_inner | epoch 001:  54607 / 60421 loss=1.256, ppl=2.39, wps=16212.7, ups=4.41, wpb=3677.3, bsz=130.9, num_updates=54600, lr=0.000135333, gnorm=1.687, loss_scale=8, train_wall=23, gb_free=10.9, wall=16634
2021-05-01 16:52:56 | INFO | train_inner | epoch 001:  54707 / 60421 loss=1.282, ppl=2.43, wps=16618.2, ups=4.41, wpb=3771.5, bsz=133.5, num_updates=54700, lr=0.000135209, gnorm=1.586, loss_scale=8, train_wall=23, gb_free=10.8, wall=16657
2021-05-01 16:53:19 | INFO | train_inner | epoch 001:  54807 / 60421 loss=1.374, ppl=2.59, wps=16515.9, ups=4.43, wpb=3732, bsz=146.8, num_updates=54800, lr=0.000135086, gnorm=1.826, loss_scale=8, train_wall=22, gb_free=10.9, wall=16680
2021-05-01 16:53:41 | INFO | train_inner | epoch 001:  54907 / 60421 loss=1.349, ppl=2.55, wps=16415, ups=4.49, wpb=3652.6, bsz=111.5, num_updates=54900, lr=0.000134963, gnorm=1.799, loss_scale=8, train_wall=22, gb_free=11.1, wall=16702
2021-05-01 16:54:04 | INFO | train_inner | epoch 001:  55007 / 60421 loss=1.31, ppl=2.48, wps=16458.2, ups=4.39, wpb=3747.4, bsz=138.2, num_updates=55000, lr=0.00013484, gnorm=1.777, loss_scale=8, train_wall=23, gb_free=11.5, wall=16725
2021-05-01 16:54:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:54:04 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:54:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:54:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:54:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:54:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:54:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:54:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:54:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:54:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:54:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:54:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:54:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:54:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:54:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:54:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:54:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:55:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.157 | ppl 17.84 | bleu 27.01 | wps 2432.7 | wpb 2024.1 | bsz 97.5 | num_updates 55000 | best_bleu 27.59
2021-05-01 16:55:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 55000 updates
2021-05-01 16:55:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_55000.pt
2021-05-01 16:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_55000.pt
2021-05-01 16:55:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_55000.pt (epoch 1 @ 55000 updates, score 27.01) (writing took 7.7830469379987335 seconds)
2021-05-01 16:55:40 | INFO | train_inner | epoch 001:  55107 / 60421 loss=1.306, ppl=2.47, wps=3916.8, ups=1.04, wpb=3762.1, bsz=119.9, num_updates=55100, lr=0.000134718, gnorm=1.544, loss_scale=8, train_wall=23, gb_free=11.2, wall=16821
2021-05-01 16:56:03 | INFO | train_inner | epoch 001:  55207 / 60421 loss=1.295, ppl=2.45, wps=16224.4, ups=4.35, wpb=3726, bsz=133.1, num_updates=55200, lr=0.000134595, gnorm=1.594, loss_scale=8, train_wall=23, gb_free=11, wall=16844
2021-05-01 16:56:26 | INFO | train_inner | epoch 001:  55307 / 60421 loss=1.265, ppl=2.4, wps=16032.3, ups=4.3, wpb=3730.4, bsz=129.2, num_updates=55300, lr=0.000134474, gnorm=1.667, loss_scale=8, train_wall=23, gb_free=11, wall=16867
2021-05-01 16:56:49 | INFO | train_inner | epoch 001:  55407 / 60421 loss=1.323, ppl=2.5, wps=15985, ups=4.25, wpb=3758.3, bsz=139, num_updates=55400, lr=0.000134352, gnorm=1.661, loss_scale=8, train_wall=23, gb_free=10.5, wall=16890
2021-05-01 16:57:13 | INFO | train_inner | epoch 001:  55507 / 60421 loss=1.406, ppl=2.65, wps=16251.1, ups=4.32, wpb=3761.1, bsz=127.7, num_updates=55500, lr=0.000134231, gnorm=1.733, loss_scale=8, train_wall=23, gb_free=10.8, wall=16914
2021-05-01 16:57:35 | INFO | train_inner | epoch 001:  55607 / 60421 loss=1.292, ppl=2.45, wps=16440.6, ups=4.4, wpb=3738.8, bsz=127, num_updates=55600, lr=0.00013411, gnorm=1.63, loss_scale=8, train_wall=23, gb_free=10.7, wall=16936
2021-05-01 16:57:58 | INFO | train_inner | epoch 001:  55707 / 60421 loss=1.327, ppl=2.51, wps=16516.1, ups=4.4, wpb=3751.6, bsz=140.8, num_updates=55700, lr=0.00013399, gnorm=1.668, loss_scale=8, train_wall=23, gb_free=10.7, wall=16959
2021-05-01 16:58:21 | INFO | train_inner | epoch 001:  55807 / 60421 loss=1.22, ppl=2.33, wps=16551.2, ups=4.41, wpb=3755.2, bsz=145.9, num_updates=55800, lr=0.00013387, gnorm=1.443, loss_scale=8, train_wall=23, gb_free=11, wall=16982
2021-05-01 16:58:43 | INFO | train_inner | epoch 001:  55907 / 60421 loss=1.25, ppl=2.38, wps=16450.9, ups=4.43, wpb=3710, bsz=135.3, num_updates=55900, lr=0.00013375, gnorm=1.608, loss_scale=8, train_wall=22, gb_free=11, wall=17004
2021-05-01 16:59:06 | INFO | train_inner | epoch 001:  56007 / 60421 loss=1.401, ppl=2.64, wps=16323.4, ups=4.39, wpb=3721.2, bsz=122.6, num_updates=56000, lr=0.000133631, gnorm=2.277, loss_scale=8, train_wall=23, gb_free=10.8, wall=17027
2021-05-01 16:59:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 16:59:06 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 16:59:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:59:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:59:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:59:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:59:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:59:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:59:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:59:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:59:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:59:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:59:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:59:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 16:59:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 16:59:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 16:59:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:00:11 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.112 | ppl 17.29 | bleu 27.32 | wps 2436.1 | wpb 2024.1 | bsz 97.5 | num_updates 56000 | best_bleu 27.59
2021-05-01 17:00:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 56000 updates
2021-05-01 17:00:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_56000.pt
2021-05-01 17:00:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_56000.pt
2021-05-01 17:00:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_56000.pt (epoch 1 @ 56000 updates, score 27.32) (writing took 7.782900535996305 seconds)
2021-05-01 17:00:42 | INFO | train_inner | epoch 001:  56107 / 60421 loss=1.337, ppl=2.53, wps=3886.8, ups=1.04, wpb=3728.2, bsz=120, num_updates=56100, lr=0.000133511, gnorm=1.611, loss_scale=8, train_wall=23, gb_free=10.8, wall=17123
2021-05-01 17:01:05 | INFO | train_inner | epoch 001:  56207 / 60421 loss=1.245, ppl=2.37, wps=16011.7, ups=4.34, wpb=3685.8, bsz=127.2, num_updates=56200, lr=0.000133393, gnorm=1.62, loss_scale=8, train_wall=23, gb_free=10.9, wall=17146
2021-05-01 17:01:28 | INFO | train_inner | epoch 001:  56307 / 60421 loss=1.335, ppl=2.52, wps=16051.3, ups=4.29, wpb=3742.5, bsz=135.9, num_updates=56300, lr=0.000133274, gnorm=1.723, loss_scale=8, train_wall=23, gb_free=11, wall=17169
2021-05-01 17:01:52 | INFO | train_inner | epoch 001:  56407 / 60421 loss=1.263, ppl=2.4, wps=15739, ups=4.25, wpb=3705.6, bsz=136.6, num_updates=56400, lr=0.000133156, gnorm=1.595, loss_scale=8, train_wall=23, gb_free=10.7, wall=17193
2021-05-01 17:02:15 | INFO | train_inner | epoch 001:  56507 / 60421 loss=1.211, ppl=2.31, wps=16351.2, ups=4.27, wpb=3827.8, bsz=133.7, num_updates=56500, lr=0.000133038, gnorm=1.23, loss_scale=8, train_wall=23, gb_free=11.2, wall=17216
2021-05-01 17:02:38 | INFO | train_inner | epoch 001:  56607 / 60421 loss=1.278, ppl=2.43, wps=16419, ups=4.35, wpb=3771.5, bsz=140.3, num_updates=56600, lr=0.00013292, gnorm=1.558, loss_scale=8, train_wall=23, gb_free=10.4, wall=17239
2021-05-01 17:03:01 | INFO | train_inner | epoch 001:  56707 / 60421 loss=1.418, ppl=2.67, wps=16369.9, ups=4.41, wpb=3711.7, bsz=145.8, num_updates=56700, lr=0.000132803, gnorm=2.272, loss_scale=8, train_wall=22, gb_free=11.1, wall=17262
2021-05-01 17:03:23 | INFO | train_inner | epoch 001:  56807 / 60421 loss=1.357, ppl=2.56, wps=16324.3, ups=4.44, wpb=3676.4, bsz=129.8, num_updates=56800, lr=0.000132686, gnorm=1.863, loss_scale=8, train_wall=22, gb_free=12, wall=17284
2021-05-01 17:03:46 | INFO | train_inner | epoch 001:  56907 / 60421 loss=1.279, ppl=2.43, wps=16573.6, ups=4.4, wpb=3762.9, bsz=140.3, num_updates=56900, lr=0.00013257, gnorm=1.733, loss_scale=8, train_wall=23, gb_free=10.8, wall=17307
2021-05-01 17:04:09 | INFO | train_inner | epoch 001:  57007 / 60421 loss=1.381, ppl=2.61, wps=16401.7, ups=4.48, wpb=3664.1, bsz=114.5, num_updates=57000, lr=0.000132453, gnorm=1.955, loss_scale=8, train_wall=22, gb_free=10.8, wall=17329
2021-05-01 17:04:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:04:09 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:04:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:04:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:04:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:04:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:04:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:04:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:04:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:04:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:04:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:04:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:04:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:04:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:04:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:04:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:04:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:05:14 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.131 | ppl 17.52 | bleu 27.33 | wps 2423.4 | wpb 2024.1 | bsz 97.5 | num_updates 57000 | best_bleu 27.59
2021-05-01 17:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 57000 updates
2021-05-01 17:05:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_57000.pt
2021-05-01 17:05:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_57000.pt
2021-05-01 17:05:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_57000.pt (epoch 1 @ 57000 updates, score 27.33) (writing took 7.769899462000467 seconds)
2021-05-01 17:05:45 | INFO | train_inner | epoch 001:  57107 / 60421 loss=1.292, ppl=2.45, wps=3868.2, ups=1.04, wpb=3722.4, bsz=123.1, num_updates=57100, lr=0.000132337, gnorm=1.728, loss_scale=8, train_wall=23, gb_free=10.8, wall=17426
2021-05-01 17:06:08 | INFO | train_inner | epoch 001:  57207 / 60421 loss=1.321, ppl=2.5, wps=16282.2, ups=4.33, wpb=3761.8, bsz=134.7, num_updates=57200, lr=0.000132221, gnorm=1.764, loss_scale=8, train_wall=23, gb_free=10.8, wall=17449
2021-05-01 17:06:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-01 17:06:31 | INFO | train_inner | epoch 001:  57308 / 60421 loss=1.386, ppl=2.61, wps=16022.5, ups=4.24, wpb=3781.2, bsz=109.3, num_updates=57300, lr=0.000132106, gnorm=1.508, loss_scale=4, train_wall=23, gb_free=10.9, wall=17472
2021-05-01 17:06:55 | INFO | train_inner | epoch 001:  57408 / 60421 loss=1.36, ppl=2.57, wps=15910.8, ups=4.25, wpb=3742.5, bsz=126.7, num_updates=57400, lr=0.000131991, gnorm=1.693, loss_scale=4, train_wall=23, gb_free=10.8, wall=17496
2021-05-01 17:07:18 | INFO | train_inner | epoch 001:  57508 / 60421 loss=1.344, ppl=2.54, wps=16324.8, ups=4.37, wpb=3734.9, bsz=115.4, num_updates=57500, lr=0.000131876, gnorm=1.749, loss_scale=4, train_wall=23, gb_free=10.8, wall=17519
2021-05-01 17:07:40 | INFO | train_inner | epoch 001:  57608 / 60421 loss=1.271, ppl=2.41, wps=16478.5, ups=4.42, wpb=3724.8, bsz=125.2, num_updates=57600, lr=0.000131762, gnorm=1.349, loss_scale=4, train_wall=22, gb_free=10.8, wall=17541
2021-05-01 17:08:03 | INFO | train_inner | epoch 001:  57708 / 60421 loss=1.245, ppl=2.37, wps=16497.8, ups=4.41, wpb=3743.8, bsz=123, num_updates=57700, lr=0.000131647, gnorm=1.575, loss_scale=4, train_wall=23, gb_free=10.8, wall=17564
2021-05-01 17:08:26 | INFO | train_inner | epoch 001:  57808 / 60421 loss=1.2, ppl=2.3, wps=16519.3, ups=4.46, wpb=3704.8, bsz=129.2, num_updates=57800, lr=0.000131533, gnorm=1.423, loss_scale=4, train_wall=22, gb_free=10.7, wall=17587
2021-05-01 17:08:48 | INFO | train_inner | epoch 001:  57908 / 60421 loss=1.357, ppl=2.56, wps=16580.4, ups=4.41, wpb=3761.5, bsz=126, num_updates=57900, lr=0.00013142, gnorm=1.755, loss_scale=4, train_wall=23, gb_free=10.9, wall=17609
2021-05-01 17:09:11 | INFO | train_inner | epoch 001:  58008 / 60421 loss=1.226, ppl=2.34, wps=16629.7, ups=4.37, wpb=3802.6, bsz=130.3, num_updates=58000, lr=0.000131306, gnorm=1.36, loss_scale=4, train_wall=23, gb_free=11, wall=17632
2021-05-01 17:09:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:09:11 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:09:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:09:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:09:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:09:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:09:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:09:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:09:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:09:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:09:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:09:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:09:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:09:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:09:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:09:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:09:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:10:16 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.174 | ppl 18.06 | bleu 27.63 | wps 2432 | wpb 2024.1 | bsz 97.5 | num_updates 58000 | best_bleu 27.63
2021-05-01 17:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 58000 updates
2021-05-01 17:10:16 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_58000.pt
2021-05-01 17:10:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_58000.pt
2021-05-01 17:10:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_58000.pt (epoch 1 @ 58000 updates, score 27.63) (writing took 14.403575035998074 seconds)
2021-05-01 17:10:54 | INFO | train_inner | epoch 001:  58108 / 60421 loss=1.268, ppl=2.41, wps=3708.3, ups=0.97, wpb=3813.6, bsz=128.6, num_updates=58100, lr=0.000131193, gnorm=1.567, loss_scale=4, train_wall=23, gb_free=10.7, wall=17735
2021-05-01 17:11:17 | INFO | train_inner | epoch 001:  58208 / 60421 loss=1.243, ppl=2.37, wps=16161.7, ups=4.27, wpb=3784.3, bsz=147.8, num_updates=58200, lr=0.000131081, gnorm=1.554, loss_scale=4, train_wall=23, gb_free=11, wall=17758
2021-05-01 17:11:41 | INFO | train_inner | epoch 001:  58308 / 60421 loss=1.309, ppl=2.48, wps=16032.2, ups=4.25, wpb=3774.9, bsz=125.1, num_updates=58300, lr=0.000130968, gnorm=1.617, loss_scale=4, train_wall=23, gb_free=10.8, wall=17782
2021-05-01 17:12:04 | INFO | train_inner | epoch 001:  58408 / 60421 loss=1.28, ppl=2.43, wps=15949.9, ups=4.33, wpb=3681.6, bsz=138.6, num_updates=58400, lr=0.000130856, gnorm=1.935, loss_scale=4, train_wall=23, gb_free=10.8, wall=17805
2021-05-01 17:12:27 | INFO | train_inner | epoch 001:  58508 / 60421 loss=1.428, ppl=2.69, wps=16427.1, ups=4.41, wpb=3720.8, bsz=146, num_updates=58500, lr=0.000130744, gnorm=1.965, loss_scale=4, train_wall=22, gb_free=10.8, wall=17828
2021-05-01 17:12:50 | INFO | train_inner | epoch 001:  58608 / 60421 loss=1.302, ppl=2.47, wps=16497.6, ups=4.37, wpb=3771.3, bsz=127.8, num_updates=58600, lr=0.000130632, gnorm=1.576, loss_scale=4, train_wall=23, gb_free=10.7, wall=17851
2021-05-01 17:13:12 | INFO | train_inner | epoch 001:  58708 / 60421 loss=1.326, ppl=2.51, wps=16531.5, ups=4.41, wpb=3748.7, bsz=128.8, num_updates=58700, lr=0.000130521, gnorm=1.7, loss_scale=4, train_wall=22, gb_free=11, wall=17873
2021-05-01 17:13:35 | INFO | train_inner | epoch 001:  58808 / 60421 loss=1.314, ppl=2.49, wps=16325.8, ups=4.45, wpb=3670, bsz=133.6, num_updates=58800, lr=0.00013041, gnorm=1.937, loss_scale=4, train_wall=22, gb_free=10.9, wall=17896
2021-05-01 17:13:57 | INFO | train_inner | epoch 001:  58908 / 60421 loss=1.346, ppl=2.54, wps=16547.6, ups=4.42, wpb=3746.2, bsz=133.3, num_updates=58900, lr=0.000130299, gnorm=1.86, loss_scale=4, train_wall=22, gb_free=10.8, wall=17918
2021-05-01 17:14:20 | INFO | train_inner | epoch 001:  59008 / 60421 loss=1.238, ppl=2.36, wps=16661.4, ups=4.38, wpb=3800.3, bsz=142.6, num_updates=59000, lr=0.000130189, gnorm=1.223, loss_scale=4, train_wall=23, gb_free=11, wall=17941
2021-05-01 17:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:14:20 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:14:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:14:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:14:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:14:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:14:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:14:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:14:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:14:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:14:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:14:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:14:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:14:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:15:26 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.177 | ppl 18.08 | bleu 27.98 | wps 2422.2 | wpb 2024.1 | bsz 97.5 | num_updates 59000 | best_bleu 27.98
2021-05-01 17:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 59000 updates
2021-05-01 17:15:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_59000.pt
2021-05-01 17:15:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_59000.pt
2021-05-01 17:15:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_59000.pt (epoch 1 @ 59000 updates, score 27.98) (writing took 14.357723339999211 seconds)
2021-05-01 17:16:03 | INFO | train_inner | epoch 001:  59108 / 60421 loss=1.28, ppl=2.43, wps=3585.8, ups=0.97, wpb=3690.3, bsz=119.7, num_updates=59100, lr=0.000130079, gnorm=1.787, loss_scale=4, train_wall=23, gb_free=10.7, wall=18044
2021-05-01 17:16:26 | INFO | train_inner | epoch 001:  59208 / 60421 loss=1.223, ppl=2.33, wps=16260.7, ups=4.34, wpb=3743.8, bsz=140.6, num_updates=59200, lr=0.000129969, gnorm=1.442, loss_scale=4, train_wall=23, gb_free=10.8, wall=18067
2021-05-01 17:16:49 | INFO | train_inner | epoch 001:  59308 / 60421 loss=1.362, ppl=2.57, wps=15819.9, ups=4.31, wpb=3668.9, bsz=131.4, num_updates=59300, lr=0.000129859, gnorm=1.99, loss_scale=4, train_wall=23, gb_free=11.1, wall=18090
2021-05-01 17:17:12 | INFO | train_inner | epoch 001:  59408 / 60421 loss=1.258, ppl=2.39, wps=16390.3, ups=4.31, wpb=3798.6, bsz=127.6, num_updates=59400, lr=0.00012975, gnorm=1.316, loss_scale=4, train_wall=23, gb_free=10.8, wall=18113
2021-05-01 17:17:35 | INFO | train_inner | epoch 001:  59508 / 60421 loss=1.302, ppl=2.47, wps=16594.1, ups=4.42, wpb=3758, bsz=116, num_updates=59500, lr=0.000129641, gnorm=1.575, loss_scale=4, train_wall=22, gb_free=10.7, wall=18136
2021-05-01 17:17:58 | INFO | train_inner | epoch 001:  59608 / 60421 loss=1.218, ppl=2.33, wps=16456.4, ups=4.4, wpb=3736.9, bsz=112.8, num_updates=59600, lr=0.000129532, gnorm=1.306, loss_scale=4, train_wall=23, gb_free=10.7, wall=18159
2021-05-01 17:18:20 | INFO | train_inner | epoch 001:  59708 / 60421 loss=1.258, ppl=2.39, wps=16420.1, ups=4.42, wpb=3711.8, bsz=131.4, num_updates=59700, lr=0.000129423, gnorm=1.842, loss_scale=4, train_wall=22, gb_free=10.9, wall=18181
2021-05-01 17:18:43 | INFO | train_inner | epoch 001:  59808 / 60421 loss=1.346, ppl=2.54, wps=16272, ups=4.46, wpb=3647.2, bsz=115.6, num_updates=59800, lr=0.000129315, gnorm=2.111, loss_scale=4, train_wall=22, gb_free=10.8, wall=18204
2021-05-01 17:19:06 | INFO | train_inner | epoch 001:  59908 / 60421 loss=1.315, ppl=2.49, wps=16327.1, ups=4.39, wpb=3720.4, bsz=128.8, num_updates=59900, lr=0.000129207, gnorm=1.822, loss_scale=4, train_wall=23, gb_free=10.7, wall=18227
2021-05-01 17:19:28 | INFO | train_inner | epoch 001:  60008 / 60421 loss=1.376, ppl=2.6, wps=16344.7, ups=4.43, wpb=3685.6, bsz=140.9, num_updates=60000, lr=0.000129099, gnorm=2.404, loss_scale=4, train_wall=22, gb_free=11.1, wall=18249
2021-05-01 17:19:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:19:28 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:19:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:19:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:19:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:19:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:19:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:19:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:19:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:19:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:19:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:19:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:19:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:19:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:20:34 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.153 | ppl 17.79 | bleu 26.78 | wps 2415.5 | wpb 2024.1 | bsz 97.5 | num_updates 60000 | best_bleu 27.98
2021-05-01 17:20:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 60000 updates
2021-05-01 17:20:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_60000.pt
2021-05-01 17:20:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_60000.pt
2021-05-01 17:20:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_1_60000.pt (epoch 1 @ 60000 updates, score 26.78) (writing took 7.974179595999885 seconds)
2021-05-01 17:21:05 | INFO | train_inner | epoch 001:  60108 / 60421 loss=1.296, ppl=2.46, wps=3832.1, ups=1.03, wpb=3705.9, bsz=127.5, num_updates=60100, lr=0.000128992, gnorm=1.975, loss_scale=4, train_wall=23, gb_free=10.8, wall=18346
2021-05-01 17:21:28 | INFO | train_inner | epoch 001:  60208 / 60421 loss=1.312, ppl=2.48, wps=16251.3, ups=4.3, wpb=3778.7, bsz=138, num_updates=60200, lr=0.000128885, gnorm=1.508, loss_scale=4, train_wall=23, gb_free=10.8, wall=18369
2021-05-01 17:21:52 | INFO | train_inner | epoch 001:  60308 / 60421 loss=1.327, ppl=2.51, wps=15934.5, ups=4.25, wpb=3750.9, bsz=112.8, num_updates=60300, lr=0.000128778, gnorm=1.569, loss_scale=4, train_wall=23, gb_free=10.7, wall=18393
2021-05-01 17:22:15 | INFO | train_inner | epoch 001:  60408 / 60421 loss=1.345, ppl=2.54, wps=16306.1, ups=4.3, wpb=3789.1, bsz=123.9, num_updates=60400, lr=0.000128671, gnorm=1.698, loss_scale=4, train_wall=23, gb_free=10.7, wall=18416
2021-05-01 17:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:22:18 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:22:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:22:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:22:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:22:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:22:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:22:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:22:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:22:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:22:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:22:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:22:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:22:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:23:23 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.192 | ppl 18.27 | bleu 27.41 | wps 2427.5 | wpb 2024.1 | bsz 97.5 | num_updates 60413 | best_bleu 27.98
2021-05-01 17:23:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 60413 updates
2021-05-01 17:23:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint1.pt
2021-05-01 17:23:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint1.pt
2021-05-01 17:23:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint1.pt (epoch 1 @ 60413 updates, score 27.41) (writing took 7.944408765004482 seconds)
2021-05-01 17:23:31 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-05-01 17:23:31 | INFO | train | epoch 001 | loss 1.732 | ppl 3.32 | wps 12213 | ups 3.27 | wpb 3737.5 | bsz 132.4 | num_updates 60413 | lr 0.000128657 | gnorm 1.878 | loss_scale 4 | train_wall 13706 | gb_free 10.8 | wall 18492
2021-05-01 17:23:31 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:23:31 | INFO | fairseq.trainer | begin training epoch 2
2021-05-01 17:23:31 | INFO | fairseq_cli.train | Start iterating over samples
2021-05-01 17:23:51 | INFO | train_inner | epoch 002:     87 / 60421 loss=1.149, ppl=2.22, wps=3888.4, ups=1.04, wpb=3742.8, bsz=139.8, num_updates=60500, lr=0.000128565, gnorm=1.393, loss_scale=4, train_wall=23, gb_free=10.9, wall=18512
2021-05-01 17:24:14 | INFO | train_inner | epoch 002:    187 / 60421 loss=1.093, ppl=2.13, wps=16425.5, ups=4.44, wpb=3697.9, bsz=153.9, num_updates=60600, lr=0.000128459, gnorm=1.437, loss_scale=4, train_wall=22, gb_free=10.8, wall=18535
2021-05-01 17:24:37 | INFO | train_inner | epoch 002:    287 / 60421 loss=1.155, ppl=2.23, wps=16506.4, ups=4.37, wpb=3781, bsz=133.6, num_updates=60700, lr=0.000128353, gnorm=1.385, loss_scale=4, train_wall=23, gb_free=10.7, wall=18558
2021-05-01 17:24:59 | INFO | train_inner | epoch 002:    387 / 60421 loss=1.195, ppl=2.29, wps=16432.7, ups=4.4, wpb=3731, bsz=120.5, num_updates=60800, lr=0.000128247, gnorm=1.433, loss_scale=4, train_wall=23, gb_free=10.8, wall=18580
2021-05-01 17:25:22 | INFO | train_inner | epoch 002:    487 / 60421 loss=1.106, ppl=2.15, wps=16297.2, ups=4.39, wpb=3712.8, bsz=137.7, num_updates=60900, lr=0.000128142, gnorm=1.331, loss_scale=4, train_wall=23, gb_free=11.5, wall=18603
2021-05-01 17:25:45 | INFO | train_inner | epoch 002:    587 / 60421 loss=1.129, ppl=2.19, wps=16325.2, ups=4.29, wpb=3805.9, bsz=137.8, num_updates=61000, lr=0.000128037, gnorm=1.357, loss_scale=4, train_wall=23, gb_free=10.8, wall=18626
2021-05-01 17:25:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:25:45 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:25:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:25:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:25:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:25:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:25:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:25:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:26:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:26:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:26:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:26:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:26:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:26:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:26:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.244 | ppl 18.95 | bleu 28.03 | wps 2378.6 | wpb 2024.1 | bsz 97.5 | num_updates 61000 | best_bleu 28.03
2021-05-01 17:26:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 61000 updates
2021-05-01 17:26:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_61000.pt
2021-05-01 17:26:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_61000.pt
2021-05-01 17:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_61000.pt (epoch 2 @ 61000 updates, score 28.03) (writing took 14.479153278996819 seconds)
2021-05-01 17:27:29 | INFO | train_inner | epoch 002:    687 / 60421 loss=1.159, ppl=2.23, wps=3578.9, ups=0.96, wpb=3714.2, bsz=127.7, num_updates=61100, lr=0.000127932, gnorm=1.338, loss_scale=4, train_wall=22, gb_free=10.9, wall=18730
2021-05-01 17:27:52 | INFO | train_inner | epoch 002:    787 / 60421 loss=1.25, ppl=2.38, wps=16566.8, ups=4.4, wpb=3768.4, bsz=120.4, num_updates=61200, lr=0.000127827, gnorm=1.605, loss_scale=4, train_wall=23, gb_free=10.7, wall=18753
2021-05-01 17:28:15 | INFO | train_inner | epoch 002:    887 / 60421 loss=1.192, ppl=2.28, wps=16522.6, ups=4.41, wpb=3747.7, bsz=131.4, num_updates=61300, lr=0.000127723, gnorm=1.543, loss_scale=4, train_wall=22, gb_free=10.6, wall=18776
2021-05-01 17:28:37 | INFO | train_inner | epoch 002:    987 / 60421 loss=1.188, ppl=2.28, wps=16380.7, ups=4.42, wpb=3706.6, bsz=130.2, num_updates=61400, lr=0.000127619, gnorm=1.54, loss_scale=4, train_wall=22, gb_free=10.8, wall=18798
2021-05-01 17:29:00 | INFO | train_inner | epoch 002:   1087 / 60421 loss=1.122, ppl=2.18, wps=16434.6, ups=4.43, wpb=3713.5, bsz=155.8, num_updates=61500, lr=0.000127515, gnorm=1.513, loss_scale=4, train_wall=22, gb_free=10.9, wall=18821
2021-05-01 17:29:23 | INFO | train_inner | epoch 002:   1187 / 60421 loss=1.129, ppl=2.19, wps=16562.7, ups=4.37, wpb=3786.3, bsz=139.8, num_updates=61600, lr=0.000127412, gnorm=1.26, loss_scale=4, train_wall=23, gb_free=10.7, wall=18844
2021-05-01 17:29:45 | INFO | train_inner | epoch 002:   1287 / 60421 loss=1.251, ppl=2.38, wps=16191.3, ups=4.45, wpb=3636.3, bsz=133.8, num_updates=61700, lr=0.000127309, gnorm=1.93, loss_scale=4, train_wall=22, gb_free=11.1, wall=18866
2021-05-01 17:30:08 | INFO | train_inner | epoch 002:   1387 / 60421 loss=1.146, ppl=2.21, wps=16383.1, ups=4.37, wpb=3752.9, bsz=139.1, num_updates=61800, lr=0.000127205, gnorm=1.545, loss_scale=4, train_wall=23, gb_free=10.9, wall=18889
2021-05-01 17:30:31 | INFO | train_inner | epoch 002:   1487 / 60421 loss=1.194, ppl=2.29, wps=16162.8, ups=4.32, wpb=3743.1, bsz=131.8, num_updates=61900, lr=0.000127103, gnorm=1.635, loss_scale=4, train_wall=23, gb_free=10.8, wall=18912
2021-05-01 17:30:54 | INFO | train_inner | epoch 002:   1587 / 60421 loss=1.184, ppl=2.27, wps=16202.2, ups=4.31, wpb=3758.6, bsz=128.6, num_updates=62000, lr=0.000127, gnorm=1.474, loss_scale=4, train_wall=23, gb_free=10.7, wall=18935
2021-05-01 17:30:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:30:54 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:31:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:31:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:31:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:31:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:31:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:31:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:31:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:31:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:31:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:31:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:31:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:31:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:31:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:31:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:31:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:32:00 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.197 | ppl 18.34 | bleu 28.05 | wps 2405.7 | wpb 2024.1 | bsz 97.5 | num_updates 62000 | best_bleu 28.05
2021-05-01 17:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 62000 updates
2021-05-01 17:32:00 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_62000.pt
2021-05-01 17:32:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_62000.pt
2021-05-01 17:32:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_62000.pt (epoch 2 @ 62000 updates, score 28.05) (writing took 14.361362269999518 seconds)
2021-05-01 17:32:37 | INFO | train_inner | epoch 002:   1687 / 60421 loss=1.114, ppl=2.17, wps=3619, ups=0.97, wpb=3727.2, bsz=156.9, num_updates=62100, lr=0.000126898, gnorm=1.522, loss_scale=4, train_wall=22, gb_free=10.9, wall=19038
2021-05-01 17:33:00 | INFO | train_inner | epoch 002:   1787 / 60421 loss=1.174, ppl=2.26, wps=16588.1, ups=4.44, wpb=3734.5, bsz=126.2, num_updates=62200, lr=0.000126796, gnorm=1.443, loss_scale=4, train_wall=22, gb_free=11, wall=19061
2021-05-01 17:33:22 | INFO | train_inner | epoch 002:   1887 / 60421 loss=1.158, ppl=2.23, wps=16685.8, ups=4.44, wpb=3755, bsz=123.6, num_updates=62300, lr=0.000126694, gnorm=1.414, loss_scale=4, train_wall=22, gb_free=11, wall=19083
2021-05-01 17:33:45 | INFO | train_inner | epoch 002:   1987 / 60421 loss=1.099, ppl=2.14, wps=16645.1, ups=4.4, wpb=3781.5, bsz=144.3, num_updates=62400, lr=0.000126592, gnorm=1.302, loss_scale=4, train_wall=23, gb_free=10.9, wall=19106
2021-05-01 17:34:08 | INFO | train_inner | epoch 002:   2087 / 60421 loss=1.281, ppl=2.43, wps=16749.8, ups=4.4, wpb=3806.8, bsz=111.5, num_updates=62500, lr=0.000126491, gnorm=1.546, loss_scale=4, train_wall=23, gb_free=10.6, wall=19129
2021-05-01 17:34:30 | INFO | train_inner | epoch 002:   2187 / 60421 loss=1.224, ppl=2.34, wps=16503.4, ups=4.43, wpb=3722.8, bsz=117.3, num_updates=62600, lr=0.00012639, gnorm=1.615, loss_scale=4, train_wall=22, gb_free=10.9, wall=19151
2021-05-01 17:34:53 | INFO | train_inner | epoch 002:   2287 / 60421 loss=1.146, ppl=2.21, wps=16468.2, ups=4.34, wpb=3792, bsz=130, num_updates=62700, lr=0.000126289, gnorm=1.506, loss_scale=4, train_wall=23, gb_free=10.7, wall=19174
2021-05-01 17:35:16 | INFO | train_inner | epoch 002:   2387 / 60421 loss=1.212, ppl=2.32, wps=16307.9, ups=4.38, wpb=3722.1, bsz=129.8, num_updates=62800, lr=0.000126189, gnorm=1.751, loss_scale=4, train_wall=23, gb_free=10.7, wall=19197
2021-05-01 17:35:39 | INFO | train_inner | epoch 002:   2487 / 60421 loss=1.186, ppl=2.28, wps=16334, ups=4.34, wpb=3767.5, bsz=124.3, num_updates=62900, lr=0.000126088, gnorm=1.513, loss_scale=4, train_wall=23, gb_free=11, wall=19220
2021-05-01 17:36:03 | INFO | train_inner | epoch 002:   2587 / 60421 loss=1.198, ppl=2.29, wps=16012, ups=4.29, wpb=3733.2, bsz=135.4, num_updates=63000, lr=0.000125988, gnorm=1.539, loss_scale=4, train_wall=23, gb_free=10.7, wall=19244
2021-05-01 17:36:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:36:03 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:36:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:36:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:36:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:36:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:36:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:36:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:36:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:36:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:36:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:37:08 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.313 | ppl 19.88 | bleu 27.64 | wps 2417 | wpb 2024.1 | bsz 97.5 | num_updates 63000 | best_bleu 28.05
2021-05-01 17:37:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 63000 updates
2021-05-01 17:37:08 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_63000.pt
2021-05-01 17:37:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_63000.pt
2021-05-01 17:37:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_63000.pt (epoch 2 @ 63000 updates, score 27.64) (writing took 7.864367331996618 seconds)
2021-05-01 17:37:39 | INFO | train_inner | epoch 002:   2687 / 60421 loss=1.175, ppl=2.26, wps=3879.3, ups=1.04, wpb=3735.6, bsz=126.9, num_updates=63100, lr=0.000125888, gnorm=1.503, loss_scale=4, train_wall=22, gb_free=10.7, wall=19340
2021-05-01 17:38:02 | INFO | train_inner | epoch 002:   2787 / 60421 loss=1.168, ppl=2.25, wps=16678, ups=4.35, wpb=3836.4, bsz=139.1, num_updates=63200, lr=0.000125789, gnorm=1.45, loss_scale=4, train_wall=23, gb_free=10.7, wall=19363
2021-05-01 17:38:25 | INFO | train_inner | epoch 002:   2887 / 60421 loss=1.14, ppl=2.2, wps=16406.2, ups=4.43, wpb=3703.2, bsz=131.1, num_updates=63300, lr=0.000125689, gnorm=1.652, loss_scale=4, train_wall=22, gb_free=10.8, wall=19385
2021-05-01 17:38:47 | INFO | train_inner | epoch 002:   2987 / 60421 loss=1.22, ppl=2.33, wps=16534.9, ups=4.43, wpb=3736.2, bsz=121.2, num_updates=63400, lr=0.00012559, gnorm=1.405, loss_scale=4, train_wall=22, gb_free=10.8, wall=19408
2021-05-01 17:39:10 | INFO | train_inner | epoch 002:   3087 / 60421 loss=1.168, ppl=2.25, wps=16612.9, ups=4.39, wpb=3785, bsz=129.2, num_updates=63500, lr=0.000125491, gnorm=1.295, loss_scale=4, train_wall=23, gb_free=10.9, wall=19431
2021-05-01 17:39:33 | INFO | train_inner | epoch 002:   3187 / 60421 loss=1.153, ppl=2.22, wps=16403.4, ups=4.4, wpb=3730.8, bsz=145.2, num_updates=63600, lr=0.000125392, gnorm=1.581, loss_scale=4, train_wall=23, gb_free=10.9, wall=19454
2021-05-01 17:39:56 | INFO | train_inner | epoch 002:   3287 / 60421 loss=1.141, ppl=2.21, wps=16530.5, ups=4.34, wpb=3806.2, bsz=141.4, num_updates=63700, lr=0.000125294, gnorm=1.231, loss_scale=4, train_wall=23, gb_free=10.7, wall=19477
2021-05-01 17:40:18 | INFO | train_inner | epoch 002:   3387 / 60421 loss=1.145, ppl=2.21, wps=16106.6, ups=4.41, wpb=3652.8, bsz=132.2, num_updates=63800, lr=0.000125196, gnorm=1.559, loss_scale=4, train_wall=23, gb_free=10.7, wall=19499
2021-05-01 17:40:41 | INFO | train_inner | epoch 002:   3487 / 60421 loss=1.21, ppl=2.31, wps=16419.9, ups=4.33, wpb=3791.7, bsz=133.4, num_updates=63900, lr=0.000125098, gnorm=1.516, loss_scale=4, train_wall=23, gb_free=11, wall=19522
2021-05-01 17:41:05 | INFO | train_inner | epoch 002:   3587 / 60421 loss=1.152, ppl=2.22, wps=16100.6, ups=4.29, wpb=3753.1, bsz=133.6, num_updates=64000, lr=0.000125, gnorm=1.32, loss_scale=4, train_wall=23, gb_free=10.7, wall=19546
2021-05-01 17:41:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:41:05 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:41:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:41:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:41:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:41:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:41:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:41:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:41:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:41:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:41:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:41:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:41:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:41:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:41:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:41:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:41:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:42:10 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.29 | ppl 19.56 | bleu 27.97 | wps 2442.1 | wpb 2024.1 | bsz 97.5 | num_updates 64000 | best_bleu 28.05
2021-05-01 17:42:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 64000 updates
2021-05-01 17:42:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_64000.pt
2021-05-01 17:42:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_64000.pt
2021-05-01 17:42:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_64000.pt (epoch 2 @ 64000 updates, score 27.97) (writing took 7.623133588997007 seconds)
2021-05-01 17:42:40 | INFO | train_inner | epoch 002:   3687 / 60421 loss=1.216, ppl=2.32, wps=3980.1, ups=1.05, wpb=3799.2, bsz=124.7, num_updates=64100, lr=0.000124902, gnorm=1.436, loss_scale=4, train_wall=23, gb_free=10.8, wall=19641
2021-05-01 17:43:03 | INFO | train_inner | epoch 002:   3787 / 60421 loss=1.209, ppl=2.31, wps=16520.1, ups=4.44, wpb=3717.6, bsz=102.4, num_updates=64200, lr=0.000124805, gnorm=1.407, loss_scale=4, train_wall=22, gb_free=10.9, wall=19664
2021-05-01 17:43:25 | INFO | train_inner | epoch 002:   3887 / 60421 loss=1.102, ppl=2.15, wps=16631.4, ups=4.46, wpb=3726.1, bsz=128.3, num_updates=64300, lr=0.000124708, gnorm=1.367, loss_scale=4, train_wall=22, gb_free=10.8, wall=19686
2021-05-01 17:43:48 | INFO | train_inner | epoch 002:   3987 / 60421 loss=1.188, ppl=2.28, wps=16672.5, ups=4.43, wpb=3760.4, bsz=117.5, num_updates=64400, lr=0.000124611, gnorm=1.376, loss_scale=4, train_wall=22, gb_free=10.8, wall=19709
2021-05-01 17:44:10 | INFO | train_inner | epoch 002:   4087 / 60421 loss=1.065, ppl=2.09, wps=16466.8, ups=4.41, wpb=3737.9, bsz=163.1, num_updates=64500, lr=0.000124515, gnorm=1.442, loss_scale=4, train_wall=23, gb_free=10.8, wall=19731
2021-05-01 17:44:33 | INFO | train_inner | epoch 002:   4187 / 60421 loss=1.187, ppl=2.28, wps=16594.9, ups=4.46, wpb=3718.9, bsz=139, num_updates=64600, lr=0.000124418, gnorm=1.603, loss_scale=4, train_wall=22, gb_free=10.8, wall=19754
2021-05-01 17:44:55 | INFO | train_inner | epoch 002:   4287 / 60421 loss=1.107, ppl=2.15, wps=16599.5, ups=4.43, wpb=3748.8, bsz=149.4, num_updates=64700, lr=0.000124322, gnorm=1.605, loss_scale=4, train_wall=22, gb_free=10.8, wall=19776
2021-05-01 17:45:18 | INFO | train_inner | epoch 002:   4387 / 60421 loss=1.257, ppl=2.39, wps=16665, ups=4.47, wpb=3731.2, bsz=115.8, num_updates=64800, lr=0.000124226, gnorm=1.704, loss_scale=4, train_wall=22, gb_free=10.8, wall=19799
2021-05-01 17:45:40 | INFO | train_inner | epoch 002:   4487 / 60421 loss=1.157, ppl=2.23, wps=16770.4, ups=4.42, wpb=3792.8, bsz=129, num_updates=64900, lr=0.00012413, gnorm=1.333, loss_scale=4, train_wall=22, gb_free=10.8, wall=19821
2021-05-01 17:46:03 | INFO | train_inner | epoch 002:   4587 / 60421 loss=1.145, ppl=2.21, wps=16494.3, ups=4.47, wpb=3687.3, bsz=134.2, num_updates=65000, lr=0.000124035, gnorm=1.641, loss_scale=4, train_wall=22, gb_free=11.6, wall=19844
2021-05-01 17:46:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:46:03 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:46:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:46:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:46:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:46:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:46:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:46:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:46:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:46:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:46:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:46:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:46:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:46:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:47:07 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.32 | ppl 19.97 | bleu 27.71 | wps 2451.3 | wpb 2024.1 | bsz 97.5 | num_updates 65000 | best_bleu 28.05
2021-05-01 17:47:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 65000 updates
2021-05-01 17:47:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_65000.pt
2021-05-01 17:47:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_65000.pt
2021-05-01 17:47:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_65000.pt (epoch 2 @ 65000 updates, score 27.71) (writing took 8.255740139000409 seconds)
2021-05-01 17:47:38 | INFO | train_inner | epoch 002:   4687 / 60421 loss=1.145, ppl=2.21, wps=3919.2, ups=1.04, wpb=3750.8, bsz=126.4, num_updates=65100, lr=0.000123939, gnorm=1.412, loss_scale=4, train_wall=23, gb_free=11.3, wall=19939
2021-05-01 17:48:01 | INFO | train_inner | epoch 002:   4787 / 60421 loss=1.178, ppl=2.26, wps=16733.5, ups=4.43, wpb=3780.4, bsz=129.5, num_updates=65200, lr=0.000123844, gnorm=1.302, loss_scale=4, train_wall=22, gb_free=10.9, wall=19962
2021-05-01 17:48:24 | INFO | train_inner | epoch 002:   4887 / 60421 loss=1.092, ppl=2.13, wps=16606.8, ups=4.42, wpb=3760.3, bsz=145.8, num_updates=65300, lr=0.000123749, gnorm=1.199, loss_scale=4, train_wall=22, gb_free=11, wall=19985
2021-05-01 17:48:46 | INFO | train_inner | epoch 002:   4987 / 60421 loss=1.137, ppl=2.2, wps=16589.1, ups=4.42, wpb=3751.6, bsz=136, num_updates=65400, lr=0.000123655, gnorm=1.329, loss_scale=4, train_wall=22, gb_free=10.8, wall=20007
2021-05-01 17:49:09 | INFO | train_inner | epoch 002:   5087 / 60421 loss=1.148, ppl=2.22, wps=16586.6, ups=4.41, wpb=3763.8, bsz=135.4, num_updates=65500, lr=0.00012356, gnorm=1.335, loss_scale=4, train_wall=23, gb_free=10.8, wall=20030
2021-05-01 17:49:32 | INFO | train_inner | epoch 002:   5187 / 60421 loss=1.149, ppl=2.22, wps=16497.1, ups=4.42, wpb=3733.8, bsz=128.5, num_updates=65600, lr=0.000123466, gnorm=1.607, loss_scale=4, train_wall=22, gb_free=10.9, wall=20053
2021-05-01 17:49:54 | INFO | train_inner | epoch 002:   5287 / 60421 loss=1.198, ppl=2.29, wps=16754.3, ups=4.38, wpb=3821.8, bsz=142.4, num_updates=65700, lr=0.000123372, gnorm=1.436, loss_scale=4, train_wall=23, gb_free=10.8, wall=20075
2021-05-01 17:50:17 | INFO | train_inner | epoch 002:   5387 / 60421 loss=1.154, ppl=2.23, wps=16416, ups=4.44, wpb=3700.8, bsz=149, num_updates=65800, lr=0.000123278, gnorm=1.577, loss_scale=4, train_wall=22, gb_free=10.8, wall=20098
2021-05-01 17:50:40 | INFO | train_inner | epoch 002:   5487 / 60421 loss=1.199, ppl=2.3, wps=16219.9, ups=4.34, wpb=3734.1, bsz=136.3, num_updates=65900, lr=0.000123185, gnorm=1.589, loss_scale=4, train_wall=23, gb_free=10.7, wall=20121
2021-05-01 17:51:03 | INFO | train_inner | epoch 002:   5587 / 60421 loss=1.135, ppl=2.2, wps=16276.3, ups=4.3, wpb=3785.7, bsz=135.3, num_updates=66000, lr=0.000123091, gnorm=1.314, loss_scale=4, train_wall=23, gb_free=10.8, wall=20144
2021-05-01 17:51:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:51:03 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:51:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:51:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:51:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:51:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:51:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:51:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:51:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:51:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:51:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:51:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:51:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:51:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:51:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:51:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:51:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:52:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.325 | ppl 20.05 | bleu 28.09 | wps 2412.7 | wpb 2024.1 | bsz 97.5 | num_updates 66000 | best_bleu 28.09
2021-05-01 17:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 66000 updates
2021-05-01 17:52:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_66000.pt
2021-05-01 17:52:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_66000.pt
2021-05-01 17:52:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_66000.pt (epoch 2 @ 66000 updates, score 28.09) (writing took 14.36791287399683 seconds)
2021-05-01 17:52:46 | INFO | train_inner | epoch 002:   5687 / 60421 loss=1.136, ppl=2.2, wps=3644.6, ups=0.97, wpb=3751.9, bsz=129.9, num_updates=66100, lr=0.000122998, gnorm=1.361, loss_scale=4, train_wall=23, gb_free=10.8, wall=20247
2021-05-01 17:53:09 | INFO | train_inner | epoch 002:   5787 / 60421 loss=1.103, ppl=2.15, wps=16423, ups=4.44, wpb=3697.2, bsz=141.4, num_updates=66200, lr=0.000122905, gnorm=1.403, loss_scale=4, train_wall=22, gb_free=10.7, wall=20270
2021-05-01 17:53:31 | INFO | train_inner | epoch 002:   5887 / 60421 loss=1.22, ppl=2.33, wps=16481.5, ups=4.5, wpb=3666.6, bsz=123.8, num_updates=66300, lr=0.000122813, gnorm=1.726, loss_scale=4, train_wall=22, gb_free=10.7, wall=20292
2021-05-01 17:53:54 | INFO | train_inner | epoch 002:   5987 / 60421 loss=1.171, ppl=2.25, wps=16537.8, ups=4.38, wpb=3777.5, bsz=120.1, num_updates=66400, lr=0.00012272, gnorm=1.439, loss_scale=4, train_wall=23, gb_free=10.8, wall=20315
2021-05-01 17:54:16 | INFO | train_inner | epoch 002:   6087 / 60421 loss=1.149, ppl=2.22, wps=16488.1, ups=4.43, wpb=3724.6, bsz=136, num_updates=66500, lr=0.000122628, gnorm=1.365, loss_scale=4, train_wall=22, gb_free=10.8, wall=20337
2021-05-01 17:54:39 | INFO | train_inner | epoch 002:   6187 / 60421 loss=1.174, ppl=2.26, wps=16461.8, ups=4.43, wpb=3720, bsz=127, num_updates=66600, lr=0.000122536, gnorm=1.451, loss_scale=4, train_wall=22, gb_free=10.7, wall=20360
2021-05-01 17:55:02 | INFO | train_inner | epoch 002:   6287 / 60421 loss=1.098, ppl=2.14, wps=16418.1, ups=4.36, wpb=3767.4, bsz=132.3, num_updates=66700, lr=0.000122444, gnorm=1.321, loss_scale=4, train_wall=23, gb_free=10.9, wall=20383
2021-05-01 17:55:25 | INFO | train_inner | epoch 002:   6387 / 60421 loss=1.134, ppl=2.19, wps=16356.1, ups=4.33, wpb=3775.2, bsz=133.6, num_updates=66800, lr=0.000122352, gnorm=1.318, loss_scale=4, train_wall=23, gb_free=10.9, wall=20406
2021-05-01 17:55:48 | INFO | train_inner | epoch 002:   6487 / 60421 loss=1.152, ppl=2.22, wps=16397.6, ups=4.31, wpb=3804.8, bsz=150.2, num_updates=66900, lr=0.000122261, gnorm=1.352, loss_scale=4, train_wall=23, gb_free=10.8, wall=20429
2021-05-01 17:56:11 | INFO | train_inner | epoch 002:   6587 / 60421 loss=1.15, ppl=2.22, wps=15920.7, ups=4.37, wpb=3646.7, bsz=122.6, num_updates=67000, lr=0.000122169, gnorm=1.65, loss_scale=4, train_wall=23, gb_free=10.7, wall=20452
2021-05-01 17:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 17:56:11 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 17:56:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:56:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:56:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:56:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:56:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:56:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:56:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:56:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:56:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:56:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:56:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:56:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:56:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 17:56:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 17:56:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 17:57:17 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.402 | ppl 21.15 | bleu 28.05 | wps 2423.2 | wpb 2024.1 | bsz 97.5 | num_updates 67000 | best_bleu 28.09
2021-05-01 17:57:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 67000 updates
2021-05-01 17:57:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_67000.pt
2021-05-01 17:57:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_67000.pt
2021-05-01 17:57:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_67000.pt (epoch 2 @ 67000 updates, score 28.05) (writing took 7.724351785000181 seconds)
2021-05-01 17:57:47 | INFO | train_inner | epoch 002:   6687 / 60421 loss=1.24, ppl=2.36, wps=3931.2, ups=1.04, wpb=3781.3, bsz=123, num_updates=67100, lr=0.000122078, gnorm=1.502, loss_scale=4, train_wall=23, gb_free=10.8, wall=20548
2021-05-01 17:58:10 | INFO | train_inner | epoch 002:   6787 / 60421 loss=1.161, ppl=2.24, wps=16401.5, ups=4.42, wpb=3711.3, bsz=125.3, num_updates=67200, lr=0.000121988, gnorm=1.468, loss_scale=4, train_wall=22, gb_free=10.8, wall=20571
2021-05-01 17:58:32 | INFO | train_inner | epoch 002:   6887 / 60421 loss=1.141, ppl=2.2, wps=16526.6, ups=4.45, wpb=3713.3, bsz=129.9, num_updates=67300, lr=0.000121897, gnorm=1.57, loss_scale=4, train_wall=22, gb_free=10.9, wall=20593
2021-05-01 17:58:55 | INFO | train_inner | epoch 002:   6987 / 60421 loss=1.119, ppl=2.17, wps=16412.5, ups=4.44, wpb=3697.9, bsz=143.3, num_updates=67400, lr=0.000121806, gnorm=1.513, loss_scale=4, train_wall=22, gb_free=10.9, wall=20616
2021-05-01 17:59:18 | INFO | train_inner | epoch 002:   7087 / 60421 loss=1.161, ppl=2.24, wps=16494.7, ups=4.38, wpb=3763.3, bsz=141.8, num_updates=67500, lr=0.000121716, gnorm=1.476, loss_scale=4, train_wall=23, gb_free=10.7, wall=20639
2021-05-01 17:59:40 | INFO | train_inner | epoch 002:   7187 / 60421 loss=1.153, ppl=2.22, wps=16517.4, ups=4.4, wpb=3754.5, bsz=138.9, num_updates=67600, lr=0.000121626, gnorm=1.579, loss_scale=4, train_wall=23, gb_free=11, wall=20661
2021-05-01 18:00:03 | INFO | train_inner | epoch 002:   7287 / 60421 loss=1.193, ppl=2.29, wps=16317.1, ups=4.38, wpb=3722.1, bsz=122.9, num_updates=67700, lr=0.000121536, gnorm=1.532, loss_scale=4, train_wall=23, gb_free=11.3, wall=20684
2021-05-01 18:00:26 | INFO | train_inner | epoch 002:   7387 / 60421 loss=1.134, ppl=2.19, wps=16125.9, ups=4.4, wpb=3661.8, bsz=141.4, num_updates=67800, lr=0.000121447, gnorm=1.643, loss_scale=4, train_wall=23, gb_free=10.8, wall=20707
2021-05-01 18:00:49 | INFO | train_inner | epoch 002:   7487 / 60421 loss=1.128, ppl=2.19, wps=16202.9, ups=4.3, wpb=3772.3, bsz=135.6, num_updates=67900, lr=0.000121357, gnorm=1.282, loss_scale=4, train_wall=23, gb_free=10.7, wall=20730
2021-05-01 18:01:13 | INFO | train_inner | epoch 002:   7587 / 60421 loss=1.148, ppl=2.22, wps=16083, ups=4.24, wpb=3789.1, bsz=131.6, num_updates=68000, lr=0.000121268, gnorm=1.281, loss_scale=4, train_wall=23, gb_free=11, wall=20754
2021-05-01 18:01:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:01:13 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:01:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:01:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:01:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:01:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:01:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:01:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:01:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:01:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:01:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:01:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:01:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:01:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:02:18 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.378 | ppl 20.8 | bleu 28.25 | wps 2423.6 | wpb 2024.1 | bsz 97.5 | num_updates 68000 | best_bleu 28.25
2021-05-01 18:02:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 68000 updates
2021-05-01 18:02:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_68000.pt
2021-05-01 18:02:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_68000.pt
2021-05-01 18:02:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_68000.pt (epoch 2 @ 68000 updates, score 28.25) (writing took 14.334258089002105 seconds)
2021-05-01 18:02:55 | INFO | train_inner | epoch 002:   7687 / 60421 loss=1.185, ppl=2.27, wps=3630.3, ups=0.98, wpb=3718.8, bsz=135.5, num_updates=68100, lr=0.000121179, gnorm=1.767, loss_scale=4, train_wall=22, gb_free=10.9, wall=20856
2021-05-01 18:03:18 | INFO | train_inner | epoch 002:   7787 / 60421 loss=1.158, ppl=2.23, wps=16514.5, ups=4.41, wpb=3748.4, bsz=132.7, num_updates=68200, lr=0.00012109, gnorm=1.63, loss_scale=4, train_wall=23, gb_free=10.8, wall=20879
2021-05-01 18:03:41 | INFO | train_inner | epoch 002:   7887 / 60421 loss=1.107, ppl=2.15, wps=16601.7, ups=4.38, wpb=3787.7, bsz=153.8, num_updates=68300, lr=0.000121001, gnorm=1.322, loss_scale=4, train_wall=23, gb_free=10.9, wall=20902
2021-05-01 18:04:03 | INFO | train_inner | epoch 002:   7987 / 60421 loss=1.163, ppl=2.24, wps=16656, ups=4.4, wpb=3785.8, bsz=133.8, num_updates=68400, lr=0.000120913, gnorm=1.479, loss_scale=4, train_wall=23, gb_free=11, wall=20924
2021-05-01 18:04:26 | INFO | train_inner | epoch 002:   8087 / 60421 loss=1.039, ppl=2.05, wps=16370.5, ups=4.4, wpb=3722.8, bsz=142.2, num_updates=68500, lr=0.000120824, gnorm=1.301, loss_scale=4, train_wall=23, gb_free=10.7, wall=20947
2021-05-01 18:04:49 | INFO | train_inner | epoch 002:   8187 / 60421 loss=1.115, ppl=2.17, wps=16571.6, ups=4.36, wpb=3799.2, bsz=145.3, num_updates=68600, lr=0.000120736, gnorm=1.233, loss_scale=4, train_wall=23, gb_free=10.9, wall=20970
2021-05-01 18:05:12 | INFO | train_inner | epoch 002:   8287 / 60421 loss=1.152, ppl=2.22, wps=16238.5, ups=4.4, wpb=3693.3, bsz=131.3, num_updates=68700, lr=0.000120648, gnorm=1.444, loss_scale=4, train_wall=23, gb_free=10.8, wall=20993
2021-05-01 18:05:35 | INFO | train_inner | epoch 002:   8387 / 60421 loss=1.165, ppl=2.24, wps=16154, ups=4.32, wpb=3740.4, bsz=127, num_updates=68800, lr=0.000120561, gnorm=1.583, loss_scale=4, train_wall=23, gb_free=11.2, wall=21016
2021-05-01 18:05:58 | INFO | train_inner | epoch 002:   8487 / 60421 loss=1.165, ppl=2.24, wps=16087.6, ups=4.32, wpb=3725.2, bsz=118.9, num_updates=68900, lr=0.000120473, gnorm=1.534, loss_scale=4, train_wall=23, gb_free=10.8, wall=21039
2021-05-01 18:06:22 | INFO | train_inner | epoch 002:   8587 / 60421 loss=1.085, ppl=2.12, wps=16143.9, ups=4.22, wpb=3829, bsz=150.6, num_updates=69000, lr=0.000120386, gnorm=1.175, loss_scale=4, train_wall=24, gb_free=10.9, wall=21063
2021-05-01 18:06:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:06:22 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:06:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:06:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:06:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:06:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:06:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:06:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:06:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:06:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:06:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:06:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:06:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:06:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:06:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:06:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:06:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:07:29 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.343 | ppl 20.29 | bleu 28.5 | wps 2373.8 | wpb 2024.1 | bsz 97.5 | num_updates 69000 | best_bleu 28.5
2021-05-01 18:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 69000 updates
2021-05-01 18:07:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_69000.pt
2021-05-01 18:07:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_69000.pt
2021-05-01 18:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_69000.pt (epoch 2 @ 69000 updates, score 28.5) (writing took 14.37655858999642 seconds)
2021-05-01 18:08:06 | INFO | train_inner | epoch 002:   8687 / 60421 loss=1.134, ppl=2.2, wps=3554.9, ups=0.96, wpb=3692, bsz=139.3, num_updates=69100, lr=0.000120299, gnorm=1.403, loss_scale=4, train_wall=22, gb_free=11.1, wall=21167
2021-05-01 18:08:29 | INFO | train_inner | epoch 002:   8787 / 60421 loss=1.114, ppl=2.16, wps=16605.9, ups=4.39, wpb=3780.9, bsz=141.8, num_updates=69200, lr=0.000120212, gnorm=1.465, loss_scale=4, train_wall=23, gb_free=10.7, wall=21190
2021-05-01 18:08:51 | INFO | train_inner | epoch 002:   8887 / 60421 loss=1.185, ppl=2.27, wps=16573.7, ups=4.4, wpb=3767.9, bsz=129.1, num_updates=69300, lr=0.000120125, gnorm=1.609, loss_scale=4, train_wall=23, gb_free=10.8, wall=21212
2021-05-01 18:09:14 | INFO | train_inner | epoch 002:   8987 / 60421 loss=1.072, ppl=2.1, wps=16382.6, ups=4.44, wpb=3686.5, bsz=135.8, num_updates=69400, lr=0.000120038, gnorm=1.563, loss_scale=4, train_wall=22, gb_free=10.8, wall=21235
2021-05-01 18:09:37 | INFO | train_inner | epoch 002:   9087 / 60421 loss=1.074, ppl=2.11, wps=16469, ups=4.4, wpb=3738.8, bsz=161, num_updates=69500, lr=0.000119952, gnorm=1.269, loss_scale=4, train_wall=23, gb_free=11.2, wall=21257
2021-05-01 18:09:59 | INFO | train_inner | epoch 002:   9187 / 60421 loss=1.144, ppl=2.21, wps=16445.1, ups=4.39, wpb=3744.3, bsz=136.5, num_updates=69600, lr=0.000119866, gnorm=1.416, loss_scale=4, train_wall=23, gb_free=10.8, wall=21280
2021-05-01 18:10:22 | INFO | train_inner | epoch 002:   9287 / 60421 loss=1.145, ppl=2.21, wps=16265.6, ups=4.4, wpb=3697.1, bsz=136.2, num_updates=69700, lr=0.00011978, gnorm=1.59, loss_scale=4, train_wall=23, gb_free=10.9, wall=21303
2021-05-01 18:10:45 | INFO | train_inner | epoch 002:   9387 / 60421 loss=1.136, ppl=2.2, wps=16330, ups=4.34, wpb=3761.8, bsz=129.8, num_updates=69800, lr=0.000119694, gnorm=1.323, loss_scale=4, train_wall=23, gb_free=10.8, wall=21326
2021-05-01 18:11:08 | INFO | train_inner | epoch 002:   9487 / 60421 loss=1.188, ppl=2.28, wps=16032.2, ups=4.29, wpb=3735.6, bsz=128.8, num_updates=69900, lr=0.000119608, gnorm=1.62, loss_scale=4, train_wall=23, gb_free=10.7, wall=21349
2021-05-01 18:11:32 | INFO | train_inner | epoch 002:   9587 / 60421 loss=1.151, ppl=2.22, wps=16013.3, ups=4.25, wpb=3770.9, bsz=127.8, num_updates=70000, lr=0.000119523, gnorm=1.357, loss_scale=4, train_wall=23, gb_free=10.9, wall=21373
2021-05-01 18:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:11:32 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:11:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:11:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:11:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:11:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:11:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:11:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:11:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:11:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:11:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:11:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:11:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:11:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:12:38 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.397 | ppl 21.06 | bleu 28.06 | wps 2389.5 | wpb 2024.1 | bsz 97.5 | num_updates 70000 | best_bleu 28.5
2021-05-01 18:12:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 70000 updates
2021-05-01 18:12:38 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_70000.pt
2021-05-01 18:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_70000.pt
2021-05-01 18:12:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_70000.pt (epoch 2 @ 70000 updates, score 28.06) (writing took 8.259345155995106 seconds)
2021-05-01 18:13:09 | INFO | train_inner | epoch 002:   9687 / 60421 loss=1.222, ppl=2.33, wps=3815.4, ups=1.03, wpb=3710.8, bsz=114.2, num_updates=70100, lr=0.000119438, gnorm=1.896, loss_scale=4, train_wall=22, gb_free=10.9, wall=21470
2021-05-01 18:13:32 | INFO | train_inner | epoch 002:   9787 / 60421 loss=1.113, ppl=2.16, wps=16679.6, ups=4.4, wpb=3790.5, bsz=150.6, num_updates=70200, lr=0.000119352, gnorm=1.375, loss_scale=4, train_wall=23, gb_free=10.8, wall=21493
2021-05-01 18:13:54 | INFO | train_inner | epoch 002:   9887 / 60421 loss=1.196, ppl=2.29, wps=16714.2, ups=4.43, wpb=3777.1, bsz=125.4, num_updates=70300, lr=0.000119268, gnorm=1.525, loss_scale=4, train_wall=22, gb_free=10.7, wall=21515
2021-05-01 18:14:17 | INFO | train_inner | epoch 002:   9987 / 60421 loss=1.117, ppl=2.17, wps=16626, ups=4.38, wpb=3795.1, bsz=141.6, num_updates=70400, lr=0.000119183, gnorm=1.297, loss_scale=4, train_wall=23, gb_free=10.7, wall=21538
2021-05-01 18:14:40 | INFO | train_inner | epoch 002:  10087 / 60421 loss=1.136, ppl=2.2, wps=16392.5, ups=4.39, wpb=3731.3, bsz=124.3, num_updates=70500, lr=0.000119098, gnorm=1.531, loss_scale=4, train_wall=23, gb_free=10.9, wall=21561
2021-05-01 18:15:03 | INFO | train_inner | epoch 002:  10187 / 60421 loss=1.124, ppl=2.18, wps=16457.2, ups=4.4, wpb=3744.4, bsz=135.1, num_updates=70600, lr=0.000119014, gnorm=1.293, loss_scale=4, train_wall=23, gb_free=11, wall=21584
2021-05-01 18:15:26 | INFO | train_inner | epoch 002:  10287 / 60421 loss=1.079, ppl=2.11, wps=16324.1, ups=4.38, wpb=3723.6, bsz=138.7, num_updates=70700, lr=0.00011893, gnorm=1.324, loss_scale=4, train_wall=23, gb_free=10.8, wall=21607
2021-05-01 18:15:49 | INFO | train_inner | epoch 002:  10387 / 60421 loss=1.111, ppl=2.16, wps=16213.4, ups=4.31, wpb=3763.8, bsz=146.3, num_updates=70800, lr=0.000118846, gnorm=1.445, loss_scale=4, train_wall=23, gb_free=10.6, wall=21630
2021-05-01 18:16:12 | INFO | train_inner | epoch 002:  10487 / 60421 loss=1.148, ppl=2.22, wps=16134.1, ups=4.32, wpb=3738.8, bsz=137.8, num_updates=70900, lr=0.000118762, gnorm=1.431, loss_scale=4, train_wall=23, gb_free=10.9, wall=21653
2021-05-01 18:16:36 | INFO | train_inner | epoch 002:  10587 / 60421 loss=1.127, ppl=2.18, wps=15806.5, ups=4.25, wpb=3722.7, bsz=124.9, num_updates=71000, lr=0.000118678, gnorm=1.461, loss_scale=4, train_wall=23, gb_free=10.6, wall=21677
2021-05-01 18:16:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:16:36 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:16:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:16:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:16:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:16:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:16:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:16:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:16:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:16:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:16:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:16:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:16:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:16:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:17:42 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.425 | ppl 21.48 | bleu 28.23 | wps 2377.1 | wpb 2024.1 | bsz 97.5 | num_updates 71000 | best_bleu 28.5
2021-05-01 18:17:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 71000 updates
2021-05-01 18:17:42 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_71000.pt
2021-05-01 18:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_71000.pt
2021-05-01 18:17:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_71000.pt (epoch 2 @ 71000 updates, score 28.23) (writing took 7.63960073099588 seconds)
2021-05-01 18:18:13 | INFO | train_inner | epoch 002:  10687 / 60421 loss=1.127, ppl=2.18, wps=3839.7, ups=1.03, wpb=3729.5, bsz=131.5, num_updates=71100, lr=0.000118595, gnorm=1.399, loss_scale=4, train_wall=22, gb_free=10.8, wall=21774
2021-05-01 18:18:35 | INFO | train_inner | epoch 002:  10787 / 60421 loss=1.088, ppl=2.13, wps=16515, ups=4.44, wpb=3718.4, bsz=136.9, num_updates=71200, lr=0.000118511, gnorm=1.284, loss_scale=4, train_wall=22, gb_free=10.9, wall=21796
2021-05-01 18:18:58 | INFO | train_inner | epoch 002:  10887 / 60421 loss=1.204, ppl=2.3, wps=16443.1, ups=4.42, wpb=3716.1, bsz=138.2, num_updates=71300, lr=0.000118428, gnorm=1.591, loss_scale=4, train_wall=22, gb_free=11.2, wall=21819
2021-05-01 18:19:21 | INFO | train_inner | epoch 002:  10987 / 60421 loss=1.096, ppl=2.14, wps=16489.4, ups=4.37, wpb=3770.6, bsz=144.3, num_updates=71400, lr=0.000118345, gnorm=1.42, loss_scale=4, train_wall=23, gb_free=10.8, wall=21842
2021-05-01 18:19:43 | INFO | train_inner | epoch 002:  11087 / 60421 loss=1.17, ppl=2.25, wps=16528, ups=4.4, wpb=3757.8, bsz=142.6, num_updates=71500, lr=0.000118262, gnorm=1.688, loss_scale=4, train_wall=23, gb_free=10.9, wall=21864
2021-05-01 18:20:06 | INFO | train_inner | epoch 002:  11187 / 60421 loss=1.169, ppl=2.25, wps=16286.3, ups=4.37, wpb=3730.8, bsz=126.1, num_updates=71600, lr=0.00011818, gnorm=1.53, loss_scale=4, train_wall=23, gb_free=10.7, wall=21887
2021-05-01 18:20:29 | INFO | train_inner | epoch 002:  11287 / 60421 loss=1.105, ppl=2.15, wps=16319.3, ups=4.38, wpb=3724.7, bsz=129.8, num_updates=71700, lr=0.000118097, gnorm=1.485, loss_scale=4, train_wall=23, gb_free=11.1, wall=21910
2021-05-01 18:20:52 | INFO | train_inner | epoch 002:  11387 / 60421 loss=1.233, ppl=2.35, wps=16395, ups=4.35, wpb=3770.8, bsz=114.8, num_updates=71800, lr=0.000118015, gnorm=1.522, loss_scale=4, train_wall=23, gb_free=10.9, wall=21933
2021-05-01 18:21:15 | INFO | train_inner | epoch 002:  11487 / 60421 loss=1.272, ppl=2.41, wps=16091, ups=4.3, wpb=3740, bsz=141, num_updates=71900, lr=0.000117933, gnorm=1.832, loss_scale=4, train_wall=23, gb_free=11, wall=21956
2021-05-01 18:21:39 | INFO | train_inner | epoch 002:  11587 / 60421 loss=1.11, ppl=2.16, wps=15792.4, ups=4.21, wpb=3754.7, bsz=127.3, num_updates=72000, lr=0.000117851, gnorm=1.339, loss_scale=4, train_wall=24, gb_free=10.6, wall=21980
2021-05-01 18:21:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:21:39 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:21:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:21:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:21:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:21:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:21:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:21:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:21:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:21:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:21:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:21:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:21:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:21:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:22:45 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.43 | ppl 21.55 | bleu 28.42 | wps 2407.3 | wpb 2024.1 | bsz 97.5 | num_updates 72000 | best_bleu 28.5
2021-05-01 18:22:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 72000 updates
2021-05-01 18:22:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_72000.pt
2021-05-01 18:22:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_72000.pt
2021-05-01 18:22:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_72000.pt (epoch 2 @ 72000 updates, score 28.42) (writing took 7.672804130997974 seconds)
2021-05-01 18:23:16 | INFO | train_inner | epoch 002:  11687 / 60421 loss=1.122, ppl=2.18, wps=3873.7, ups=1.04, wpb=3732.6, bsz=148.2, num_updates=72100, lr=0.000117769, gnorm=1.581, loss_scale=4, train_wall=23, gb_free=10.9, wall=22076
2021-05-01 18:23:38 | INFO | train_inner | epoch 002:  11787 / 60421 loss=1.112, ppl=2.16, wps=16706.9, ups=4.39, wpb=3803.5, bsz=127.6, num_updates=72200, lr=0.000117688, gnorm=1.27, loss_scale=4, train_wall=23, gb_free=10.9, wall=22099
2021-05-01 18:24:01 | INFO | train_inner | epoch 002:  11887 / 60421 loss=1.19, ppl=2.28, wps=16551.8, ups=4.42, wpb=3745.1, bsz=129.3, num_updates=72300, lr=0.000117606, gnorm=1.602, loss_scale=4, train_wall=22, gb_free=10.9, wall=22122
2021-05-01 18:24:24 | INFO | train_inner | epoch 002:  11987 / 60421 loss=1.143, ppl=2.21, wps=16524.2, ups=4.39, wpb=3763.4, bsz=123.1, num_updates=72400, lr=0.000117525, gnorm=1.53, loss_scale=4, train_wall=23, gb_free=10.7, wall=22145
2021-05-01 18:24:46 | INFO | train_inner | epoch 002:  12087 / 60421 loss=1.146, ppl=2.21, wps=16437.2, ups=4.41, wpb=3729.4, bsz=141.4, num_updates=72500, lr=0.000117444, gnorm=1.651, loss_scale=4, train_wall=23, gb_free=10.9, wall=22167
2021-05-01 18:25:09 | INFO | train_inner | epoch 002:  12187 / 60421 loss=1.133, ppl=2.19, wps=16478.9, ups=4.34, wpb=3799, bsz=137.9, num_updates=72600, lr=0.000117363, gnorm=1.308, loss_scale=4, train_wall=23, gb_free=10.9, wall=22190
2021-05-01 18:25:32 | INFO | train_inner | epoch 002:  12287 / 60421 loss=1.147, ppl=2.21, wps=16273.1, ups=4.38, wpb=3715.1, bsz=122.2, num_updates=72700, lr=0.000117282, gnorm=1.484, loss_scale=4, train_wall=23, gb_free=10.9, wall=22213
2021-05-01 18:25:56 | INFO | train_inner | epoch 002:  12387 / 60421 loss=1.144, ppl=2.21, wps=16213.6, ups=4.3, wpb=3772.9, bsz=123.8, num_updates=72800, lr=0.000117202, gnorm=1.391, loss_scale=4, train_wall=23, gb_free=10.7, wall=22237
2021-05-01 18:26:19 | INFO | train_inner | epoch 002:  12487 / 60421 loss=1.14, ppl=2.2, wps=15998.2, ups=4.31, wpb=3710.8, bsz=120.4, num_updates=72900, lr=0.000117121, gnorm=1.578, loss_scale=4, train_wall=23, gb_free=11, wall=22260
2021-05-01 18:26:42 | INFO | train_inner | epoch 002:  12587 / 60421 loss=1.183, ppl=2.27, wps=15686.9, ups=4.23, wpb=3708.4, bsz=132.7, num_updates=73000, lr=0.000117041, gnorm=1.779, loss_scale=4, train_wall=23, gb_free=10.8, wall=22283
2021-05-01 18:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:26:42 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:26:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:26:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:26:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:26:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:26:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:26:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:26:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:26:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:26:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:27:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:27:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:27:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:27:49 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.431 | ppl 21.57 | bleu 28.06 | wps 2385.3 | wpb 2024.1 | bsz 97.5 | num_updates 73000 | best_bleu 28.5
2021-05-01 18:27:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 73000 updates
2021-05-01 18:27:49 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_73000.pt
2021-05-01 18:27:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_73000.pt
2021-05-01 18:27:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_73000.pt (epoch 2 @ 73000 updates, score 28.06) (writing took 7.716998020005121 seconds)
2021-05-01 18:28:19 | INFO | train_inner | epoch 002:  12687 / 60421 loss=1.132, ppl=2.19, wps=3835.5, ups=1.03, wpb=3714.9, bsz=131.4, num_updates=73100, lr=0.000116961, gnorm=1.598, loss_scale=4, train_wall=22, gb_free=10.8, wall=22380
2021-05-01 18:28:42 | INFO | train_inner | epoch 002:  12787 / 60421 loss=1.175, ppl=2.26, wps=16664.8, ups=4.41, wpb=3781.2, bsz=134.1, num_updates=73200, lr=0.000116881, gnorm=1.492, loss_scale=4, train_wall=23, gb_free=10.8, wall=22403
2021-05-01 18:29:05 | INFO | train_inner | epoch 002:  12887 / 60421 loss=1.169, ppl=2.25, wps=16611.1, ups=4.39, wpb=3785.1, bsz=129.3, num_updates=73300, lr=0.000116801, gnorm=1.394, loss_scale=4, train_wall=23, gb_free=10.9, wall=22426
2021-05-01 18:29:28 | INFO | train_inner | epoch 002:  12987 / 60421 loss=1.152, ppl=2.22, wps=16662.1, ups=4.39, wpb=3799.4, bsz=141.3, num_updates=73400, lr=0.000116722, gnorm=1.364, loss_scale=4, train_wall=23, gb_free=10.8, wall=22448
2021-05-01 18:29:50 | INFO | train_inner | epoch 002:  13087 / 60421 loss=1.098, ppl=2.14, wps=16373.1, ups=4.43, wpb=3695.1, bsz=132.3, num_updates=73500, lr=0.000116642, gnorm=1.562, loss_scale=4, train_wall=22, gb_free=10.9, wall=22471
2021-05-01 18:30:13 | INFO | train_inner | epoch 002:  13187 / 60421 loss=1.188, ppl=2.28, wps=16357.7, ups=4.39, wpb=3723.1, bsz=113.7, num_updates=73600, lr=0.000116563, gnorm=1.597, loss_scale=4, train_wall=23, gb_free=10.8, wall=22494
2021-05-01 18:30:36 | INFO | train_inner | epoch 002:  13287 / 60421 loss=1.183, ppl=2.27, wps=16152.6, ups=4.31, wpb=3749, bsz=116.3, num_updates=73700, lr=0.000116484, gnorm=1.468, loss_scale=8, train_wall=23, gb_free=10.8, wall=22517
2021-05-01 18:30:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-01 18:30:59 | INFO | train_inner | epoch 002:  13388 / 60421 loss=1.165, ppl=2.24, wps=16029.9, ups=4.3, wpb=3728.1, bsz=132.4, num_updates=73800, lr=0.000116405, gnorm=1.519, loss_scale=4, train_wall=23, gb_free=10.7, wall=22540
2021-05-01 18:31:23 | INFO | train_inner | epoch 002:  13488 / 60421 loss=1.138, ppl=2.2, wps=16027.8, ups=4.21, wpb=3804.6, bsz=138.2, num_updates=73900, lr=0.000116326, gnorm=1.194, loss_scale=4, train_wall=24, gb_free=10.7, wall=22564
2021-05-01 18:31:47 | INFO | train_inner | epoch 002:  13588 / 60421 loss=1.124, ppl=2.18, wps=15772, ups=4.22, wpb=3734.5, bsz=152, num_updates=74000, lr=0.000116248, gnorm=1.387, loss_scale=4, train_wall=23, gb_free=10.8, wall=22588
2021-05-01 18:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:31:47 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:31:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:31:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:31:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:32:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:32:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:32:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:32:53 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.434 | ppl 21.62 | bleu 28.61 | wps 2408.5 | wpb 2024.1 | bsz 97.5 | num_updates 74000 | best_bleu 28.61
2021-05-01 18:32:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 74000 updates
2021-05-01 18:32:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_74000.pt
2021-05-01 18:32:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_74000.pt
2021-05-01 18:33:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_74000.pt (epoch 2 @ 74000 updates, score 28.61) (writing took 14.359161975000461 seconds)
2021-05-01 18:33:30 | INFO | train_inner | epoch 002:  13688 / 60421 loss=1.121, ppl=2.17, wps=3607.8, ups=0.97, wpb=3709.3, bsz=120.7, num_updates=74100, lr=0.000116169, gnorm=1.384, loss_scale=4, train_wall=22, gb_free=10.9, wall=22691
2021-05-01 18:33:52 | INFO | train_inner | epoch 002:  13788 / 60421 loss=1.154, ppl=2.23, wps=16735.6, ups=4.44, wpb=3767, bsz=115.2, num_updates=74200, lr=0.000116091, gnorm=1.228, loss_scale=4, train_wall=22, gb_free=10.8, wall=22713
2021-05-01 18:34:15 | INFO | train_inner | epoch 002:  13888 / 60421 loss=1.161, ppl=2.24, wps=16593.6, ups=4.43, wpb=3742.2, bsz=121.7, num_updates=74300, lr=0.000116013, gnorm=1.469, loss_scale=4, train_wall=22, gb_free=10.9, wall=22736
2021-05-01 18:34:37 | INFO | train_inner | epoch 002:  13988 / 60421 loss=1.166, ppl=2.24, wps=16422.6, ups=4.45, wpb=3687.7, bsz=122.2, num_updates=74400, lr=0.000115935, gnorm=1.718, loss_scale=4, train_wall=22, gb_free=10.9, wall=22758
2021-05-01 18:35:00 | INFO | train_inner | epoch 002:  14088 / 60421 loss=1.175, ppl=2.26, wps=16391.7, ups=4.38, wpb=3742.3, bsz=123.8, num_updates=74500, lr=0.000115857, gnorm=1.613, loss_scale=4, train_wall=23, gb_free=10.8, wall=22781
2021-05-01 18:35:23 | INFO | train_inner | epoch 002:  14188 / 60421 loss=1.171, ppl=2.25, wps=16301.3, ups=4.39, wpb=3716.1, bsz=147, num_updates=74600, lr=0.000115779, gnorm=1.708, loss_scale=4, train_wall=23, gb_free=10.8, wall=22804
2021-05-01 18:35:45 | INFO | train_inner | epoch 002:  14288 / 60421 loss=1.112, ppl=2.16, wps=16144.2, ups=4.4, wpb=3669.5, bsz=128.3, num_updates=74700, lr=0.000115702, gnorm=1.579, loss_scale=4, train_wall=23, gb_free=10.9, wall=22826
2021-05-01 18:36:08 | INFO | train_inner | epoch 002:  14388 / 60421 loss=1.126, ppl=2.18, wps=15999.5, ups=4.35, wpb=3675.9, bsz=145, num_updates=74800, lr=0.000115624, gnorm=1.66, loss_scale=4, train_wall=23, gb_free=10.8, wall=22849
2021-05-01 18:36:32 | INFO | train_inner | epoch 002:  14488 / 60421 loss=1.128, ppl=2.19, wps=16089.1, ups=4.25, wpb=3787.6, bsz=150.7, num_updates=74900, lr=0.000115547, gnorm=1.424, loss_scale=4, train_wall=23, gb_free=10.8, wall=22873
2021-05-01 18:36:56 | INFO | train_inner | epoch 002:  14588 / 60421 loss=1.143, ppl=2.21, wps=15993.8, ups=4.24, wpb=3772.6, bsz=129.7, num_updates=75000, lr=0.00011547, gnorm=1.473, loss_scale=4, train_wall=23, gb_free=10.7, wall=22896
2021-05-01 18:36:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:36:56 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:37:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:37:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:37:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:37:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:37:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:37:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:37:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:37:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:37:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:37:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:37:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:37:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:37:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:37:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:37:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:38:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.441 | ppl 21.71 | bleu 28.21 | wps 2418.7 | wpb 2024.1 | bsz 97.5 | num_updates 75000 | best_bleu 28.61
2021-05-01 18:38:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 75000 updates
2021-05-01 18:38:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_75000.pt
2021-05-01 18:38:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_75000.pt
2021-05-01 18:38:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_75000.pt (epoch 2 @ 75000 updates, score 28.21) (writing took 10.085022614999616 seconds)
2021-05-01 18:38:34 | INFO | train_inner | epoch 002:  14688 / 60421 loss=1.128, ppl=2.19, wps=3799.4, ups=1.02, wpb=3738.8, bsz=132.2, num_updates=75100, lr=0.000115393, gnorm=1.555, loss_scale=4, train_wall=22, gb_free=10.8, wall=22995
2021-05-01 18:38:56 | INFO | train_inner | epoch 002:  14788 / 60421 loss=1.14, ppl=2.2, wps=16635, ups=4.44, wpb=3750.3, bsz=130.7, num_updates=75200, lr=0.000115316, gnorm=1.391, loss_scale=4, train_wall=22, gb_free=10.8, wall=23017
2021-05-01 18:39:19 | INFO | train_inner | epoch 002:  14888 / 60421 loss=1.112, ppl=2.16, wps=16519.1, ups=4.47, wpb=3692.6, bsz=134.3, num_updates=75300, lr=0.00011524, gnorm=1.612, loss_scale=4, train_wall=22, gb_free=10.9, wall=23040
2021-05-01 18:39:42 | INFO | train_inner | epoch 002:  14988 / 60421 loss=1.119, ppl=2.17, wps=16660.2, ups=4.4, wpb=3785.5, bsz=144.9, num_updates=75400, lr=0.000115163, gnorm=1.315, loss_scale=4, train_wall=23, gb_free=11.1, wall=23063
2021-05-01 18:40:04 | INFO | train_inner | epoch 002:  15088 / 60421 loss=1.125, ppl=2.18, wps=16403.2, ups=4.4, wpb=3728.7, bsz=113.8, num_updates=75500, lr=0.000115087, gnorm=1.377, loss_scale=4, train_wall=23, gb_free=10.8, wall=23085
2021-05-01 18:40:27 | INFO | train_inner | epoch 002:  15188 / 60421 loss=1.084, ppl=2.12, wps=16365.5, ups=4.34, wpb=3768.2, bsz=153.2, num_updates=75600, lr=0.000115011, gnorm=1.262, loss_scale=4, train_wall=23, gb_free=10.9, wall=23108
2021-05-01 18:40:51 | INFO | train_inner | epoch 002:  15288 / 60421 loss=1.065, ppl=2.09, wps=16351.8, ups=4.3, wpb=3800.1, bsz=137.7, num_updates=75700, lr=0.000114935, gnorm=1.284, loss_scale=4, train_wall=23, gb_free=10.7, wall=23132
2021-05-01 18:41:14 | INFO | train_inner | epoch 002:  15388 / 60421 loss=1.104, ppl=2.15, wps=15912.1, ups=4.28, wpb=3721.6, bsz=147.1, num_updates=75800, lr=0.000114859, gnorm=1.542, loss_scale=4, train_wall=23, gb_free=10.8, wall=23155
2021-05-01 18:41:38 | INFO | train_inner | epoch 002:  15488 / 60421 loss=1.06, ppl=2.09, wps=15992.4, ups=4.22, wpb=3787.2, bsz=132.2, num_updates=75900, lr=0.000114783, gnorm=1.231, loss_scale=4, train_wall=24, gb_free=10.7, wall=23179
2021-05-01 18:42:01 | INFO | train_inner | epoch 002:  15588 / 60421 loss=1.055, ppl=2.08, wps=16013.1, ups=4.27, wpb=3747.3, bsz=151.6, num_updates=76000, lr=0.000114708, gnorm=1.544, loss_scale=4, train_wall=23, gb_free=10.8, wall=23202
2021-05-01 18:42:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:42:01 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:42:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:42:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:42:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:42:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:42:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:42:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:42:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:42:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:42:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:42:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:42:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:42:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:43:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.487 | ppl 22.42 | bleu 28.53 | wps 2426.3 | wpb 2024.1 | bsz 97.5 | num_updates 76000 | best_bleu 28.61
2021-05-01 18:43:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 76000 updates
2021-05-01 18:43:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_76000.pt
2021-05-01 18:43:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_76000.pt
2021-05-01 18:43:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_76000.pt (epoch 2 @ 76000 updates, score 28.53) (writing took 10.085268212998926 seconds)
2021-05-01 18:43:39 | INFO | train_inner | epoch 002:  15688 / 60421 loss=1.121, ppl=2.18, wps=3796.3, ups=1.02, wpb=3722.5, bsz=126.9, num_updates=76100, lr=0.000114632, gnorm=1.421, loss_scale=4, train_wall=22, gb_free=11.2, wall=23300
2021-05-01 18:44:02 | INFO | train_inner | epoch 002:  15788 / 60421 loss=1.137, ppl=2.2, wps=16785.8, ups=4.45, wpb=3771, bsz=124, num_updates=76200, lr=0.000114557, gnorm=1.382, loss_scale=4, train_wall=22, gb_free=10.7, wall=23322
2021-05-01 18:44:24 | INFO | train_inner | epoch 002:  15888 / 60421 loss=1.083, ppl=2.12, wps=16662.9, ups=4.38, wpb=3803.1, bsz=128.6, num_updates=76300, lr=0.000114482, gnorm=1.197, loss_scale=4, train_wall=23, gb_free=10.8, wall=23345
2021-05-01 18:44:47 | INFO | train_inner | epoch 002:  15988 / 60421 loss=1.195, ppl=2.29, wps=16555.8, ups=4.45, wpb=3716.6, bsz=140.2, num_updates=76400, lr=0.000114407, gnorm=1.821, loss_scale=4, train_wall=22, gb_free=11.3, wall=23368
2021-05-01 18:45:10 | INFO | train_inner | epoch 002:  16088 / 60421 loss=1.225, ppl=2.34, wps=16644.2, ups=4.32, wpb=3856.9, bsz=138.2, num_updates=76500, lr=0.000114332, gnorm=1.497, loss_scale=4, train_wall=23, gb_free=10.7, wall=23391
2021-05-01 18:45:33 | INFO | train_inner | epoch 002:  16188 / 60421 loss=1.143, ppl=2.21, wps=16233, ups=4.4, wpb=3691.8, bsz=118.6, num_updates=76600, lr=0.000114258, gnorm=1.54, loss_scale=4, train_wall=23, gb_free=10.8, wall=23414
2021-05-01 18:45:56 | INFO | train_inner | epoch 002:  16288 / 60421 loss=1.14, ppl=2.2, wps=16030.2, ups=4.37, wpb=3667.1, bsz=133.8, num_updates=76700, lr=0.000114183, gnorm=1.644, loss_scale=4, train_wall=23, gb_free=11, wall=23437
2021-05-01 18:46:19 | INFO | train_inner | epoch 002:  16388 / 60421 loss=1.009, ppl=2.01, wps=15919.6, ups=4.3, wpb=3698.1, bsz=132.6, num_updates=76800, lr=0.000114109, gnorm=1.29, loss_scale=4, train_wall=23, gb_free=10.9, wall=23460
2021-05-01 18:46:43 | INFO | train_inner | epoch 002:  16488 / 60421 loss=1.103, ppl=2.15, wps=15957.9, ups=4.22, wpb=3782.7, bsz=121.6, num_updates=76900, lr=0.000114035, gnorm=1.346, loss_scale=4, train_wall=24, gb_free=10.7, wall=23483
2021-05-01 18:47:06 | INFO | train_inner | epoch 002:  16588 / 60421 loss=1.069, ppl=2.1, wps=16128.2, ups=4.28, wpb=3765.5, bsz=138, num_updates=77000, lr=0.000113961, gnorm=1.293, loss_scale=4, train_wall=23, gb_free=10.7, wall=23507
2021-05-01 18:47:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:47:06 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:47:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:47:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:47:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:47:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:47:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:47:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:47:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:47:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:47:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:47:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:47:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:47:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:47:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:47:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:47:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:48:11 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.49 | ppl 22.47 | bleu 28.24 | wps 2450 | wpb 2024.1 | bsz 97.5 | num_updates 77000 | best_bleu 28.61
2021-05-01 18:48:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 77000 updates
2021-05-01 18:48:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_77000.pt
2021-05-01 18:48:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_77000.pt
2021-05-01 18:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_77000.pt (epoch 2 @ 77000 updates, score 28.24) (writing took 8.228822194003442 seconds)
2021-05-01 18:48:42 | INFO | train_inner | epoch 002:  16688 / 60421 loss=1.187, ppl=2.28, wps=3959.1, ups=1.05, wpb=3786, bsz=138.6, num_updates=77100, lr=0.000113887, gnorm=1.58, loss_scale=4, train_wall=22, gb_free=10.9, wall=23602
2021-05-01 18:49:04 | INFO | train_inner | epoch 002:  16788 / 60421 loss=1.034, ppl=2.05, wps=16459, ups=4.44, wpb=3703, bsz=135.9, num_updates=77200, lr=0.000113813, gnorm=1.461, loss_scale=4, train_wall=22, gb_free=11.4, wall=23625
2021-05-01 18:49:26 | INFO | train_inner | epoch 002:  16888 / 60421 loss=1.175, ppl=2.26, wps=16547, ups=4.48, wpb=3691.2, bsz=115.7, num_updates=77300, lr=0.000113739, gnorm=1.737, loss_scale=4, train_wall=22, gb_free=10.8, wall=23647
2021-05-01 18:49:49 | INFO | train_inner | epoch 002:  16988 / 60421 loss=1.113, ppl=2.16, wps=16591.2, ups=4.38, wpb=3784.3, bsz=140.5, num_updates=77400, lr=0.000113666, gnorm=1.42, loss_scale=4, train_wall=23, gb_free=10.8, wall=23670
2021-05-01 18:50:12 | INFO | train_inner | epoch 002:  17088 / 60421 loss=1.139, ppl=2.2, wps=16300.4, ups=4.37, wpb=3729.5, bsz=125.1, num_updates=77500, lr=0.000113592, gnorm=1.679, loss_scale=4, train_wall=23, gb_free=11.3, wall=23693
2021-05-01 18:50:35 | INFO | train_inner | epoch 002:  17188 / 60421 loss=1.113, ppl=2.16, wps=16405.9, ups=4.37, wpb=3755.4, bsz=129.1, num_updates=77600, lr=0.000113519, gnorm=1.26, loss_scale=4, train_wall=23, gb_free=10.7, wall=23716
2021-05-01 18:50:58 | INFO | train_inner | epoch 002:  17288 / 60421 loss=1.131, ppl=2.19, wps=16312, ups=4.37, wpb=3733.8, bsz=136.3, num_updates=77700, lr=0.000113446, gnorm=1.45, loss_scale=4, train_wall=23, gb_free=10.9, wall=23739
2021-05-01 18:51:21 | INFO | train_inner | epoch 002:  17388 / 60421 loss=1.107, ppl=2.15, wps=16033.9, ups=4.28, wpb=3748, bsz=127.7, num_updates=77800, lr=0.000113373, gnorm=1.315, loss_scale=4, train_wall=23, gb_free=11.1, wall=23762
2021-05-01 18:51:45 | INFO | train_inner | epoch 002:  17488 / 60421 loss=1.101, ppl=2.14, wps=16047.1, ups=4.25, wpb=3771.4, bsz=126.6, num_updates=77900, lr=0.0001133, gnorm=1.266, loss_scale=4, train_wall=23, gb_free=10.9, wall=23786
2021-05-01 18:52:08 | INFO | train_inner | epoch 002:  17588 / 60421 loss=1.08, ppl=2.11, wps=16215.4, ups=4.32, wpb=3757.2, bsz=133.4, num_updates=78000, lr=0.000113228, gnorm=1.287, loss_scale=4, train_wall=23, gb_free=11, wall=23809
2021-05-01 18:52:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:52:08 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:52:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:52:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:52:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:52:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:52:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:52:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:52:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:52:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:52:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:52:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:52:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:52:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:52:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:52:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:52:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:53:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.524 | ppl 23.01 | bleu 28.32 | wps 2426.8 | wpb 2024.1 | bsz 97.5 | num_updates 78000 | best_bleu 28.61
2021-05-01 18:53:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 78000 updates
2021-05-01 18:53:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_78000.pt
2021-05-01 18:53:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_78000.pt
2021-05-01 18:53:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_78000.pt (epoch 2 @ 78000 updates, score 28.32) (writing took 8.230850111998734 seconds)
2021-05-01 18:53:44 | INFO | train_inner | epoch 002:  17688 / 60421 loss=1.204, ppl=2.3, wps=3902, ups=1.04, wpb=3756.9, bsz=119.5, num_updates=78100, lr=0.000113155, gnorm=1.541, loss_scale=4, train_wall=22, gb_free=10.9, wall=23905
2021-05-01 18:54:07 | INFO | train_inner | epoch 002:  17788 / 60421 loss=1.097, ppl=2.14, wps=16481.7, ups=4.45, wpb=3700.5, bsz=143.5, num_updates=78200, lr=0.000113083, gnorm=1.306, loss_scale=4, train_wall=22, gb_free=10.7, wall=23928
2021-05-01 18:54:29 | INFO | train_inner | epoch 002:  17888 / 60421 loss=1.134, ppl=2.19, wps=16535.2, ups=4.42, wpb=3737.6, bsz=135.9, num_updates=78300, lr=0.000113011, gnorm=1.532, loss_scale=4, train_wall=22, gb_free=11.1, wall=23950
2021-05-01 18:54:52 | INFO | train_inner | epoch 002:  17988 / 60421 loss=1.16, ppl=2.24, wps=16381.9, ups=4.46, wpb=3676.8, bsz=128.1, num_updates=78400, lr=0.000112938, gnorm=1.764, loss_scale=4, train_wall=22, gb_free=10.8, wall=23973
2021-05-01 18:55:15 | INFO | train_inner | epoch 002:  18088 / 60421 loss=1.105, ppl=2.15, wps=16400.9, ups=4.36, wpb=3762.7, bsz=131.2, num_updates=78500, lr=0.000112867, gnorm=1.491, loss_scale=4, train_wall=23, gb_free=10.8, wall=23996
2021-05-01 18:55:37 | INFO | train_inner | epoch 002:  18188 / 60421 loss=1.209, ppl=2.31, wps=16208.2, ups=4.41, wpb=3677.3, bsz=127.8, num_updates=78600, lr=0.000112795, gnorm=1.801, loss_scale=4, train_wall=23, gb_free=10.9, wall=24018
2021-05-01 18:56:00 | INFO | train_inner | epoch 002:  18288 / 60421 loss=1.087, ppl=2.12, wps=16026.1, ups=4.32, wpb=3708.9, bsz=133.3, num_updates=78700, lr=0.000112723, gnorm=1.308, loss_scale=4, train_wall=23, gb_free=10.9, wall=24041
2021-05-01 18:56:24 | INFO | train_inner | epoch 002:  18388 / 60421 loss=1.076, ppl=2.11, wps=16194.8, ups=4.28, wpb=3784.2, bsz=135.7, num_updates=78800, lr=0.000112651, gnorm=1.282, loss_scale=4, train_wall=23, gb_free=10.7, wall=24065
2021-05-01 18:56:48 | INFO | train_inner | epoch 002:  18488 / 60421 loss=1.123, ppl=2.18, wps=15875.5, ups=4.19, wpb=3790.6, bsz=152.4, num_updates=78900, lr=0.00011258, gnorm=1.447, loss_scale=4, train_wall=24, gb_free=10.7, wall=24089
2021-05-01 18:57:11 | INFO | train_inner | epoch 002:  18588 / 60421 loss=1.199, ppl=2.3, wps=16347.8, ups=4.33, wpb=3777.4, bsz=110, num_updates=79000, lr=0.000112509, gnorm=1.401, loss_scale=4, train_wall=23, gb_free=10.9, wall=24112
2021-05-01 18:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 18:57:11 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 18:57:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:57:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:57:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:57:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:57:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:57:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:57:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:57:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:57:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:57:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:57:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:57:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:57:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:57:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:57:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:57:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 18:57:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 18:57:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 18:58:17 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.468 | ppl 22.13 | bleu 28.22 | wps 2408.2 | wpb 2024.1 | bsz 97.5 | num_updates 79000 | best_bleu 28.61
2021-05-01 18:58:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 79000 updates
2021-05-01 18:58:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_79000.pt
2021-05-01 18:58:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_79000.pt
2021-05-01 18:58:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_79000.pt (epoch 2 @ 79000 updates, score 28.22) (writing took 10.116183227000874 seconds)
2021-05-01 18:58:50 | INFO | train_inner | epoch 002:  18688 / 60421 loss=1.113, ppl=2.16, wps=3809.7, ups=1.01, wpb=3769.5, bsz=123.5, num_updates=79100, lr=0.000112438, gnorm=1.499, loss_scale=4, train_wall=23, gb_free=10.8, wall=24211
2021-05-01 18:59:12 | INFO | train_inner | epoch 002:  18788 / 60421 loss=1.224, ppl=2.34, wps=16635.1, ups=4.41, wpb=3776.3, bsz=137.1, num_updates=79200, lr=0.000112367, gnorm=1.555, loss_scale=4, train_wall=23, gb_free=10.7, wall=24233
2021-05-01 18:59:35 | INFO | train_inner | epoch 002:  18888 / 60421 loss=1.097, ppl=2.14, wps=16630.7, ups=4.38, wpb=3797.2, bsz=123.5, num_updates=79300, lr=0.000112296, gnorm=1.131, loss_scale=4, train_wall=23, gb_free=10.9, wall=24256
2021-05-01 18:59:58 | INFO | train_inner | epoch 002:  18988 / 60421 loss=1.15, ppl=2.22, wps=16385.1, ups=4.42, wpb=3707.7, bsz=118.4, num_updates=79400, lr=0.000112225, gnorm=1.528, loss_scale=4, train_wall=22, gb_free=10.9, wall=24279
2021-05-01 19:00:21 | INFO | train_inner | epoch 002:  19088 / 60421 loss=1.023, ppl=2.03, wps=16371.5, ups=4.35, wpb=3765.8, bsz=152.7, num_updates=79500, lr=0.000112154, gnorm=1.321, loss_scale=4, train_wall=23, gb_free=10.8, wall=24302
2021-05-01 19:00:44 | INFO | train_inner | epoch 002:  19188 / 60421 loss=1.071, ppl=2.1, wps=16256.7, ups=4.32, wpb=3761.1, bsz=135.5, num_updates=79600, lr=0.000112084, gnorm=1.256, loss_scale=4, train_wall=23, gb_free=10.8, wall=24325
2021-05-01 19:01:07 | INFO | train_inner | epoch 002:  19288 / 60421 loss=1.191, ppl=2.28, wps=16253.6, ups=4.33, wpb=3752.2, bsz=112.8, num_updates=79700, lr=0.000112014, gnorm=1.492, loss_scale=4, train_wall=23, gb_free=10.7, wall=24348
2021-05-01 19:01:30 | INFO | train_inner | epoch 002:  19388 / 60421 loss=1.063, ppl=2.09, wps=15856.1, ups=4.28, wpb=3702.5, bsz=137, num_updates=79800, lr=0.000111943, gnorm=1.316, loss_scale=4, train_wall=23, gb_free=10.9, wall=24371
2021-05-01 19:01:54 | INFO | train_inner | epoch 002:  19488 / 60421 loss=1.065, ppl=2.09, wps=15709.3, ups=4.28, wpb=3674.2, bsz=130.6, num_updates=79900, lr=0.000111873, gnorm=1.681, loss_scale=4, train_wall=23, gb_free=10.8, wall=24395
2021-05-01 19:02:17 | INFO | train_inner | epoch 002:  19588 / 60421 loss=1.113, ppl=2.16, wps=16134, ups=4.35, wpb=3711.3, bsz=135.1, num_updates=80000, lr=0.000111803, gnorm=1.474, loss_scale=4, train_wall=23, gb_free=10.7, wall=24418
2021-05-01 19:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:02:17 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:02:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:02:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:02:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:02:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:02:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:02:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:02:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:02:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:02:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:02:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:02:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:02:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:02:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:02:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:02:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:03:23 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.493 | ppl 22.52 | bleu 28.19 | wps 2396.1 | wpb 2024.1 | bsz 97.5 | num_updates 80000 | best_bleu 28.61
2021-05-01 19:03:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 80000 updates
2021-05-01 19:03:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_80000.pt
2021-05-01 19:03:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_80000.pt
2021-05-01 19:03:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_80000.pt (epoch 2 @ 80000 updates, score 28.19) (writing took 7.707590783000342 seconds)
2021-05-01 19:03:53 | INFO | train_inner | epoch 002:  19688 / 60421 loss=1.149, ppl=2.22, wps=3807.5, ups=1.04, wpb=3666.4, bsz=130, num_updates=80100, lr=0.000111734, gnorm=1.655, loss_scale=4, train_wall=22, gb_free=10.8, wall=24514
2021-05-01 19:04:16 | INFO | train_inner | epoch 002:  19788 / 60421 loss=1.144, ppl=2.21, wps=16624.5, ups=4.39, wpb=3784.4, bsz=143, num_updates=80200, lr=0.000111664, gnorm=1.403, loss_scale=4, train_wall=23, gb_free=10.9, wall=24537
2021-05-01 19:04:38 | INFO | train_inner | epoch 002:  19888 / 60421 loss=1.073, ppl=2.1, wps=16351.5, ups=4.44, wpb=3685.1, bsz=145.5, num_updates=80300, lr=0.000111594, gnorm=1.742, loss_scale=4, train_wall=22, gb_free=10.7, wall=24559
2021-05-01 19:05:01 | INFO | train_inner | epoch 002:  19988 / 60421 loss=1.095, ppl=2.14, wps=16566.9, ups=4.36, wpb=3803.1, bsz=139.9, num_updates=80400, lr=0.000111525, gnorm=1.256, loss_scale=4, train_wall=23, gb_free=10.8, wall=24582
2021-05-01 19:05:24 | INFO | train_inner | epoch 002:  20088 / 60421 loss=1.181, ppl=2.27, wps=16411.7, ups=4.38, wpb=3749.5, bsz=135.5, num_updates=80500, lr=0.000111456, gnorm=1.667, loss_scale=4, train_wall=23, gb_free=10.9, wall=24605
2021-05-01 19:05:47 | INFO | train_inner | epoch 002:  20188 / 60421 loss=1.129, ppl=2.19, wps=16390, ups=4.32, wpb=3790, bsz=122.7, num_updates=80600, lr=0.000111386, gnorm=1.265, loss_scale=4, train_wall=23, gb_free=10.9, wall=24628
2021-05-01 19:06:10 | INFO | train_inner | epoch 002:  20288 / 60421 loss=1.114, ppl=2.16, wps=15868.1, ups=4.4, wpb=3610.1, bsz=119.2, num_updates=80700, lr=0.000111317, gnorm=1.735, loss_scale=4, train_wall=23, gb_free=10.9, wall=24651
2021-05-01 19:06:33 | INFO | train_inner | epoch 002:  20388 / 60421 loss=1.087, ppl=2.12, wps=15758.1, ups=4.3, wpb=3666.5, bsz=134.6, num_updates=80800, lr=0.000111249, gnorm=1.672, loss_scale=4, train_wall=23, gb_free=10.6, wall=24674
2021-05-01 19:06:57 | INFO | train_inner | epoch 002:  20488 / 60421 loss=1.092, ppl=2.13, wps=15804.9, ups=4.26, wpb=3706.5, bsz=140.9, num_updates=80900, lr=0.00011118, gnorm=1.372, loss_scale=4, train_wall=23, gb_free=10.9, wall=24698
2021-05-01 19:07:19 | INFO | train_inner | epoch 002:  20588 / 60421 loss=1.132, ppl=2.19, wps=16302.2, ups=4.41, wpb=3697.7, bsz=109.5, num_updates=81000, lr=0.000111111, gnorm=1.393, loss_scale=4, train_wall=23, gb_free=10.8, wall=24720
2021-05-01 19:07:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:07:20 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:07:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:07:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:07:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:07:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:07:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:07:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:08:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.527 | ppl 23.06 | bleu 28.64 | wps 2405 | wpb 2024.1 | bsz 97.5 | num_updates 81000 | best_bleu 28.64
2021-05-01 19:08:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 81000 updates
2021-05-01 19:08:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_81000.pt
2021-05-01 19:08:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_81000.pt
2021-05-01 19:08:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_81000.pt (epoch 2 @ 81000 updates, score 28.64) (writing took 14.356120003998512 seconds)
2021-05-01 19:09:03 | INFO | train_inner | epoch 002:  20688 / 60421 loss=1.091, ppl=2.13, wps=3632.4, ups=0.97, wpb=3742.2, bsz=129.6, num_updates=81100, lr=0.000111043, gnorm=1.549, loss_scale=4, train_wall=22, gb_free=10.8, wall=24823
2021-05-01 19:09:25 | INFO | train_inner | epoch 002:  20788 / 60421 loss=1.158, ppl=2.23, wps=16537.2, ups=4.39, wpb=3766, bsz=102.5, num_updates=81200, lr=0.000110974, gnorm=1.34, loss_scale=4, train_wall=23, gb_free=10.7, wall=24846
2021-05-01 19:09:48 | INFO | train_inner | epoch 002:  20888 / 60421 loss=1.149, ppl=2.22, wps=16500.5, ups=4.4, wpb=3748.6, bsz=119.5, num_updates=81300, lr=0.000110906, gnorm=1.327, loss_scale=4, train_wall=23, gb_free=10.8, wall=24869
2021-05-01 19:10:11 | INFO | train_inner | epoch 002:  20988 / 60421 loss=1.096, ppl=2.14, wps=16590, ups=4.35, wpb=3814.9, bsz=125, num_updates=81400, lr=0.000110838, gnorm=1.148, loss_scale=4, train_wall=23, gb_free=11.1, wall=24892
2021-05-01 19:10:34 | INFO | train_inner | epoch 002:  21088 / 60421 loss=1.167, ppl=2.25, wps=16207.8, ups=4.4, wpb=3680.6, bsz=124.2, num_updates=81500, lr=0.00011077, gnorm=1.682, loss_scale=4, train_wall=23, gb_free=10.9, wall=24915
2021-05-01 19:10:57 | INFO | train_inner | epoch 002:  21188 / 60421 loss=1.098, ppl=2.14, wps=16284.6, ups=4.31, wpb=3774.3, bsz=128.7, num_updates=81600, lr=0.000110702, gnorm=1.232, loss_scale=4, train_wall=23, gb_free=10.8, wall=24938
2021-05-01 19:11:20 | INFO | train_inner | epoch 002:  21288 / 60421 loss=1.102, ppl=2.15, wps=16133.6, ups=4.24, wpb=3807.1, bsz=119.6, num_updates=81700, lr=0.000110634, gnorm=1.191, loss_scale=4, train_wall=23, gb_free=10.7, wall=24961
2021-05-01 19:11:44 | INFO | train_inner | epoch 002:  21388 / 60421 loss=1.127, ppl=2.18, wps=15844.3, ups=4.21, wpb=3765.1, bsz=127.9, num_updates=81800, lr=0.000110566, gnorm=1.299, loss_scale=4, train_wall=24, gb_free=10.7, wall=24985
2021-05-01 19:12:07 | INFO | train_inner | epoch 002:  21488 / 60421 loss=1.079, ppl=2.11, wps=16106.7, ups=4.32, wpb=3729, bsz=123.4, num_updates=81900, lr=0.000110499, gnorm=1.232, loss_scale=4, train_wall=23, gb_free=10.6, wall=25008
2021-05-01 19:12:30 | INFO | train_inner | epoch 002:  21588 / 60421 loss=1.074, ppl=2.1, wps=16271.6, ups=4.41, wpb=3690.6, bsz=126.2, num_updates=82000, lr=0.000110432, gnorm=1.461, loss_scale=4, train_wall=23, gb_free=10.9, wall=25031
2021-05-01 19:12:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:12:30 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:12:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:12:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:12:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:12:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:12:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:12:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:12:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:12:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:12:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:12:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:12:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:12:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:12:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:12:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:12:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:13:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.573 | ppl 23.79 | bleu 28.72 | wps 2440.2 | wpb 2024.1 | bsz 97.5 | num_updates 82000 | best_bleu 28.72
2021-05-01 19:13:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 82000 updates
2021-05-01 19:13:35 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_82000.pt
2021-05-01 19:13:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_82000.pt
2021-05-01 19:13:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_82000.pt (epoch 2 @ 82000 updates, score 28.72) (writing took 14.35924458399677 seconds)
2021-05-01 19:14:12 | INFO | train_inner | epoch 002:  21688 / 60421 loss=1.207, ppl=2.31, wps=3681.6, ups=0.98, wpb=3762.4, bsz=132.3, num_updates=82100, lr=0.000110364, gnorm=1.522, loss_scale=4, train_wall=23, gb_free=11, wall=25133
2021-05-01 19:14:35 | INFO | train_inner | epoch 002:  21788 / 60421 loss=1.071, ppl=2.1, wps=16594.5, ups=4.44, wpb=3734.1, bsz=139.3, num_updates=82200, lr=0.000110297, gnorm=1.519, loss_scale=4, train_wall=22, gb_free=10.9, wall=25156
2021-05-01 19:14:57 | INFO | train_inner | epoch 002:  21888 / 60421 loss=1.188, ppl=2.28, wps=16473.9, ups=4.43, wpb=3722.2, bsz=118.2, num_updates=82300, lr=0.00011023, gnorm=1.656, loss_scale=4, train_wall=22, gb_free=10.8, wall=25178
2021-05-01 19:15:20 | INFO | train_inner | epoch 002:  21988 / 60421 loss=1.167, ppl=2.24, wps=16482, ups=4.4, wpb=3747.8, bsz=120.4, num_updates=82400, lr=0.000110163, gnorm=1.468, loss_scale=4, train_wall=23, gb_free=11.1, wall=25201
2021-05-01 19:15:43 | INFO | train_inner | epoch 002:  22088 / 60421 loss=1.022, ppl=2.03, wps=16127.3, ups=4.33, wpb=3723.5, bsz=154.2, num_updates=82500, lr=0.000110096, gnorm=1.294, loss_scale=4, train_wall=23, gb_free=10.8, wall=25224
2021-05-01 19:16:06 | INFO | train_inner | epoch 002:  22188 / 60421 loss=1.111, ppl=2.16, wps=16016.1, ups=4.36, wpb=3669.5, bsz=146.5, num_updates=82600, lr=0.00011003, gnorm=1.906, loss_scale=4, train_wall=23, gb_free=11, wall=25247
2021-05-01 19:16:30 | INFO | train_inner | epoch 002:  22288 / 60421 loss=1.116, ppl=2.17, wps=16051.5, ups=4.28, wpb=3753.7, bsz=129.1, num_updates=82700, lr=0.000109963, gnorm=1.351, loss_scale=4, train_wall=23, gb_free=10.8, wall=25270
2021-05-01 19:16:53 | INFO | train_inner | epoch 002:  22388 / 60421 loss=1.061, ppl=2.09, wps=15649.4, ups=4.22, wpb=3707.3, bsz=130.6, num_updates=82800, lr=0.000109897, gnorm=1.323, loss_scale=4, train_wall=24, gb_free=10.8, wall=25294
2021-05-01 19:17:16 | INFO | train_inner | epoch 002:  22488 / 60421 loss=1.113, ppl=2.16, wps=16327.9, ups=4.32, wpb=3776.4, bsz=132.6, num_updates=82900, lr=0.00010983, gnorm=1.462, loss_scale=4, train_wall=23, gb_free=11, wall=25317
2021-05-01 19:17:39 | INFO | train_inner | epoch 002:  22588 / 60421 loss=1.143, ppl=2.21, wps=16584.4, ups=4.45, wpb=3725.6, bsz=118.6, num_updates=83000, lr=0.000109764, gnorm=1.566, loss_scale=4, train_wall=22, gb_free=10.7, wall=25340
2021-05-01 19:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:17:39 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:17:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:17:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:17:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:17:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:17:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:17:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:17:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:17:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:17:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:17:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:17:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:17:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:18:45 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.525 | ppl 23.02 | bleu 28.2 | wps 2401.2 | wpb 2024.1 | bsz 97.5 | num_updates 83000 | best_bleu 28.72
2021-05-01 19:18:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 83000 updates
2021-05-01 19:18:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_83000.pt
2021-05-01 19:18:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_83000.pt
2021-05-01 19:18:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_83000.pt (epoch 2 @ 83000 updates, score 28.2) (writing took 8.248185823002132 seconds)
2021-05-01 19:19:16 | INFO | train_inner | epoch 002:  22688 / 60421 loss=1.091, ppl=2.13, wps=3862.3, ups=1.03, wpb=3751.3, bsz=140.5, num_updates=83100, lr=0.000109698, gnorm=1.459, loss_scale=4, train_wall=23, gb_free=10.9, wall=25437
2021-05-01 19:19:38 | INFO | train_inner | epoch 002:  22788 / 60421 loss=1.087, ppl=2.12, wps=16365.1, ups=4.45, wpb=3680.7, bsz=142.7, num_updates=83200, lr=0.000109632, gnorm=1.531, loss_scale=4, train_wall=22, gb_free=10.7, wall=25459
2021-05-01 19:20:01 | INFO | train_inner | epoch 002:  22888 / 60421 loss=1.109, ppl=2.16, wps=16271.5, ups=4.46, wpb=3646.1, bsz=129.8, num_updates=83300, lr=0.000109566, gnorm=1.728, loss_scale=4, train_wall=22, gb_free=10.7, wall=25482
2021-05-01 19:20:24 | INFO | train_inner | epoch 002:  22988 / 60421 loss=1.101, ppl=2.14, wps=16371.6, ups=4.35, wpb=3762.6, bsz=143.2, num_updates=83400, lr=0.000109501, gnorm=1.43, loss_scale=4, train_wall=23, gb_free=10.8, wall=25505
2021-05-01 19:20:47 | INFO | train_inner | epoch 002:  23088 / 60421 loss=1.109, ppl=2.16, wps=16080.5, ups=4.35, wpb=3693, bsz=142.4, num_updates=83500, lr=0.000109435, gnorm=1.599, loss_scale=4, train_wall=23, gb_free=11.2, wall=25528
2021-05-01 19:21:10 | INFO | train_inner | epoch 002:  23188 / 60421 loss=1.099, ppl=2.14, wps=16242.5, ups=4.29, wpb=3789.3, bsz=138, num_updates=83600, lr=0.00010937, gnorm=1.3, loss_scale=4, train_wall=23, gb_free=10.8, wall=25551
2021-05-01 19:21:34 | INFO | train_inner | epoch 002:  23288 / 60421 loss=1.192, ppl=2.29, wps=16002.2, ups=4.2, wpb=3808.6, bsz=125.3, num_updates=83700, lr=0.000109304, gnorm=1.578, loss_scale=4, train_wall=24, gb_free=10.8, wall=25575
2021-05-01 19:21:58 | INFO | train_inner | epoch 002:  23388 / 60421 loss=1.187, ppl=2.28, wps=16116.5, ups=4.23, wpb=3812.8, bsz=126, num_updates=83800, lr=0.000109239, gnorm=1.444, loss_scale=4, train_wall=23, gb_free=10.7, wall=25599
2021-05-01 19:22:20 | INFO | train_inner | epoch 002:  23488 / 60421 loss=1.06, ppl=2.08, wps=16164.7, ups=4.38, wpb=3693.5, bsz=141.1, num_updates=83900, lr=0.000109174, gnorm=1.784, loss_scale=4, train_wall=23, gb_free=10.9, wall=25621
2021-05-01 19:22:43 | INFO | train_inner | epoch 002:  23588 / 60421 loss=1.066, ppl=2.09, wps=16528.6, ups=4.4, wpb=3756.1, bsz=119.8, num_updates=84000, lr=0.000109109, gnorm=1.188, loss_scale=4, train_wall=23, gb_free=10.9, wall=25644
2021-05-01 19:22:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:22:43 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:22:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:22:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:22:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:22:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:22:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:22:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:23:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:23:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:23:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:23:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:23:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:23:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:23:50 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.559 | ppl 23.57 | bleu 28.25 | wps 2385.6 | wpb 2024.1 | bsz 97.5 | num_updates 84000 | best_bleu 28.72
2021-05-01 19:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 84000 updates
2021-05-01 19:23:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_84000.pt
2021-05-01 19:23:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_84000.pt
2021-05-01 19:23:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_84000.pt (epoch 2 @ 84000 updates, score 28.25) (writing took 8.21519770300074 seconds)
2021-05-01 19:24:21 | INFO | train_inner | epoch 002:  23688 / 60421 loss=1.177, ppl=2.26, wps=3847.5, ups=1.03, wpb=3749.8, bsz=128.9, num_updates=84100, lr=0.000109044, gnorm=1.469, loss_scale=4, train_wall=22, gb_free=10.9, wall=25742
2021-05-01 19:24:43 | INFO | train_inner | epoch 002:  23788 / 60421 loss=1.105, ppl=2.15, wps=16518.4, ups=4.42, wpb=3737.9, bsz=131.4, num_updates=84200, lr=0.000108979, gnorm=1.522, loss_scale=4, train_wall=22, gb_free=11.3, wall=25764
2021-05-01 19:25:06 | INFO | train_inner | epoch 002:  23888 / 60421 loss=1.071, ppl=2.1, wps=16446.7, ups=4.37, wpb=3759.4, bsz=133.4, num_updates=84300, lr=0.000108915, gnorm=1.324, loss_scale=4, train_wall=23, gb_free=10.7, wall=25787
2021-05-01 19:25:29 | INFO | train_inner | epoch 002:  23988 / 60421 loss=1.058, ppl=2.08, wps=16336.3, ups=4.36, wpb=3746.3, bsz=133.4, num_updates=84400, lr=0.00010885, gnorm=1.289, loss_scale=4, train_wall=23, gb_free=11, wall=25810
2021-05-01 19:25:52 | INFO | train_inner | epoch 002:  24088 / 60421 loss=1.031, ppl=2.04, wps=16192.9, ups=4.3, wpb=3762.7, bsz=139.6, num_updates=84500, lr=0.000108786, gnorm=1.264, loss_scale=4, train_wall=23, gb_free=11.3, wall=25833
2021-05-01 19:26:16 | INFO | train_inner | epoch 002:  24188 / 60421 loss=1.048, ppl=2.07, wps=16126.1, ups=4.25, wpb=3797.6, bsz=165, num_updates=84600, lr=0.000108721, gnorm=1.438, loss_scale=4, train_wall=23, gb_free=10.8, wall=25857
2021-05-01 19:26:39 | INFO | train_inner | epoch 002:  24288 / 60421 loss=1.076, ppl=2.11, wps=15676.5, ups=4.23, wpb=3705.7, bsz=147.3, num_updates=84700, lr=0.000108657, gnorm=1.435, loss_scale=4, train_wall=23, gb_free=10.8, wall=25880
2021-05-01 19:27:03 | INFO | train_inner | epoch 002:  24388 / 60421 loss=1.083, ppl=2.12, wps=16011, ups=4.22, wpb=3796.2, bsz=127.4, num_updates=84800, lr=0.000108593, gnorm=1.288, loss_scale=4, train_wall=24, gb_free=10.8, wall=25904
2021-05-01 19:27:26 | INFO | train_inner | epoch 002:  24488 / 60421 loss=1.035, ppl=2.05, wps=16092.2, ups=4.36, wpb=3688, bsz=146.8, num_updates=84900, lr=0.000108529, gnorm=1.405, loss_scale=4, train_wall=23, gb_free=10.8, wall=25927
2021-05-01 19:27:49 | INFO | train_inner | epoch 002:  24588 / 60421 loss=1.088, ppl=2.13, wps=16395.7, ups=4.39, wpb=3735.9, bsz=123.2, num_updates=85000, lr=0.000108465, gnorm=1.629, loss_scale=4, train_wall=23, gb_free=10.7, wall=25950
2021-05-01 19:27:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:27:49 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:28:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:28:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:28:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:28:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:28:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:28:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:28:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:28:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:28:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:28:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:28:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:28:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:28:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.584 | ppl 23.99 | bleu 28.61 | wps 2434.4 | wpb 2024.1 | bsz 97.5 | num_updates 85000 | best_bleu 28.72
2021-05-01 19:28:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 85000 updates
2021-05-01 19:28:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_85000.pt
2021-05-01 19:28:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_85000.pt
2021-05-01 19:29:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_85000.pt (epoch 2 @ 85000 updates, score 28.61) (writing took 7.668445121002151 seconds)
2021-05-01 19:29:24 | INFO | train_inner | epoch 002:  24688 / 60421 loss=1.052, ppl=2.07, wps=3875, ups=1.05, wpb=3699.3, bsz=140.5, num_updates=85100, lr=0.000108401, gnorm=1.559, loss_scale=4, train_wall=22, gb_free=10.9, wall=26045
2021-05-01 19:29:47 | INFO | train_inner | epoch 002:  24788 / 60421 loss=1.088, ppl=2.13, wps=16365.1, ups=4.42, wpb=3706.7, bsz=132.7, num_updates=85200, lr=0.000108338, gnorm=1.552, loss_scale=4, train_wall=22, gb_free=11.2, wall=26068
2021-05-01 19:30:10 | INFO | train_inner | epoch 002:  24888 / 60421 loss=1.139, ppl=2.2, wps=16493.4, ups=4.4, wpb=3749.9, bsz=141, num_updates=85300, lr=0.000108274, gnorm=1.555, loss_scale=4, train_wall=23, gb_free=10.9, wall=26091
2021-05-01 19:30:33 | INFO | train_inner | epoch 002:  24988 / 60421 loss=1.089, ppl=2.13, wps=16482.2, ups=4.31, wpb=3826.5, bsz=130, num_updates=85400, lr=0.000108211, gnorm=1.265, loss_scale=4, train_wall=23, gb_free=10.9, wall=26114
2021-05-01 19:30:56 | INFO | train_inner | epoch 002:  25088 / 60421 loss=1.099, ppl=2.14, wps=16107.4, ups=4.36, wpb=3697.5, bsz=125, num_updates=85500, lr=0.000108148, gnorm=1.377, loss_scale=4, train_wall=23, gb_free=11.1, wall=26137
2021-05-01 19:31:19 | INFO | train_inner | epoch 002:  25188 / 60421 loss=1.159, ppl=2.23, wps=16057, ups=4.25, wpb=3774.1, bsz=124.2, num_updates=85600, lr=0.000108084, gnorm=1.464, loss_scale=4, train_wall=23, gb_free=10.8, wall=26160
2021-05-01 19:31:43 | INFO | train_inner | epoch 002:  25288 / 60421 loss=1.179, ppl=2.26, wps=15640.9, ups=4.26, wpb=3671.8, bsz=111, num_updates=85700, lr=0.000108021, gnorm=1.883, loss_scale=4, train_wall=23, gb_free=10.7, wall=26184
2021-05-01 19:32:06 | INFO | train_inner | epoch 002:  25388 / 60421 loss=1.123, ppl=2.18, wps=16027.3, ups=4.29, wpb=3733.3, bsz=106.9, num_updates=85800, lr=0.000107958, gnorm=1.463, loss_scale=4, train_wall=23, gb_free=10.7, wall=26207
2021-05-01 19:32:29 | INFO | train_inner | epoch 002:  25488 / 60421 loss=1.117, ppl=2.17, wps=16320.8, ups=4.4, wpb=3711.4, bsz=118.9, num_updates=85900, lr=0.000107896, gnorm=1.633, loss_scale=4, train_wall=23, gb_free=10.5, wall=26230
2021-05-01 19:32:52 | INFO | train_inner | epoch 002:  25588 / 60421 loss=1.058, ppl=2.08, wps=16425, ups=4.39, wpb=3738.5, bsz=151.8, num_updates=86000, lr=0.000107833, gnorm=1.495, loss_scale=4, train_wall=23, gb_free=10.8, wall=26253
2021-05-01 19:32:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:32:52 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:33:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:33:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:33:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:33:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:33:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:33:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:33:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:33:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:33:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:33:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:33:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:33:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:33:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:33:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:33:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:33:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.559 | ppl 23.58 | bleu 28.67 | wps 2391.2 | wpb 2024.1 | bsz 97.5 | num_updates 86000 | best_bleu 28.72
2021-05-01 19:33:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 86000 updates
2021-05-01 19:33:58 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_86000.pt
2021-05-01 19:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_86000.pt
2021-05-01 19:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_86000.pt (epoch 2 @ 86000 updates, score 28.67) (writing took 7.76203665399953 seconds)
2021-05-01 19:34:29 | INFO | train_inner | epoch 002:  25688 / 60421 loss=1.134, ppl=2.19, wps=3880.9, ups=1.03, wpb=3763.1, bsz=133.5, num_updates=86100, lr=0.00010777, gnorm=1.477, loss_scale=4, train_wall=23, gb_free=10.8, wall=26350
2021-05-01 19:34:51 | INFO | train_inner | epoch 002:  25788 / 60421 loss=1.104, ppl=2.15, wps=16547.3, ups=4.4, wpb=3762.9, bsz=127, num_updates=86200, lr=0.000107708, gnorm=1.323, loss_scale=4, train_wall=23, gb_free=10.9, wall=26372
2021-05-01 19:35:14 | INFO | train_inner | epoch 002:  25888 / 60421 loss=1.122, ppl=2.18, wps=16380.8, ups=4.41, wpb=3715.5, bsz=122.3, num_updates=86300, lr=0.000107645, gnorm=1.499, loss_scale=4, train_wall=23, gb_free=11.1, wall=26395
2021-05-01 19:35:37 | INFO | train_inner | epoch 002:  25988 / 60421 loss=1.071, ppl=2.1, wps=16291, ups=4.32, wpb=3774.2, bsz=133.8, num_updates=86400, lr=0.000107583, gnorm=1.272, loss_scale=4, train_wall=23, gb_free=10.9, wall=26418
2021-05-01 19:36:00 | INFO | train_inner | epoch 002:  26088 / 60421 loss=1.098, ppl=2.14, wps=16119.2, ups=4.33, wpb=3726.2, bsz=137.6, num_updates=86500, lr=0.000107521, gnorm=1.474, loss_scale=4, train_wall=23, gb_free=10.9, wall=26441
2021-05-01 19:36:24 | INFO | train_inner | epoch 002:  26188 / 60421 loss=1.092, ppl=2.13, wps=16107.7, ups=4.26, wpb=3778.1, bsz=129, num_updates=86600, lr=0.000107459, gnorm=1.328, loss_scale=4, train_wall=23, gb_free=10.7, wall=26465
2021-05-01 19:36:47 | INFO | train_inner | epoch 002:  26288 / 60421 loss=1.115, ppl=2.17, wps=15625.4, ups=4.25, wpb=3675.9, bsz=130.6, num_updates=86700, lr=0.000107397, gnorm=1.955, loss_scale=4, train_wall=23, gb_free=10.9, wall=26488
2021-05-01 19:37:10 | INFO | train_inner | epoch 002:  26388 / 60421 loss=1.102, ppl=2.15, wps=16083.3, ups=4.34, wpb=3704.9, bsz=137.9, num_updates=86800, lr=0.000107335, gnorm=1.492, loss_scale=4, train_wall=23, gb_free=10.7, wall=26511
2021-05-01 19:37:33 | INFO | train_inner | epoch 002:  26488 / 60421 loss=1.142, ppl=2.21, wps=16308.8, ups=4.39, wpb=3715.6, bsz=125.8, num_updates=86900, lr=0.000107273, gnorm=1.754, loss_scale=4, train_wall=23, gb_free=11, wall=26534
2021-05-01 19:37:56 | INFO | train_inner | epoch 002:  26588 / 60421 loss=1.074, ppl=2.1, wps=16518.5, ups=4.42, wpb=3740.8, bsz=137.3, num_updates=87000, lr=0.000107211, gnorm=1.371, loss_scale=4, train_wall=22, gb_free=10.9, wall=26557
2021-05-01 19:37:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:37:56 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:38:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:38:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:38:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:38:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:38:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:38:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:38:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:38:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:38:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:38:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:38:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:38:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:38:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:38:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:38:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:39:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.591 | ppl 24.11 | bleu 28.59 | wps 2423.4 | wpb 2024.1 | bsz 97.5 | num_updates 87000 | best_bleu 28.72
2021-05-01 19:39:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 87000 updates
2021-05-01 19:39:01 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_87000.pt
2021-05-01 19:39:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_87000.pt
2021-05-01 19:39:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_87000.pt (epoch 2 @ 87000 updates, score 28.59) (writing took 7.812292295995576 seconds)
2021-05-01 19:39:32 | INFO | train_inner | epoch 002:  26688 / 60421 loss=1.108, ppl=2.16, wps=3904.7, ups=1.04, wpb=3745.5, bsz=119.8, num_updates=87100, lr=0.00010715, gnorm=1.572, loss_scale=4, train_wall=22, gb_free=10.9, wall=26653
2021-05-01 19:39:54 | INFO | train_inner | epoch 002:  26788 / 60421 loss=1.108, ppl=2.16, wps=16426.4, ups=4.4, wpb=3732.6, bsz=137.6, num_updates=87200, lr=0.000107088, gnorm=1.553, loss_scale=4, train_wall=23, gb_free=10.7, wall=26675
2021-05-01 19:40:17 | INFO | train_inner | epoch 002:  26888 / 60421 loss=1.086, ppl=2.12, wps=16421.9, ups=4.34, wpb=3783.2, bsz=132.6, num_updates=87300, lr=0.000107027, gnorm=1.21, loss_scale=4, train_wall=23, gb_free=10.8, wall=26698
2021-05-01 19:40:40 | INFO | train_inner | epoch 002:  26988 / 60421 loss=1.071, ppl=2.1, wps=16213.2, ups=4.34, wpb=3733.9, bsz=128, num_updates=87400, lr=0.000106966, gnorm=1.298, loss_scale=4, train_wall=23, gb_free=11.1, wall=26721
2021-05-01 19:41:04 | INFO | train_inner | epoch 002:  27088 / 60421 loss=1.063, ppl=2.09, wps=16112.7, ups=4.3, wpb=3749.6, bsz=140.2, num_updates=87500, lr=0.000106904, gnorm=1.351, loss_scale=4, train_wall=23, gb_free=10.8, wall=26745
2021-05-01 19:41:27 | INFO | train_inner | epoch 002:  27188 / 60421 loss=1.071, ppl=2.1, wps=15916.3, ups=4.31, wpb=3688.9, bsz=127.8, num_updates=87600, lr=0.000106843, gnorm=1.281, loss_scale=4, train_wall=23, gb_free=11.2, wall=26768
2021-05-01 19:41:51 | INFO | train_inner | epoch 002:  27288 / 60421 loss=1.019, ppl=2.03, wps=15788.4, ups=4.19, wpb=3767.4, bsz=165.7, num_updates=87700, lr=0.000106783, gnorm=1.371, loss_scale=4, train_wall=24, gb_free=10.8, wall=26792
2021-05-01 19:42:14 | INFO | train_inner | epoch 002:  27388 / 60421 loss=1.086, ppl=2.12, wps=16272.1, ups=4.32, wpb=3769.1, bsz=130, num_updates=87800, lr=0.000106722, gnorm=1.277, loss_scale=4, train_wall=23, gb_free=10.8, wall=26815
2021-05-01 19:42:37 | INFO | train_inner | epoch 002:  27488 / 60421 loss=1.092, ppl=2.13, wps=16257.1, ups=4.42, wpb=3674, bsz=129.4, num_updates=87900, lr=0.000106661, gnorm=1.861, loss_scale=4, train_wall=22, gb_free=10.8, wall=26838
2021-05-01 19:42:59 | INFO | train_inner | epoch 002:  27588 / 60421 loss=1.125, ppl=2.18, wps=16606.6, ups=4.39, wpb=3786.7, bsz=144.9, num_updates=88000, lr=0.0001066, gnorm=1.485, loss_scale=4, train_wall=23, gb_free=10.8, wall=26860
2021-05-01 19:42:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:42:59 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:43:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:43:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:43:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:43:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:43:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:43:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:43:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:43:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:43:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:43:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:43:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:43:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:43:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:43:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:43:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:44:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.632 | ppl 24.8 | bleu 28.82 | wps 2377.7 | wpb 2024.1 | bsz 97.5 | num_updates 88000 | best_bleu 28.82
2021-05-01 19:44:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 88000 updates
2021-05-01 19:44:06 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_88000.pt
2021-05-01 19:44:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_88000.pt
2021-05-01 19:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_88000.pt (epoch 2 @ 88000 updates, score 28.82) (writing took 14.392375552997692 seconds)
2021-05-01 19:44:44 | INFO | train_inner | epoch 002:  27688 / 60421 loss=1.142, ppl=2.21, wps=3631.4, ups=0.96, wpb=3782.8, bsz=124.1, num_updates=88100, lr=0.00010654, gnorm=1.338, loss_scale=4, train_wall=23, gb_free=10.7, wall=26964
2021-05-01 19:45:06 | INFO | train_inner | epoch 002:  27788 / 60421 loss=1.061, ppl=2.09, wps=16364.6, ups=4.43, wpb=3693.5, bsz=127.3, num_updates=88200, lr=0.000106479, gnorm=1.647, loss_scale=4, train_wall=22, gb_free=10.9, wall=26987
2021-05-01 19:45:29 | INFO | train_inner | epoch 002:  27888 / 60421 loss=1.083, ppl=2.12, wps=16482.9, ups=4.38, wpb=3766, bsz=123.4, num_updates=88300, lr=0.000106419, gnorm=1.206, loss_scale=4, train_wall=23, gb_free=10.9, wall=27010
2021-05-01 19:45:52 | INFO | train_inner | epoch 002:  27988 / 60421 loss=1.169, ppl=2.25, wps=16176.5, ups=4.37, wpb=3705.9, bsz=109.7, num_updates=88400, lr=0.000106359, gnorm=1.531, loss_scale=4, train_wall=23, gb_free=10.9, wall=27033
2021-05-01 19:46:15 | INFO | train_inner | epoch 002:  28088 / 60421 loss=1.078, ppl=2.11, wps=16099.8, ups=4.28, wpb=3758.8, bsz=140.1, num_updates=88500, lr=0.000106299, gnorm=1.426, loss_scale=4, train_wall=23, gb_free=10.9, wall=27056
2021-05-01 19:46:39 | INFO | train_inner | epoch 002:  28188 / 60421 loss=1.103, ppl=2.15, wps=15741.1, ups=4.26, wpb=3699, bsz=132.7, num_updates=88600, lr=0.000106239, gnorm=1.493, loss_scale=4, train_wall=23, gb_free=10.9, wall=27080
2021-05-01 19:47:02 | INFO | train_inner | epoch 002:  28288 / 60421 loss=1.099, ppl=2.14, wps=16076.8, ups=4.24, wpb=3788.4, bsz=134.8, num_updates=88700, lr=0.000106179, gnorm=1.255, loss_scale=4, train_wall=23, gb_free=10.9, wall=27103
2021-05-01 19:47:25 | INFO | train_inner | epoch 002:  28388 / 60421 loss=1.089, ppl=2.13, wps=16450.6, ups=4.36, wpb=3774.2, bsz=135.2, num_updates=88800, lr=0.000106119, gnorm=1.263, loss_scale=4, train_wall=23, gb_free=10.7, wall=27126
2021-05-01 19:47:48 | INFO | train_inner | epoch 002:  28488 / 60421 loss=1.006, ppl=2.01, wps=16408.5, ups=4.42, wpb=3709, bsz=141, num_updates=88900, lr=0.000106059, gnorm=1.261, loss_scale=4, train_wall=22, gb_free=10.7, wall=27149
2021-05-01 19:48:10 | INFO | train_inner | epoch 002:  28588 / 60421 loss=1.137, ppl=2.2, wps=16532.4, ups=4.41, wpb=3748.5, bsz=121.9, num_updates=89000, lr=0.000106, gnorm=1.566, loss_scale=4, train_wall=22, gb_free=11.2, wall=27171
2021-05-01 19:48:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:48:11 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:48:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:48:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:48:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:48:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:48:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:48:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:48:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:48:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:48:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:48:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:48:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:48:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:48:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:48:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:48:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:48:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:48:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:48:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:49:15 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.597 | ppl 24.2 | bleu 28.48 | wps 2444.9 | wpb 2024.1 | bsz 97.5 | num_updates 89000 | best_bleu 28.82
2021-05-01 19:49:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 89000 updates
2021-05-01 19:49:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_89000.pt
2021-05-01 19:49:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_89000.pt
2021-05-01 19:49:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_89000.pt (epoch 2 @ 89000 updates, score 28.48) (writing took 7.724383707995003 seconds)
2021-05-01 19:49:46 | INFO | train_inner | epoch 002:  28688 / 60421 loss=1.1, ppl=2.14, wps=3955.5, ups=1.04, wpb=3786, bsz=126.7, num_updates=89100, lr=0.00010594, gnorm=1.39, loss_scale=4, train_wall=23, gb_free=10.8, wall=27267
2021-05-01 19:50:09 | INFO | train_inner | epoch 002:  28788 / 60421 loss=1.103, ppl=2.15, wps=16374.7, ups=4.38, wpb=3742.6, bsz=132.2, num_updates=89200, lr=0.000105881, gnorm=1.287, loss_scale=4, train_wall=23, gb_free=10.9, wall=27290
2021-05-01 19:50:32 | INFO | train_inner | epoch 002:  28888 / 60421 loss=1.103, ppl=2.15, wps=16404, ups=4.34, wpb=3782, bsz=128.6, num_updates=89300, lr=0.000105822, gnorm=1.4, loss_scale=4, train_wall=23, gb_free=10.8, wall=27313
2021-05-01 19:50:55 | INFO | train_inner | epoch 002:  28988 / 60421 loss=1.058, ppl=2.08, wps=16266.9, ups=4.38, wpb=3717.9, bsz=130.2, num_updates=89400, lr=0.000105762, gnorm=1.401, loss_scale=4, train_wall=23, gb_free=10.8, wall=27336
2021-05-01 19:51:18 | INFO | train_inner | epoch 002:  29088 / 60421 loss=1.1, ppl=2.14, wps=15938.4, ups=4.37, wpb=3647.1, bsz=114.1, num_updates=89500, lr=0.000105703, gnorm=1.518, loss_scale=4, train_wall=23, gb_free=10.6, wall=27359
2021-05-01 19:51:41 | INFO | train_inner | epoch 002:  29188 / 60421 loss=1.046, ppl=2.07, wps=15721.6, ups=4.27, wpb=3678.6, bsz=137.5, num_updates=89600, lr=0.000105644, gnorm=1.472, loss_scale=4, train_wall=23, gb_free=11.2, wall=27382
2021-05-01 19:52:05 | INFO | train_inner | epoch 002:  29288 / 60421 loss=1.082, ppl=2.12, wps=15977.3, ups=4.29, wpb=3720.1, bsz=122.2, num_updates=89700, lr=0.000105585, gnorm=1.46, loss_scale=4, train_wall=23, gb_free=10.7, wall=27406
2021-05-01 19:52:27 | INFO | train_inner | epoch 002:  29388 / 60421 loss=1.113, ppl=2.16, wps=16563.5, ups=4.36, wpb=3797.2, bsz=125.8, num_updates=89800, lr=0.000105527, gnorm=1.246, loss_scale=4, train_wall=23, gb_free=10.8, wall=27428
2021-05-01 19:52:50 | INFO | train_inner | epoch 002:  29488 / 60421 loss=1.01, ppl=2.01, wps=16371.1, ups=4.37, wpb=3747.9, bsz=146.3, num_updates=89900, lr=0.000105468, gnorm=1.277, loss_scale=4, train_wall=23, gb_free=10.7, wall=27451
2021-05-01 19:53:13 | INFO | train_inner | epoch 002:  29588 / 60421 loss=1.053, ppl=2.08, wps=16620.8, ups=4.36, wpb=3812.7, bsz=131.2, num_updates=90000, lr=0.000105409, gnorm=1.249, loss_scale=4, train_wall=23, gb_free=10.8, wall=27474
2021-05-01 19:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:53:13 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:53:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:53:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:53:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:53:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:53:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:53:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:53:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:53:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:53:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:53:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:53:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:53:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:54:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.625 | ppl 24.68 | bleu 28.75 | wps 2375.2 | wpb 2024.1 | bsz 97.5 | num_updates 90000 | best_bleu 28.82
2021-05-01 19:54:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 90000 updates
2021-05-01 19:54:20 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_90000.pt
2021-05-01 19:54:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_90000.pt
2021-05-01 19:54:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_90000.pt (epoch 2 @ 90000 updates, score 28.75) (writing took 7.741595047998999 seconds)
2021-05-01 19:54:51 | INFO | train_inner | epoch 002:  29688 / 60421 loss=1.056, ppl=2.08, wps=3823.1, ups=1.02, wpb=3730.5, bsz=133, num_updates=90100, lr=0.000105351, gnorm=1.497, loss_scale=8, train_wall=23, gb_free=10.8, wall=27572
2021-05-01 19:55:14 | INFO | train_inner | epoch 002:  29788 / 60421 loss=1.12, ppl=2.17, wps=16503.4, ups=4.39, wpb=3759.5, bsz=114.6, num_updates=90200, lr=0.000105292, gnorm=1.351, loss_scale=8, train_wall=23, gb_free=11, wall=27595
2021-05-01 19:55:36 | INFO | train_inner | epoch 002:  29888 / 60421 loss=1.148, ppl=2.22, wps=16391.9, ups=4.4, wpb=3726.6, bsz=120.8, num_updates=90300, lr=0.000105234, gnorm=1.44, loss_scale=8, train_wall=23, gb_free=10.8, wall=27617
2021-05-01 19:56:00 | INFO | train_inner | epoch 002:  29988 / 60421 loss=1.118, ppl=2.17, wps=16257.6, ups=4.32, wpb=3766.3, bsz=117.5, num_updates=90400, lr=0.000105176, gnorm=1.324, loss_scale=8, train_wall=23, gb_free=10.7, wall=27641
2021-05-01 19:56:23 | INFO | train_inner | epoch 002:  30088 / 60421 loss=1.101, ppl=2.14, wps=15882.5, ups=4.34, wpb=3658.7, bsz=125.5, num_updates=90500, lr=0.000105118, gnorm=1.607, loss_scale=8, train_wall=23, gb_free=10.7, wall=27664
2021-05-01 19:56:47 | INFO | train_inner | epoch 002:  30188 / 60421 loss=1.081, ppl=2.11, wps=15973.9, ups=4.18, wpb=3822.9, bsz=135.6, num_updates=90600, lr=0.00010506, gnorm=1.158, loss_scale=8, train_wall=24, gb_free=10.7, wall=27687
2021-05-01 19:57:10 | INFO | train_inner | epoch 002:  30288 / 60421 loss=1.121, ppl=2.17, wps=16154, ups=4.31, wpb=3750.8, bsz=135.6, num_updates=90700, lr=0.000105002, gnorm=1.582, loss_scale=8, train_wall=23, gb_free=10.8, wall=27711
2021-05-01 19:57:33 | INFO | train_inner | epoch 002:  30388 / 60421 loss=1.053, ppl=2.07, wps=16421.9, ups=4.39, wpb=3740.2, bsz=130.2, num_updates=90800, lr=0.000104944, gnorm=1.307, loss_scale=8, train_wall=23, gb_free=10.8, wall=27733
2021-05-01 19:57:55 | INFO | train_inner | epoch 002:  30488 / 60421 loss=1.077, ppl=2.11, wps=16512.9, ups=4.45, wpb=3711.2, bsz=135.9, num_updates=90900, lr=0.000104886, gnorm=1.472, loss_scale=8, train_wall=22, gb_free=10.8, wall=27756
2021-05-01 19:58:18 | INFO | train_inner | epoch 002:  30588 / 60421 loss=1.077, ppl=2.11, wps=16492.9, ups=4.44, wpb=3718.3, bsz=123.1, num_updates=91000, lr=0.000104828, gnorm=1.452, loss_scale=8, train_wall=22, gb_free=10.8, wall=27779
2021-05-01 19:58:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 19:58:18 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 19:58:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:58:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:58:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:58:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:58:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:58:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:58:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:58:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:58:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:58:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 19:58:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 19:58:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 19:59:23 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.607 | ppl 24.37 | bleu 28.77 | wps 2431.2 | wpb 2024.1 | bsz 97.5 | num_updates 91000 | best_bleu 28.82
2021-05-01 19:59:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 91000 updates
2021-05-01 19:59:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_91000.pt
2021-05-01 19:59:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_91000.pt
2021-05-01 19:59:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_91000.pt (epoch 2 @ 91000 updates, score 28.77) (writing took 7.753274107999459 seconds)
2021-05-01 19:59:53 | INFO | train_inner | epoch 002:  30688 / 60421 loss=1.136, ppl=2.2, wps=3916, ups=1.04, wpb=3754.7, bsz=121.1, num_updates=91100, lr=0.000104771, gnorm=1.368, loss_scale=8, train_wall=23, gb_free=10.8, wall=27874
2021-05-01 20:00:16 | INFO | train_inner | epoch 002:  30788 / 60421 loss=1.111, ppl=2.16, wps=16271.3, ups=4.43, wpb=3672.8, bsz=126.9, num_updates=91200, lr=0.000104713, gnorm=1.643, loss_scale=8, train_wall=22, gb_free=11.7, wall=27897
2021-05-01 20:00:39 | INFO | train_inner | epoch 002:  30888 / 60421 loss=1.069, ppl=2.1, wps=16381.1, ups=4.38, wpb=3742, bsz=133, num_updates=91300, lr=0.000104656, gnorm=1.326, loss_scale=8, train_wall=23, gb_free=10.7, wall=27920
2021-05-01 20:01:02 | INFO | train_inner | epoch 002:  30988 / 60421 loss=1.051, ppl=2.07, wps=16031.6, ups=4.32, wpb=3707.9, bsz=127.4, num_updates=91400, lr=0.000104599, gnorm=1.29, loss_scale=8, train_wall=23, gb_free=10.9, wall=27943
2021-05-01 20:01:25 | INFO | train_inner | epoch 002:  31088 / 60421 loss=1.07, ppl=2.1, wps=16068.5, ups=4.28, wpb=3756.6, bsz=131.2, num_updates=91500, lr=0.000104542, gnorm=1.296, loss_scale=8, train_wall=23, gb_free=10.7, wall=27966
2021-05-01 20:01:49 | INFO | train_inner | epoch 002:  31188 / 60421 loss=1.089, ppl=2.13, wps=15730.3, ups=4.18, wpb=3759.4, bsz=135.8, num_updates=91600, lr=0.000104485, gnorm=1.453, loss_scale=8, train_wall=24, gb_free=10.7, wall=27990
2021-05-01 20:02:13 | INFO | train_inner | epoch 002:  31288 / 60421 loss=1.074, ppl=2.11, wps=16166.8, ups=4.29, wpb=3771.4, bsz=129.8, num_updates=91700, lr=0.000104428, gnorm=1.404, loss_scale=8, train_wall=23, gb_free=10.9, wall=28014
2021-05-01 20:02:35 | INFO | train_inner | epoch 002:  31388 / 60421 loss=1.084, ppl=2.12, wps=16172, ups=4.45, wpb=3634.4, bsz=123.9, num_updates=91800, lr=0.000104371, gnorm=1.608, loss_scale=8, train_wall=22, gb_free=10.7, wall=28036
2021-05-01 20:02:58 | INFO | train_inner | epoch 002:  31488 / 60421 loss=1.125, ppl=2.18, wps=16531.9, ups=4.39, wpb=3764.5, bsz=119.5, num_updates=91900, lr=0.000104314, gnorm=1.293, loss_scale=8, train_wall=23, gb_free=11, wall=28059
2021-05-01 20:03:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-01 20:03:20 | INFO | train_inner | epoch 002:  31589 / 60421 loss=1.096, ppl=2.14, wps=16456.7, ups=4.42, wpb=3723.1, bsz=114.7, num_updates=92000, lr=0.000104257, gnorm=1.436, loss_scale=4, train_wall=22, gb_free=10.8, wall=28081
2021-05-01 20:03:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:03:20 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:03:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:03:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:03:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:03:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:03:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:03:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:03:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:03:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:03:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:03:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:03:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:03:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:04:26 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.7 | ppl 25.99 | bleu 28.43 | wps 2437.3 | wpb 2024.1 | bsz 97.5 | num_updates 92000 | best_bleu 28.82
2021-05-01 20:04:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 92000 updates
2021-05-01 20:04:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_92000.pt
2021-05-01 20:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_92000.pt
2021-05-01 20:04:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_92000.pt (epoch 2 @ 92000 updates, score 28.43) (writing took 7.765229254000587 seconds)
2021-05-01 20:04:56 | INFO | train_inner | epoch 002:  31689 / 60421 loss=1.066, ppl=2.09, wps=3884.8, ups=1.05, wpb=3716.8, bsz=127.2, num_updates=92100, lr=0.000104201, gnorm=1.394, loss_scale=4, train_wall=23, gb_free=10.9, wall=28177
2021-05-01 20:05:18 | INFO | train_inner | epoch 002:  31789 / 60421 loss=1.052, ppl=2.07, wps=16015.3, ups=4.5, wpb=3558, bsz=129.4, num_updates=92200, lr=0.000104144, gnorm=1.989, loss_scale=4, train_wall=22, gb_free=10.9, wall=28199
2021-05-01 20:05:41 | INFO | train_inner | epoch 002:  31889 / 60421 loss=1.042, ppl=2.06, wps=16252.4, ups=4.37, wpb=3720.8, bsz=131.1, num_updates=92300, lr=0.000104088, gnorm=1.408, loss_scale=4, train_wall=23, gb_free=10.7, wall=28222
2021-05-01 20:06:04 | INFO | train_inner | epoch 002:  31989 / 60421 loss=1.123, ppl=2.18, wps=16222.8, ups=4.33, wpb=3750.1, bsz=126.6, num_updates=92400, lr=0.000104031, gnorm=1.336, loss_scale=4, train_wall=23, gb_free=10.9, wall=28245
2021-05-01 20:06:28 | INFO | train_inner | epoch 002:  32089 / 60421 loss=1.075, ppl=2.11, wps=15928.8, ups=4.29, wpb=3717.3, bsz=123.5, num_updates=92500, lr=0.000103975, gnorm=1.5, loss_scale=4, train_wall=23, gb_free=11.3, wall=28269
2021-05-01 20:06:51 | INFO | train_inner | epoch 002:  32189 / 60421 loss=1.164, ppl=2.24, wps=15726.7, ups=4.25, wpb=3699.1, bsz=126.5, num_updates=92600, lr=0.000103919, gnorm=1.742, loss_scale=4, train_wall=23, gb_free=10.8, wall=28292
2021-05-01 20:07:14 | INFO | train_inner | epoch 002:  32289 / 60421 loss=1.016, ppl=2.02, wps=16101.4, ups=4.34, wpb=3706.2, bsz=140.5, num_updates=92700, lr=0.000103863, gnorm=1.413, loss_scale=4, train_wall=23, gb_free=10.9, wall=28315
2021-05-01 20:07:37 | INFO | train_inner | epoch 002:  32389 / 60421 loss=1.092, ppl=2.13, wps=16406, ups=4.38, wpb=3749.2, bsz=133, num_updates=92800, lr=0.000103807, gnorm=1.174, loss_scale=4, train_wall=23, gb_free=10.8, wall=28338
2021-05-01 20:08:00 | INFO | train_inner | epoch 002:  32489 / 60421 loss=1.048, ppl=2.07, wps=16523.4, ups=4.45, wpb=3715.4, bsz=134.8, num_updates=92900, lr=0.000103751, gnorm=1.296, loss_scale=4, train_wall=22, gb_free=10.7, wall=28361
2021-05-01 20:08:22 | INFO | train_inner | epoch 002:  32589 / 60421 loss=1.043, ppl=2.06, wps=16553.3, ups=4.4, wpb=3763.5, bsz=147.4, num_updates=93000, lr=0.000103695, gnorm=1.203, loss_scale=4, train_wall=23, gb_free=11.4, wall=28383
2021-05-01 20:08:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:08:22 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:08:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:08:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:08:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:08:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:08:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:08:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:08:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:08:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:08:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:08:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:08:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:08:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:09:28 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.652 | ppl 25.14 | bleu 28.73 | wps 2425.8 | wpb 2024.1 | bsz 97.5 | num_updates 93000 | best_bleu 28.82
2021-05-01 20:09:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 93000 updates
2021-05-01 20:09:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_93000.pt
2021-05-01 20:09:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_93000.pt
2021-05-01 20:09:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_93000.pt (epoch 2 @ 93000 updates, score 28.73) (writing took 7.738659087997803 seconds)
2021-05-01 20:09:58 | INFO | train_inner | epoch 002:  32689 / 60421 loss=1.058, ppl=2.08, wps=3947.2, ups=1.04, wpb=3796.9, bsz=151.7, num_updates=93100, lr=0.000103639, gnorm=1.153, loss_scale=4, train_wall=23, gb_free=10.9, wall=28479
2021-05-01 20:10:21 | INFO | train_inner | epoch 002:  32789 / 60421 loss=1.024, ppl=2.03, wps=16365.7, ups=4.4, wpb=3722.6, bsz=141.1, num_updates=93200, lr=0.000103584, gnorm=1.489, loss_scale=4, train_wall=23, gb_free=12, wall=28502
2021-05-01 20:10:44 | INFO | train_inner | epoch 002:  32889 / 60421 loss=1.112, ppl=2.16, wps=16181, ups=4.43, wpb=3656.5, bsz=123.8, num_updates=93300, lr=0.000103528, gnorm=1.339, loss_scale=4, train_wall=22, gb_free=11.2, wall=28525
2021-05-01 20:11:07 | INFO | train_inner | epoch 002:  32989 / 60421 loss=1.164, ppl=2.24, wps=16157.2, ups=4.31, wpb=3747.3, bsz=132.7, num_updates=93400, lr=0.000103473, gnorm=1.652, loss_scale=4, train_wall=23, gb_free=10.8, wall=28548
2021-05-01 20:11:30 | INFO | train_inner | epoch 002:  33089 / 60421 loss=1.073, ppl=2.1, wps=15791.5, ups=4.3, wpb=3671.1, bsz=116.2, num_updates=93500, lr=0.000103418, gnorm=1.533, loss_scale=4, train_wall=23, gb_free=11.1, wall=28571
2021-05-01 20:11:54 | INFO | train_inner | epoch 002:  33189 / 60421 loss=1.074, ppl=2.1, wps=15625.3, ups=4.25, wpb=3678.9, bsz=159.2, num_updates=93600, lr=0.000103362, gnorm=1.673, loss_scale=4, train_wall=23, gb_free=10.8, wall=28595
2021-05-01 20:12:17 | INFO | train_inner | epoch 002:  33289 / 60421 loss=1.079, ppl=2.11, wps=16258.6, ups=4.32, wpb=3763.1, bsz=150.8, num_updates=93700, lr=0.000103307, gnorm=1.371, loss_scale=4, train_wall=23, gb_free=10.8, wall=28618
2021-05-01 20:12:40 | INFO | train_inner | epoch 002:  33389 / 60421 loss=0.972, ppl=1.96, wps=16235.7, ups=4.36, wpb=3721.4, bsz=157, num_updates=93800, lr=0.000103252, gnorm=1.496, loss_scale=4, train_wall=23, gb_free=10.8, wall=28641
2021-05-01 20:13:02 | INFO | train_inner | epoch 002:  33489 / 60421 loss=1.082, ppl=2.12, wps=16463.1, ups=4.44, wpb=3706.7, bsz=130.2, num_updates=93900, lr=0.000103197, gnorm=1.694, loss_scale=4, train_wall=22, gb_free=10.8, wall=28663
2021-05-01 20:13:25 | INFO | train_inner | epoch 002:  33589 / 60421 loss=1.127, ppl=2.18, wps=16478.9, ups=4.43, wpb=3722.7, bsz=120.6, num_updates=94000, lr=0.000103142, gnorm=1.559, loss_scale=4, train_wall=22, gb_free=11, wall=28686
2021-05-01 20:13:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:13:25 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:13:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:13:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:13:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:13:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:13:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:13:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:14:29 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.736 | ppl 26.65 | bleu 26.78 | wps 2460.8 | wpb 2024.1 | bsz 97.5 | num_updates 94000 | best_bleu 28.82
2021-05-01 20:14:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 94000 updates
2021-05-01 20:14:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_94000.pt
2021-05-01 20:14:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_94000.pt
2021-05-01 20:14:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_94000.pt (epoch 2 @ 94000 updates, score 26.78) (writing took 10.109506159999 seconds)
2021-05-01 20:15:02 | INFO | train_inner | epoch 002:  33689 / 60421 loss=1.057, ppl=2.08, wps=3832.8, ups=1.03, wpb=3733.2, bsz=139.6, num_updates=94100, lr=0.000103087, gnorm=1.309, loss_scale=4, train_wall=23, gb_free=10.9, wall=28783
2021-05-01 20:15:25 | INFO | train_inner | epoch 002:  33789 / 60421 loss=1.119, ppl=2.17, wps=16546.3, ups=4.4, wpb=3756.7, bsz=130.2, num_updates=94200, lr=0.000103033, gnorm=1.371, loss_scale=4, train_wall=23, gb_free=10.8, wall=28806
2021-05-01 20:15:48 | INFO | train_inner | epoch 002:  33889 / 60421 loss=1.061, ppl=2.09, wps=16298.9, ups=4.35, wpb=3749.9, bsz=131.1, num_updates=94300, lr=0.000102978, gnorm=1.308, loss_scale=4, train_wall=23, gb_free=10.9, wall=28829
2021-05-01 20:16:11 | INFO | train_inner | epoch 002:  33989 / 60421 loss=1.088, ppl=2.13, wps=16175.7, ups=4.33, wpb=3734.1, bsz=116.6, num_updates=94400, lr=0.000102923, gnorm=1.315, loss_scale=4, train_wall=23, gb_free=10.7, wall=28852
2021-05-01 20:16:34 | INFO | train_inner | epoch 002:  34089 / 60421 loss=1.041, ppl=2.06, wps=15790.4, ups=4.32, wpb=3656.2, bsz=120.2, num_updates=94500, lr=0.000102869, gnorm=1.37, loss_scale=4, train_wall=23, gb_free=10.7, wall=28875
2021-05-01 20:16:58 | INFO | train_inner | epoch 002:  34189 / 60421 loss=1.025, ppl=2.03, wps=15888.1, ups=4.29, wpb=3703.9, bsz=139.2, num_updates=94600, lr=0.000102815, gnorm=1.46, loss_scale=4, train_wall=23, gb_free=10.7, wall=28899
2021-05-01 20:17:20 | INFO | train_inner | epoch 002:  34289 / 60421 loss=0.962, ppl=1.95, wps=16177.4, ups=4.41, wpb=3672.1, bsz=147.2, num_updates=94700, lr=0.00010276, gnorm=1.234, loss_scale=4, train_wall=23, gb_free=10.8, wall=28921
2021-05-01 20:17:43 | INFO | train_inner | epoch 002:  34389 / 60421 loss=1.079, ppl=2.11, wps=16500.8, ups=4.43, wpb=3725.8, bsz=132.3, num_updates=94800, lr=0.000102706, gnorm=1.426, loss_scale=4, train_wall=22, gb_free=10.8, wall=28944
2021-05-01 20:18:06 | INFO | train_inner | epoch 002:  34489 / 60421 loss=1.062, ppl=2.09, wps=16470.7, ups=4.42, wpb=3727.3, bsz=131.1, num_updates=94900, lr=0.000102652, gnorm=1.46, loss_scale=4, train_wall=22, gb_free=11, wall=28967
2021-05-01 20:18:28 | INFO | train_inner | epoch 002:  34589 / 60421 loss=1.03, ppl=2.04, wps=16501.1, ups=4.43, wpb=3721.5, bsz=135, num_updates=95000, lr=0.000102598, gnorm=1.57, loss_scale=4, train_wall=22, gb_free=10.7, wall=28989
2021-05-01 20:18:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:18:28 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:18:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:18:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:18:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:18:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:18:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:18:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:18:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:18:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:18:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:18:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:18:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:18:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:18:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:18:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:18:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:18:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:18:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:18:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:19:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.667 | ppl 25.4 | bleu 28.57 | wps 2428.2 | wpb 2024.1 | bsz 97.5 | num_updates 95000 | best_bleu 28.82
2021-05-01 20:19:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 95000 updates
2021-05-01 20:19:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_95000.pt
2021-05-01 20:19:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_95000.pt
2021-05-01 20:19:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_95000.pt (epoch 2 @ 95000 updates, score 28.57) (writing took 7.777822759999253 seconds)
2021-05-01 20:20:04 | INFO | train_inner | epoch 002:  34689 / 60421 loss=1.071, ppl=2.1, wps=3914.6, ups=1.04, wpb=3761.1, bsz=135.5, num_updates=95100, lr=0.000102544, gnorm=1.589, loss_scale=4, train_wall=23, gb_free=11, wall=29085
2021-05-01 20:20:27 | INFO | train_inner | epoch 002:  34789 / 60421 loss=1.093, ppl=2.13, wps=16335.6, ups=4.46, wpb=3660, bsz=119.6, num_updates=95200, lr=0.00010249, gnorm=1.719, loss_scale=4, train_wall=22, gb_free=10.9, wall=29108
2021-05-01 20:20:49 | INFO | train_inner | epoch 002:  34889 / 60421 loss=1.09, ppl=2.13, wps=16250.8, ups=4.37, wpb=3721, bsz=127.6, num_updates=95300, lr=0.000102436, gnorm=1.359, loss_scale=4, train_wall=23, gb_free=10.6, wall=29130
2021-05-01 20:21:12 | INFO | train_inner | epoch 002:  34989 / 60421 loss=1.145, ppl=2.21, wps=16132.1, ups=4.36, wpb=3704, bsz=126.7, num_updates=95400, lr=0.000102383, gnorm=1.835, loss_scale=4, train_wall=23, gb_free=10.7, wall=29153
2021-05-01 20:21:36 | INFO | train_inner | epoch 002:  35089 / 60421 loss=1.106, ppl=2.15, wps=16004, ups=4.22, wpb=3793.5, bsz=138.9, num_updates=95500, lr=0.000102329, gnorm=1.329, loss_scale=4, train_wall=24, gb_free=10.8, wall=29177
2021-05-01 20:21:59 | INFO | train_inner | epoch 002:  35189 / 60421 loss=1.128, ppl=2.19, wps=15924.3, ups=4.32, wpb=3688.5, bsz=110.6, num_updates=95600, lr=0.000102275, gnorm=1.665, loss_scale=4, train_wall=23, gb_free=10.8, wall=29200
2021-05-01 20:22:22 | INFO | train_inner | epoch 002:  35289 / 60421 loss=1.079, ppl=2.11, wps=16446.1, ups=4.36, wpb=3769.6, bsz=140.1, num_updates=95700, lr=0.000102222, gnorm=1.187, loss_scale=4, train_wall=23, gb_free=10.8, wall=29223
2021-05-01 20:22:45 | INFO | train_inner | epoch 002:  35389 / 60421 loss=1.058, ppl=2.08, wps=16559.4, ups=4.47, wpb=3708, bsz=131.4, num_updates=95800, lr=0.000102169, gnorm=1.342, loss_scale=4, train_wall=22, gb_free=11.3, wall=29246
2021-05-01 20:23:07 | INFO | train_inner | epoch 002:  35489 / 60421 loss=1.076, ppl=2.11, wps=16627.5, ups=4.42, wpb=3760.7, bsz=137.2, num_updates=95900, lr=0.000102115, gnorm=1.232, loss_scale=4, train_wall=22, gb_free=10.7, wall=29268
2021-05-01 20:23:30 | INFO | train_inner | epoch 002:  35589 / 60421 loss=1.021, ppl=2.03, wps=16567.5, ups=4.43, wpb=3740.4, bsz=125, num_updates=96000, lr=0.000102062, gnorm=1.364, loss_scale=4, train_wall=22, gb_free=11.4, wall=29291
2021-05-01 20:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:23:30 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:23:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:23:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:23:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:23:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:23:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:23:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:23:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:23:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:23:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:24:34 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.66 | ppl 25.29 | bleu 28.55 | wps 2454 | wpb 2024.1 | bsz 97.5 | num_updates 96000 | best_bleu 28.82
2021-05-01 20:24:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 96000 updates
2021-05-01 20:24:34 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_96000.pt
2021-05-01 20:24:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_96000.pt
2021-05-01 20:24:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_96000.pt (epoch 2 @ 96000 updates, score 28.55) (writing took 7.762678020000749 seconds)
2021-05-01 20:25:05 | INFO | train_inner | epoch 002:  35689 / 60421 loss=1.122, ppl=2.18, wps=3923.7, ups=1.05, wpb=3729.5, bsz=132.8, num_updates=96100, lr=0.000102009, gnorm=1.534, loss_scale=4, train_wall=22, gb_free=10.8, wall=29386
2021-05-01 20:25:28 | INFO | train_inner | epoch 002:  35789 / 60421 loss=1.061, ppl=2.09, wps=16323.7, ups=4.42, wpb=3693.2, bsz=135.5, num_updates=96200, lr=0.000101956, gnorm=1.54, loss_scale=4, train_wall=22, gb_free=10.8, wall=29408
2021-05-01 20:25:50 | INFO | train_inner | epoch 002:  35889 / 60421 loss=1.053, ppl=2.08, wps=16254.6, ups=4.39, wpb=3706, bsz=132.6, num_updates=96300, lr=0.000101903, gnorm=1.284, loss_scale=4, train_wall=23, gb_free=10.6, wall=29431
2021-05-01 20:26:13 | INFO | train_inner | epoch 002:  35989 / 60421 loss=1.087, ppl=2.12, wps=16075.5, ups=4.34, wpb=3705.1, bsz=124, num_updates=96400, lr=0.00010185, gnorm=1.704, loss_scale=4, train_wall=23, gb_free=10.8, wall=29454
2021-05-01 20:26:37 | INFO | train_inner | epoch 002:  36089 / 60421 loss=1.083, ppl=2.12, wps=16084.8, ups=4.23, wpb=3800.3, bsz=126.4, num_updates=96500, lr=0.000101797, gnorm=1.346, loss_scale=4, train_wall=23, gb_free=11, wall=29478
2021-05-01 20:27:01 | INFO | train_inner | epoch 002:  36189 / 60421 loss=1.002, ppl=2, wps=16094.1, ups=4.23, wpb=3801.3, bsz=149.6, num_updates=96600, lr=0.000101745, gnorm=1.118, loss_scale=4, train_wall=23, gb_free=10.8, wall=29502
2021-05-01 20:27:23 | INFO | train_inner | epoch 002:  36289 / 60421 loss=1.077, ppl=2.11, wps=16303.3, ups=4.4, wpb=3706, bsz=125.1, num_updates=96700, lr=0.000101692, gnorm=1.509, loss_scale=4, train_wall=23, gb_free=10.9, wall=29524
2021-05-01 20:27:46 | INFO | train_inner | epoch 002:  36389 / 60421 loss=1.051, ppl=2.07, wps=16627.3, ups=4.41, wpb=3770.7, bsz=141.3, num_updates=96800, lr=0.000101639, gnorm=1.459, loss_scale=4, train_wall=22, gb_free=10.7, wall=29547
2021-05-01 20:28:09 | INFO | train_inner | epoch 002:  36489 / 60421 loss=1.106, ppl=2.15, wps=16704.7, ups=4.44, wpb=3759.1, bsz=125.3, num_updates=96900, lr=0.000101587, gnorm=1.404, loss_scale=4, train_wall=22, gb_free=10.7, wall=29569
2021-05-01 20:28:31 | INFO | train_inner | epoch 002:  36589 / 60421 loss=1.064, ppl=2.09, wps=16551.7, ups=4.42, wpb=3741.3, bsz=125.5, num_updates=97000, lr=0.000101535, gnorm=1.357, loss_scale=4, train_wall=22, gb_free=10.8, wall=29592
2021-05-01 20:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:28:31 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:28:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:28:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:28:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:28:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:28:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:28:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:28:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:28:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:28:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:28:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:28:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:28:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:28:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:28:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:28:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:28:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:28:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:28:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:29:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.664 | ppl 25.36 | bleu 28.97 | wps 2426.3 | wpb 2024.1 | bsz 97.5 | num_updates 97000 | best_bleu 28.97
2021-05-01 20:29:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 97000 updates
2021-05-01 20:29:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_97000.pt
2021-05-01 20:29:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_97000.pt
2021-05-01 20:29:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_97000.pt (epoch 2 @ 97000 updates, score 28.97) (writing took 14.034161368996138 seconds)
2021-05-01 20:30:13 | INFO | train_inner | epoch 002:  36689 / 60421 loss=1.047, ppl=2.07, wps=3673.2, ups=0.98, wpb=3756.3, bsz=134.6, num_updates=97100, lr=0.000101482, gnorm=1.19, loss_scale=4, train_wall=23, gb_free=11, wall=29694
2021-05-01 20:30:36 | INFO | train_inner | epoch 002:  36789 / 60421 loss=1.062, ppl=2.09, wps=16508.4, ups=4.36, wpb=3790.2, bsz=119.1, num_updates=97200, lr=0.00010143, gnorm=1.236, loss_scale=4, train_wall=23, gb_free=10.6, wall=29717
2021-05-01 20:30:59 | INFO | train_inner | epoch 002:  36889 / 60421 loss=1.147, ppl=2.22, wps=16267.5, ups=4.4, wpb=3693.3, bsz=120.5, num_updates=97300, lr=0.000101378, gnorm=1.692, loss_scale=4, train_wall=23, gb_free=10.8, wall=29740
2021-05-01 20:31:22 | INFO | train_inner | epoch 002:  36989 / 60421 loss=1.048, ppl=2.07, wps=16108.6, ups=4.31, wpb=3739.6, bsz=124.1, num_updates=97400, lr=0.000101326, gnorm=1.296, loss_scale=4, train_wall=23, gb_free=10.8, wall=29763
2021-05-01 20:31:45 | INFO | train_inner | epoch 002:  37089 / 60421 loss=1.105, ppl=2.15, wps=15853.8, ups=4.31, wpb=3676.2, bsz=118.3, num_updates=97500, lr=0.000101274, gnorm=1.714, loss_scale=4, train_wall=23, gb_free=10.8, wall=29786
2021-05-01 20:32:09 | INFO | train_inner | epoch 002:  37189 / 60421 loss=1.097, ppl=2.14, wps=16297.1, ups=4.33, wpb=3767.6, bsz=138.5, num_updates=97600, lr=0.000101222, gnorm=1.47, loss_scale=4, train_wall=23, gb_free=10.8, wall=29810
2021-05-01 20:32:31 | INFO | train_inner | epoch 002:  37289 / 60421 loss=1.034, ppl=2.05, wps=16509.6, ups=4.38, wpb=3773.3, bsz=163.8, num_updates=97700, lr=0.00010117, gnorm=1.362, loss_scale=4, train_wall=23, gb_free=10.8, wall=29832
2021-05-01 20:32:54 | INFO | train_inner | epoch 002:  37389 / 60421 loss=1.065, ppl=2.09, wps=16292.5, ups=4.46, wpb=3650.5, bsz=117.6, num_updates=97800, lr=0.000101118, gnorm=1.511, loss_scale=4, train_wall=22, gb_free=11, wall=29855
2021-05-01 20:33:17 | INFO | train_inner | epoch 002:  37489 / 60421 loss=1.016, ppl=2.02, wps=16485.7, ups=4.41, wpb=3737.7, bsz=139.9, num_updates=97900, lr=0.000101067, gnorm=1.379, loss_scale=4, train_wall=22, gb_free=10.7, wall=29877
2021-05-01 20:33:39 | INFO | train_inner | epoch 002:  37589 / 60421 loss=1.112, ppl=2.16, wps=16650.8, ups=4.43, wpb=3754.6, bsz=125.8, num_updates=98000, lr=0.000101015, gnorm=1.438, loss_scale=4, train_wall=22, gb_free=10.8, wall=29900
2021-05-01 20:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:33:39 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:33:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:33:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:33:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:33:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:33:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:33:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:33:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:33:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:33:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:33:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:33:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:33:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:34:44 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.684 | ppl 25.71 | bleu 28.79 | wps 2459.4 | wpb 2024.1 | bsz 97.5 | num_updates 98000 | best_bleu 28.97
2021-05-01 20:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 98000 updates
2021-05-01 20:34:44 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_98000.pt
2021-05-01 20:34:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_98000.pt
2021-05-01 20:34:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_98000.pt (epoch 2 @ 98000 updates, score 28.79) (writing took 7.737367159999849 seconds)
2021-05-01 20:35:14 | INFO | train_inner | epoch 002:  37689 / 60421 loss=1.036, ppl=2.05, wps=3894.2, ups=1.05, wpb=3693.4, bsz=142.2, num_updates=98100, lr=0.000100964, gnorm=1.799, loss_scale=4, train_wall=22, gb_free=11, wall=29995
2021-05-01 20:35:37 | INFO | train_inner | epoch 002:  37789 / 60421 loss=1.063, ppl=2.09, wps=16338.2, ups=4.36, wpb=3745.4, bsz=139.8, num_updates=98200, lr=0.000100912, gnorm=1.444, loss_scale=4, train_wall=23, gb_free=10.7, wall=30018
2021-05-01 20:36:00 | INFO | train_inner | epoch 002:  37889 / 60421 loss=1.048, ppl=2.07, wps=16407.6, ups=4.35, wpb=3770.9, bsz=128.3, num_updates=98300, lr=0.000100861, gnorm=1.168, loss_scale=4, train_wall=23, gb_free=11, wall=30041
2021-05-01 20:36:23 | INFO | train_inner | epoch 002:  37989 / 60421 loss=1.011, ppl=2.02, wps=15942.1, ups=4.29, wpb=3713.1, bsz=158.8, num_updates=98400, lr=0.00010081, gnorm=1.656, loss_scale=4, train_wall=23, gb_free=10.9, wall=30064
2021-05-01 20:36:47 | INFO | train_inner | epoch 002:  38089 / 60421 loss=1.069, ppl=2.1, wps=15997, ups=4.24, wpb=3773.2, bsz=133.3, num_updates=98500, lr=0.000100759, gnorm=1.413, loss_scale=4, train_wall=23, gb_free=11.1, wall=30088
2021-05-01 20:37:10 | INFO | train_inner | epoch 002:  38189 / 60421 loss=1.033, ppl=2.05, wps=16242.2, ups=4.33, wpb=3746.9, bsz=137.4, num_updates=98600, lr=0.000100707, gnorm=1.328, loss_scale=4, train_wall=23, gb_free=10.8, wall=30111
2021-05-01 20:37:33 | INFO | train_inner | epoch 002:  38289 / 60421 loss=1.094, ppl=2.13, wps=16671.6, ups=4.38, wpb=3809.1, bsz=142.6, num_updates=98700, lr=0.000100656, gnorm=1.25, loss_scale=4, train_wall=23, gb_free=10.7, wall=30134
2021-05-01 20:37:55 | INFO | train_inner | epoch 002:  38389 / 60421 loss=1.054, ppl=2.08, wps=16494.5, ups=4.39, wpb=3756.7, bsz=161.1, num_updates=98800, lr=0.000100605, gnorm=1.35, loss_scale=4, train_wall=23, gb_free=10.7, wall=30156
2021-05-01 20:38:18 | INFO | train_inner | epoch 002:  38489 / 60421 loss=1.061, ppl=2.09, wps=16556.4, ups=4.44, wpb=3732.4, bsz=135.1, num_updates=98900, lr=0.000100555, gnorm=1.389, loss_scale=4, train_wall=22, gb_free=10.8, wall=30179
2021-05-01 20:38:41 | INFO | train_inner | epoch 002:  38589 / 60421 loss=1.105, ppl=2.15, wps=16699.9, ups=4.4, wpb=3793.7, bsz=137.8, num_updates=99000, lr=0.000100504, gnorm=1.466, loss_scale=4, train_wall=23, gb_free=10.7, wall=30202
2021-05-01 20:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:38:41 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:38:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:38:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:38:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:38:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:38:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:38:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:38:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:38:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:38:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:38:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:38:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:38:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:38:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:38:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:38:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:39:46 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.681 | ppl 25.65 | bleu 28.79 | wps 2436.8 | wpb 2024.1 | bsz 97.5 | num_updates 99000 | best_bleu 28.97
2021-05-01 20:39:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 99000 updates
2021-05-01 20:39:46 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_99000.pt
2021-05-01 20:39:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_99000.pt
2021-05-01 20:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_99000.pt (epoch 2 @ 99000 updates, score 28.79) (writing took 7.752702276004129 seconds)
2021-05-01 20:40:16 | INFO | train_inner | epoch 002:  38689 / 60421 loss=1.05, ppl=2.07, wps=3939.2, ups=1.04, wpb=3775.8, bsz=142.7, num_updates=99100, lr=0.000100453, gnorm=1.557, loss_scale=4, train_wall=23, gb_free=10.6, wall=30297
2021-05-01 20:40:39 | INFO | train_inner | epoch 002:  38789 / 60421 loss=1.095, ppl=2.14, wps=16351.8, ups=4.41, wpb=3708.7, bsz=121.8, num_updates=99200, lr=0.000100402, gnorm=1.621, loss_scale=4, train_wall=23, gb_free=10.8, wall=30320
2021-05-01 20:41:02 | INFO | train_inner | epoch 002:  38889 / 60421 loss=1.043, ppl=2.06, wps=16276.1, ups=4.33, wpb=3759.3, bsz=128.1, num_updates=99300, lr=0.000100352, gnorm=1.178, loss_scale=4, train_wall=23, gb_free=10.9, wall=30343
2021-05-01 20:41:25 | INFO | train_inner | epoch 002:  38989 / 60421 loss=1.061, ppl=2.09, wps=16038, ups=4.34, wpb=3696.3, bsz=129, num_updates=99400, lr=0.000100301, gnorm=1.784, loss_scale=4, train_wall=23, gb_free=10.8, wall=30366
2021-05-01 20:41:49 | INFO | train_inner | epoch 002:  39089 / 60421 loss=1.074, ppl=2.1, wps=15821.8, ups=4.29, wpb=3691.5, bsz=111.3, num_updates=99500, lr=0.000100251, gnorm=1.373, loss_scale=4, train_wall=23, gb_free=10.9, wall=30390
2021-05-01 20:42:12 | INFO | train_inner | epoch 002:  39189 / 60421 loss=1.031, ppl=2.04, wps=16228.3, ups=4.29, wpb=3779, bsz=135.7, num_updates=99600, lr=0.000100201, gnorm=1.307, loss_scale=4, train_wall=23, gb_free=10.8, wall=30413
2021-05-01 20:42:35 | INFO | train_inner | epoch 002:  39289 / 60421 loss=1.047, ppl=2.07, wps=16612.3, ups=4.41, wpb=3769.9, bsz=134.2, num_updates=99700, lr=0.00010015, gnorm=1.149, loss_scale=4, train_wall=23, gb_free=10.8, wall=30436
2021-05-01 20:42:57 | INFO | train_inner | epoch 002:  39389 / 60421 loss=0.98, ppl=1.97, wps=16521.7, ups=4.41, wpb=3745.5, bsz=144.6, num_updates=99800, lr=0.0001001, gnorm=1.159, loss_scale=4, train_wall=22, gb_free=10.7, wall=30458
2021-05-01 20:43:20 | INFO | train_inner | epoch 002:  39489 / 60421 loss=1.015, ppl=2.02, wps=16598, ups=4.4, wpb=3774.8, bsz=136, num_updates=99900, lr=0.00010005, gnorm=1.185, loss_scale=4, train_wall=23, gb_free=10.8, wall=30481
2021-05-01 20:43:43 | INFO | train_inner | epoch 002:  39589 / 60421 loss=0.988, ppl=1.98, wps=16683.7, ups=4.39, wpb=3796.7, bsz=155.1, num_updates=100000, lr=0.0001, gnorm=1.086, loss_scale=4, train_wall=23, gb_free=10.7, wall=30504
2021-05-01 20:43:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:43:43 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:43:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:43:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:43:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:43:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:43:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:43:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:43:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:43:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:43:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:43:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:43:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:43:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:44:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:44:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:44:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:44:48 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.701 | ppl 26.01 | bleu 29.01 | wps 2443 | wpb 2024.1 | bsz 97.5 | num_updates 100000 | best_bleu 29.01
2021-05-01 20:44:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 100000 updates
2021-05-01 20:44:48 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_100000.pt
2021-05-01 20:44:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_100000.pt
2021-05-01 20:45:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_100000.pt (epoch 2 @ 100000 updates, score 29.01) (writing took 14.369416658002592 seconds)
2021-05-01 20:45:25 | INFO | train_inner | epoch 002:  39689 / 60421 loss=1.068, ppl=2.1, wps=3628.5, ups=0.98, wpb=3700.9, bsz=110.9, num_updates=100100, lr=9.995e-05, gnorm=1.386, loss_scale=4, train_wall=22, gb_free=10.9, wall=30606
2021-05-01 20:45:48 | INFO | train_inner | epoch 002:  39789 / 60421 loss=1.05, ppl=2.07, wps=16373.2, ups=4.4, wpb=3722.6, bsz=137.7, num_updates=100200, lr=9.99001e-05, gnorm=1.465, loss_scale=4, train_wall=23, gb_free=10.7, wall=30629
2021-05-01 20:46:11 | INFO | train_inner | epoch 002:  39889 / 60421 loss=1.004, ppl=2.01, wps=16062.8, ups=4.34, wpb=3702.2, bsz=152.2, num_updates=100300, lr=9.98503e-05, gnorm=1.381, loss_scale=4, train_wall=23, gb_free=10.8, wall=30652
2021-05-01 20:46:34 | INFO | train_inner | epoch 002:  39989 / 60421 loss=0.98, ppl=1.97, wps=15864.4, ups=4.3, wpb=3686, bsz=150.2, num_updates=100400, lr=9.98006e-05, gnorm=1.43, loss_scale=4, train_wall=23, gb_free=11.1, wall=30675
2021-05-01 20:46:57 | INFO | train_inner | epoch 002:  40089 / 60421 loss=1.068, ppl=2.1, wps=16204, ups=4.25, wpb=3810.8, bsz=132.4, num_updates=100500, lr=9.97509e-05, gnorm=1.149, loss_scale=4, train_wall=23, gb_free=10.8, wall=30698
2021-05-01 20:47:20 | INFO | train_inner | epoch 002:  40189 / 60421 loss=1.098, ppl=2.14, wps=16315.5, ups=4.42, wpb=3692.5, bsz=131.4, num_updates=100600, lr=9.97013e-05, gnorm=1.678, loss_scale=4, train_wall=22, gb_free=10.7, wall=30721
2021-05-01 20:47:43 | INFO | train_inner | epoch 002:  40289 / 60421 loss=1.049, ppl=2.07, wps=16596.4, ups=4.4, wpb=3769.2, bsz=146.5, num_updates=100700, lr=9.96518e-05, gnorm=1.328, loss_scale=4, train_wall=23, gb_free=10.7, wall=30744
2021-05-01 20:48:05 | INFO | train_inner | epoch 002:  40389 / 60421 loss=1.024, ppl=2.03, wps=16498.1, ups=4.41, wpb=3741.8, bsz=148.4, num_updates=100800, lr=9.96024e-05, gnorm=1.65, loss_scale=4, train_wall=22, gb_free=10.9, wall=30766
2021-05-01 20:48:28 | INFO | train_inner | epoch 002:  40489 / 60421 loss=1.072, ppl=2.1, wps=16531.6, ups=4.45, wpb=3713.7, bsz=119.1, num_updates=100900, lr=9.9553e-05, gnorm=1.373, loss_scale=4, train_wall=22, gb_free=11, wall=30789
2021-05-01 20:48:50 | INFO | train_inner | epoch 002:  40589 / 60421 loss=1.112, ppl=2.16, wps=16523.3, ups=4.47, wpb=3699, bsz=121.5, num_updates=101000, lr=9.95037e-05, gnorm=1.372, loss_scale=4, train_wall=22, gb_free=10.8, wall=30811
2021-05-01 20:48:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:48:50 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:49:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:49:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:49:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:49:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:49:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:49:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:49:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:49:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:49:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:49:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:49:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:49:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:49:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:49:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:49:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:49:55 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.693 | ppl 25.86 | bleu 28.81 | wps 2456.1 | wpb 2024.1 | bsz 97.5 | num_updates 101000 | best_bleu 29.01
2021-05-01 20:49:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 101000 updates
2021-05-01 20:49:55 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_101000.pt
2021-05-01 20:49:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_101000.pt
2021-05-01 20:50:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_101000.pt (epoch 2 @ 101000 updates, score 28.81) (writing took 7.747258743002021 seconds)
2021-05-01 20:50:25 | INFO | train_inner | epoch 002:  40689 / 60421 loss=1.001, ppl=2, wps=3919.6, ups=1.05, wpb=3731, bsz=150.8, num_updates=101100, lr=9.94545e-05, gnorm=1.69, loss_scale=4, train_wall=23, gb_free=10.9, wall=30906
2021-05-01 20:50:48 | INFO | train_inner | epoch 002:  40789 / 60421 loss=1.104, ppl=2.15, wps=16305.7, ups=4.35, wpb=3748.8, bsz=119.1, num_updates=101200, lr=9.94053e-05, gnorm=1.471, loss_scale=4, train_wall=23, gb_free=10.8, wall=30929
2021-05-01 20:51:12 | INFO | train_inner | epoch 002:  40889 / 60421 loss=1.049, ppl=2.07, wps=16338, ups=4.31, wpb=3791.9, bsz=130.6, num_updates=101300, lr=9.93563e-05, gnorm=1.178, loss_scale=4, train_wall=23, gb_free=10.7, wall=30953
2021-05-01 20:51:35 | INFO | train_inner | epoch 002:  40989 / 60421 loss=1.041, ppl=2.06, wps=15926.4, ups=4.34, wpb=3673.3, bsz=127.3, num_updates=101400, lr=9.93073e-05, gnorm=1.751, loss_scale=4, train_wall=23, gb_free=11.1, wall=30976
2021-05-01 20:51:58 | INFO | train_inner | epoch 002:  41089 / 60421 loss=1.102, ppl=2.15, wps=16054.1, ups=4.31, wpb=3727.9, bsz=116.5, num_updates=101500, lr=9.92583e-05, gnorm=1.288, loss_scale=4, train_wall=23, gb_free=10.7, wall=30999
2021-05-01 20:52:21 | INFO | train_inner | epoch 002:  41189 / 60421 loss=1.013, ppl=2.02, wps=16226.5, ups=4.41, wpb=3679, bsz=141.4, num_updates=101600, lr=9.92095e-05, gnorm=1.444, loss_scale=4, train_wall=22, gb_free=11.1, wall=31022
2021-05-01 20:52:43 | INFO | train_inner | epoch 002:  41289 / 60421 loss=1.042, ppl=2.06, wps=16621.2, ups=4.41, wpb=3770.7, bsz=142.6, num_updates=101700, lr=9.91607e-05, gnorm=1.294, loss_scale=4, train_wall=23, gb_free=10.8, wall=31044
2021-05-01 20:53:06 | INFO | train_inner | epoch 002:  41389 / 60421 loss=1.036, ppl=2.05, wps=16606, ups=4.42, wpb=3752.9, bsz=134.3, num_updates=101800, lr=9.9112e-05, gnorm=1.184, loss_scale=4, train_wall=22, gb_free=10.9, wall=31067
2021-05-01 20:53:28 | INFO | train_inner | epoch 002:  41489 / 60421 loss=1.074, ppl=2.1, wps=16669.2, ups=4.42, wpb=3771.3, bsz=119.9, num_updates=101900, lr=9.90633e-05, gnorm=1.275, loss_scale=4, train_wall=22, gb_free=10.8, wall=31089
2021-05-01 20:53:51 | INFO | train_inner | epoch 002:  41589 / 60421 loss=1.07, ppl=2.1, wps=16589.9, ups=4.4, wpb=3773.4, bsz=128.8, num_updates=102000, lr=9.90148e-05, gnorm=1.446, loss_scale=4, train_wall=23, gb_free=11.3, wall=31112
2021-05-01 20:53:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:53:51 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:54:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:54:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:54:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:54:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:54:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:54:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:54:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:54:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:54:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:54:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:54:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:54:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:54:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:54:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:54:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:54:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.701 | ppl 26.01 | bleu 29.19 | wps 2455.5 | wpb 2024.1 | bsz 97.5 | num_updates 102000 | best_bleu 29.19
2021-05-01 20:54:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 102000 updates
2021-05-01 20:54:56 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_102000.pt
2021-05-01 20:54:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_102000.pt
2021-05-01 20:55:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_102000.pt (epoch 2 @ 102000 updates, score 29.19) (writing took 14.351493659996777 seconds)
2021-05-01 20:55:33 | INFO | train_inner | epoch 002:  41689 / 60421 loss=1.139, ppl=2.2, wps=3672.5, ups=0.98, wpb=3733.7, bsz=137, num_updates=102100, lr=9.89663e-05, gnorm=1.643, loss_scale=4, train_wall=22, gb_free=10.8, wall=31214
2021-05-01 20:55:55 | INFO | train_inner | epoch 002:  41789 / 60421 loss=1.099, ppl=2.14, wps=16210.2, ups=4.43, wpb=3660.6, bsz=118.4, num_updates=102200, lr=9.89178e-05, gnorm=1.942, loss_scale=4, train_wall=22, gb_free=10.7, wall=31236
2021-05-01 20:56:19 | INFO | train_inner | epoch 002:  41889 / 60421 loss=1.054, ppl=2.08, wps=16136.8, ups=4.32, wpb=3739.5, bsz=132.2, num_updates=102300, lr=9.88695e-05, gnorm=1.443, loss_scale=4, train_wall=23, gb_free=10.8, wall=31260
2021-05-01 20:56:42 | INFO | train_inner | epoch 002:  41989 / 60421 loss=1.053, ppl=2.07, wps=16060, ups=4.24, wpb=3788.2, bsz=135.1, num_updates=102400, lr=9.88212e-05, gnorm=1.129, loss_scale=4, train_wall=23, gb_free=10.8, wall=31283
2021-05-01 20:57:05 | INFO | train_inner | epoch 002:  42089 / 60421 loss=1.074, ppl=2.1, wps=16290.1, ups=4.31, wpb=3777.8, bsz=134.9, num_updates=102500, lr=9.8773e-05, gnorm=1.301, loss_scale=4, train_wall=23, gb_free=10.7, wall=31306
2021-05-01 20:57:28 | INFO | train_inner | epoch 002:  42189 / 60421 loss=1.048, ppl=2.07, wps=16528.8, ups=4.39, wpb=3761.8, bsz=139, num_updates=102600, lr=9.87248e-05, gnorm=1.211, loss_scale=4, train_wall=23, gb_free=10.7, wall=31329
2021-05-01 20:57:51 | INFO | train_inner | epoch 002:  42289 / 60421 loss=1.036, ppl=2.05, wps=16630.2, ups=4.37, wpb=3809.1, bsz=131, num_updates=102700, lr=9.86767e-05, gnorm=1.19, loss_scale=4, train_wall=23, gb_free=10.9, wall=31352
2021-05-01 20:58:14 | INFO | train_inner | epoch 002:  42389 / 60421 loss=1.031, ppl=2.04, wps=16577.2, ups=4.38, wpb=3783.2, bsz=124.5, num_updates=102800, lr=9.86287e-05, gnorm=1.18, loss_scale=4, train_wall=23, gb_free=10.7, wall=31375
2021-05-01 20:58:37 | INFO | train_inner | epoch 002:  42489 / 60421 loss=1.03, ppl=2.04, wps=16598.5, ups=4.4, wpb=3773.8, bsz=137, num_updates=102900, lr=9.85808e-05, gnorm=1.317, loss_scale=4, train_wall=23, gb_free=10.7, wall=31398
2021-05-01 20:58:59 | INFO | train_inner | epoch 002:  42589 / 60421 loss=1.014, ppl=2.02, wps=16575.5, ups=4.42, wpb=3750.5, bsz=146.9, num_updates=103000, lr=9.85329e-05, gnorm=1.285, loss_scale=4, train_wall=22, gb_free=11, wall=31420
2021-05-01 20:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 20:58:59 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 20:59:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:59:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:59:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:59:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:59:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:59:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:59:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:59:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:59:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:59:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:59:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:59:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 20:59:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 20:59:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 20:59:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:00:05 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.738 | ppl 26.68 | bleu 28.91 | wps 2429.4 | wpb 2024.1 | bsz 97.5 | num_updates 103000 | best_bleu 29.19
2021-05-01 21:00:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 103000 updates
2021-05-01 21:00:05 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_103000.pt
2021-05-01 21:00:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_103000.pt
2021-05-01 21:00:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_103000.pt (epoch 2 @ 103000 updates, score 28.91) (writing took 7.746075520997692 seconds)
2021-05-01 21:00:35 | INFO | train_inner | epoch 002:  42689 / 60421 loss=1.072, ppl=2.1, wps=3821.2, ups=1.04, wpb=3657.8, bsz=118.6, num_updates=103100, lr=9.84851e-05, gnorm=1.724, loss_scale=4, train_wall=22, gb_free=10.8, wall=31516
2021-05-01 21:00:58 | INFO | train_inner | epoch 002:  42789 / 60421 loss=1.011, ppl=2.01, wps=16215.5, ups=4.37, wpb=3714.7, bsz=147.6, num_updates=103200, lr=9.84374e-05, gnorm=1.621, loss_scale=4, train_wall=23, gb_free=10.8, wall=31539
2021-05-01 21:01:21 | INFO | train_inner | epoch 002:  42889 / 60421 loss=1.007, ppl=2.01, wps=16028.5, ups=4.33, wpb=3705.8, bsz=127.4, num_updates=103300, lr=9.83897e-05, gnorm=1.329, loss_scale=4, train_wall=23, gb_free=10.7, wall=31562
2021-05-01 21:01:44 | INFO | train_inner | epoch 002:  42989 / 60421 loss=1.065, ppl=2.09, wps=15779.1, ups=4.27, wpb=3698, bsz=139.8, num_updates=103400, lr=9.83422e-05, gnorm=1.477, loss_scale=4, train_wall=23, gb_free=10.7, wall=31585
2021-05-01 21:02:08 | INFO | train_inner | epoch 002:  43089 / 60421 loss=1.035, ppl=2.05, wps=16198.6, ups=4.29, wpb=3778.8, bsz=132.5, num_updates=103500, lr=9.82946e-05, gnorm=1.196, loss_scale=4, train_wall=23, gb_free=11.1, wall=31609
2021-05-01 21:02:31 | INFO | train_inner | epoch 002:  43189 / 60421 loss=1.034, ppl=2.05, wps=16312, ups=4.38, wpb=3728, bsz=144.3, num_updates=103600, lr=9.82472e-05, gnorm=1.471, loss_scale=4, train_wall=23, gb_free=10.8, wall=31632
2021-05-01 21:02:54 | INFO | train_inner | epoch 002:  43289 / 60421 loss=0.988, ppl=1.98, wps=16557.9, ups=4.36, wpb=3800.9, bsz=138.6, num_updates=103700, lr=9.81998e-05, gnorm=1.196, loss_scale=4, train_wall=23, gb_free=10.9, wall=31655
2021-05-01 21:03:16 | INFO | train_inner | epoch 002:  43389 / 60421 loss=1.02, ppl=2.03, wps=16475.8, ups=4.42, wpb=3731.2, bsz=129, num_updates=103800, lr=9.81525e-05, gnorm=1.342, loss_scale=4, train_wall=22, gb_free=10.7, wall=31677
2021-05-01 21:03:39 | INFO | train_inner | epoch 002:  43489 / 60421 loss=1.014, ppl=2.02, wps=16498.7, ups=4.42, wpb=3733.3, bsz=131.4, num_updates=103900, lr=9.81052e-05, gnorm=1.272, loss_scale=4, train_wall=22, gb_free=11.3, wall=31700
2021-05-01 21:04:01 | INFO | train_inner | epoch 002:  43589 / 60421 loss=1.122, ppl=2.18, wps=16392, ups=4.5, wpb=3638.9, bsz=116.8, num_updates=104000, lr=9.80581e-05, gnorm=1.913, loss_scale=4, train_wall=22, gb_free=10.8, wall=31722
2021-05-01 21:04:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:04:01 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:04:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:04:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:04:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:04:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:04:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:04:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:04:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:04:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:04:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:04:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:04:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:04:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:04:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:04:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:04:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:05:07 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.714 | ppl 26.25 | bleu 28.28 | wps 2390.2 | wpb 2024.1 | bsz 97.5 | num_updates 104000 | best_bleu 29.19
2021-05-01 21:05:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 104000 updates
2021-05-01 21:05:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_104000.pt
2021-05-01 21:05:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_104000.pt
2021-05-01 21:05:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_104000.pt (epoch 2 @ 104000 updates, score 28.28) (writing took 7.739239688002272 seconds)
2021-05-01 21:05:38 | INFO | train_inner | epoch 002:  43689 / 60421 loss=1.109, ppl=2.16, wps=3871.6, ups=1.03, wpb=3767.4, bsz=144.1, num_updates=104100, lr=9.8011e-05, gnorm=1.677, loss_scale=4, train_wall=23, gb_free=10.9, wall=31819
2021-05-01 21:06:01 | INFO | train_inner | epoch 002:  43789 / 60421 loss=1.007, ppl=2.01, wps=16039.6, ups=4.4, wpb=3642, bsz=141.8, num_updates=104200, lr=9.79639e-05, gnorm=1.5, loss_scale=4, train_wall=23, gb_free=11.3, wall=31842
2021-05-01 21:06:24 | INFO | train_inner | epoch 002:  43889 / 60421 loss=1.04, ppl=2.06, wps=16103.3, ups=4.28, wpb=3761.8, bsz=139.8, num_updates=104300, lr=9.79169e-05, gnorm=1.399, loss_scale=4, train_wall=23, gb_free=10.9, wall=31865
2021-05-01 21:06:47 | INFO | train_inner | epoch 002:  43989 / 60421 loss=1.073, ppl=2.1, wps=15787.7, ups=4.34, wpb=3638, bsz=121.1, num_updates=104400, lr=9.787e-05, gnorm=1.536, loss_scale=4, train_wall=23, gb_free=11, wall=31888
2021-05-01 21:07:11 | INFO | train_inner | epoch 002:  44089 / 60421 loss=1.156, ppl=2.23, wps=16339.6, ups=4.34, wpb=3764.8, bsz=117.9, num_updates=104500, lr=9.78232e-05, gnorm=1.427, loss_scale=4, train_wall=23, gb_free=10.8, wall=31911
2021-05-01 21:07:33 | INFO | train_inner | epoch 002:  44189 / 60421 loss=1.09, ppl=2.13, wps=16493, ups=4.4, wpb=3748.5, bsz=128.9, num_updates=104600, lr=9.77764e-05, gnorm=1.38, loss_scale=4, train_wall=23, gb_free=10.7, wall=31934
2021-05-01 21:07:56 | INFO | train_inner | epoch 002:  44289 / 60421 loss=1.048, ppl=2.07, wps=16614.3, ups=4.44, wpb=3742.2, bsz=119.6, num_updates=104700, lr=9.77297e-05, gnorm=1.077, loss_scale=4, train_wall=22, gb_free=11.1, wall=31957
2021-05-01 21:08:18 | INFO | train_inner | epoch 002:  44389 / 60421 loss=1.063, ppl=2.09, wps=16764.5, ups=4.43, wpb=3786.9, bsz=134.4, num_updates=104800, lr=9.76831e-05, gnorm=1.31, loss_scale=4, train_wall=22, gb_free=10.9, wall=31979
2021-05-01 21:08:41 | INFO | train_inner | epoch 002:  44489 / 60421 loss=1.077, ppl=2.11, wps=16458.8, ups=4.46, wpb=3691.3, bsz=122, num_updates=104900, lr=9.76365e-05, gnorm=1.538, loss_scale=4, train_wall=22, gb_free=11.3, wall=32002
2021-05-01 21:09:03 | INFO | train_inner | epoch 002:  44589 / 60421 loss=1.031, ppl=2.04, wps=16412.1, ups=4.44, wpb=3699.4, bsz=144.4, num_updates=105000, lr=9.759e-05, gnorm=1.558, loss_scale=4, train_wall=22, gb_free=11.3, wall=32024
2021-05-01 21:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:09:03 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:09:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:09:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:09:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:09:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:09:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:09:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:09:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:09:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:09:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:10:10 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.656 | ppl 25.2 | bleu 28.56 | wps 2394.2 | wpb 2024.1 | bsz 97.5 | num_updates 105000 | best_bleu 29.19
2021-05-01 21:10:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 105000 updates
2021-05-01 21:10:10 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_105000.pt
2021-05-01 21:10:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_105000.pt
2021-05-01 21:10:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_105000.pt (epoch 2 @ 105000 updates, score 28.56) (writing took 7.718223595999007 seconds)
2021-05-01 21:10:40 | INFO | train_inner | epoch 002:  44689 / 60421 loss=1.045, ppl=2.06, wps=3818.8, ups=1.03, wpb=3694.9, bsz=137.8, num_updates=105100, lr=9.75436e-05, gnorm=1.487, loss_scale=4, train_wall=23, gb_free=10.8, wall=32121
2021-05-01 21:11:03 | INFO | train_inner | epoch 002:  44789 / 60421 loss=1.002, ppl=2, wps=16267.3, ups=4.35, wpb=3738.9, bsz=139.8, num_updates=105200, lr=9.74972e-05, gnorm=1.137, loss_scale=4, train_wall=23, gb_free=10.7, wall=32144
2021-05-01 21:11:26 | INFO | train_inner | epoch 002:  44889 / 60421 loss=1.069, ppl=2.1, wps=16140.3, ups=4.3, wpb=3757.3, bsz=123.4, num_updates=105300, lr=9.74509e-05, gnorm=1.311, loss_scale=4, train_wall=23, gb_free=10.8, wall=32167
2021-05-01 21:11:50 | INFO | train_inner | epoch 002:  44989 / 60421 loss=1.012, ppl=2.02, wps=15782, ups=4.27, wpb=3693.5, bsz=132.2, num_updates=105400, lr=9.74047e-05, gnorm=1.339, loss_scale=4, train_wall=23, gb_free=11, wall=32191
2021-05-01 21:12:13 | INFO | train_inner | epoch 002:  45089 / 60421 loss=1.005, ppl=2.01, wps=16252.4, ups=4.39, wpb=3704, bsz=140.6, num_updates=105500, lr=9.73585e-05, gnorm=1.082, loss_scale=4, train_wall=23, gb_free=10.8, wall=32214
2021-05-01 21:12:35 | INFO | train_inner | epoch 002:  45189 / 60421 loss=1.058, ppl=2.08, wps=16489.1, ups=4.41, wpb=3739.8, bsz=125.4, num_updates=105600, lr=9.73124e-05, gnorm=1.374, loss_scale=4, train_wall=23, gb_free=11.1, wall=32236
2021-05-01 21:12:58 | INFO | train_inner | epoch 002:  45289 / 60421 loss=1.018, ppl=2.03, wps=16594.5, ups=4.4, wpb=3775.2, bsz=139.8, num_updates=105700, lr=9.72663e-05, gnorm=1.249, loss_scale=4, train_wall=23, gb_free=10.9, wall=32259
2021-05-01 21:13:21 | INFO | train_inner | epoch 002:  45389 / 60421 loss=1.04, ppl=2.06, wps=16668.9, ups=4.41, wpb=3782, bsz=113.4, num_updates=105800, lr=9.72203e-05, gnorm=1.193, loss_scale=4, train_wall=23, gb_free=10.9, wall=32282
2021-05-01 21:13:43 | INFO | train_inner | epoch 002:  45489 / 60421 loss=1.021, ppl=2.03, wps=16662.5, ups=4.41, wpb=3779.1, bsz=148.6, num_updates=105900, lr=9.71744e-05, gnorm=1.277, loss_scale=4, train_wall=23, gb_free=10.7, wall=32304
2021-05-01 21:14:06 | INFO | train_inner | epoch 002:  45589 / 60421 loss=0.976, ppl=1.97, wps=16261.5, ups=4.42, wpb=3679.6, bsz=145.9, num_updates=106000, lr=9.71286e-05, gnorm=1.589, loss_scale=4, train_wall=22, gb_free=10.9, wall=32327
2021-05-01 21:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:14:06 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:14:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:14:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:14:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:14:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:14:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:14:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:14:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:14:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:14:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:14:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:14:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:14:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:14:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:14:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:14:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:14:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:14:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:14:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:14:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:14:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:14:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:15:11 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.794 | ppl 27.74 | bleu 28.01 | wps 2454 | wpb 2024.1 | bsz 97.5 | num_updates 106000 | best_bleu 29.19
2021-05-01 21:15:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 106000 updates
2021-05-01 21:15:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_106000.pt
2021-05-01 21:15:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_106000.pt
2021-05-01 21:15:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_106000.pt (epoch 2 @ 106000 updates, score 28.01) (writing took 7.779471093999746 seconds)
2021-05-01 21:15:41 | INFO | train_inner | epoch 002:  45689 / 60421 loss=1.023, ppl=2.03, wps=3892.9, ups=1.05, wpb=3707.6, bsz=125, num_updates=106100, lr=9.70828e-05, gnorm=1.259, loss_scale=4, train_wall=23, gb_free=10.8, wall=32422
2021-05-01 21:16:04 | INFO | train_inner | epoch 002:  45789 / 60421 loss=1.057, ppl=2.08, wps=16067.7, ups=4.34, wpb=3703.6, bsz=137.9, num_updates=106200, lr=9.70371e-05, gnorm=1.568, loss_scale=4, train_wall=23, gb_free=10.8, wall=32445
2021-05-01 21:16:28 | INFO | train_inner | epoch 002:  45889 / 60421 loss=1.051, ppl=2.07, wps=15972, ups=4.3, wpb=3718.7, bsz=122.2, num_updates=106300, lr=9.69914e-05, gnorm=1.424, loss_scale=4, train_wall=23, gb_free=10.9, wall=32469
2021-05-01 21:16:51 | INFO | train_inner | epoch 002:  45989 / 60421 loss=1.093, ppl=2.13, wps=15748.8, ups=4.28, wpb=3683.6, bsz=129, num_updates=106400, lr=9.69458e-05, gnorm=1.591, loss_scale=4, train_wall=23, gb_free=11.2, wall=32492
2021-05-01 21:17:14 | INFO | train_inner | epoch 002:  46089 / 60421 loss=1.045, ppl=2.06, wps=16222.9, ups=4.35, wpb=3731.2, bsz=142.5, num_updates=106500, lr=9.69003e-05, gnorm=1.446, loss_scale=4, train_wall=23, gb_free=10.8, wall=32515
2021-05-01 21:17:36 | INFO | train_inner | epoch 002:  46189 / 60421 loss=1.113, ppl=2.16, wps=16448.8, ups=4.44, wpb=3706.7, bsz=107.7, num_updates=106600, lr=9.68549e-05, gnorm=1.639, loss_scale=4, train_wall=22, gb_free=10.9, wall=32537
2021-05-01 21:17:59 | INFO | train_inner | epoch 002:  46289 / 60421 loss=1.046, ppl=2.07, wps=16279.4, ups=4.44, wpb=3665.5, bsz=123.4, num_updates=106700, lr=9.68095e-05, gnorm=1.409, loss_scale=4, train_wall=22, gb_free=10.9, wall=32560
2021-05-01 21:18:22 | INFO | train_inner | epoch 002:  46389 / 60421 loss=1.02, ppl=2.03, wps=16500.6, ups=4.42, wpb=3729.3, bsz=124.7, num_updates=106800, lr=9.67641e-05, gnorm=1.12, loss_scale=4, train_wall=22, gb_free=10.9, wall=32583
2021-05-01 21:18:44 | INFO | train_inner | epoch 002:  46489 / 60421 loss=1.039, ppl=2.06, wps=16288, ups=4.49, wpb=3624.3, bsz=136, num_updates=106900, lr=9.67189e-05, gnorm=1.769, loss_scale=4, train_wall=22, gb_free=10.7, wall=32605
2021-05-01 21:19:07 | INFO | train_inner | epoch 002:  46589 / 60421 loss=1.128, ppl=2.19, wps=16609.2, ups=4.36, wpb=3811.2, bsz=115.4, num_updates=107000, lr=9.66736e-05, gnorm=1.426, loss_scale=4, train_wall=23, gb_free=10.7, wall=32628
2021-05-01 21:19:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:19:07 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:19:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:19:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:19:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:19:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:19:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:19:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:19:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:19:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:19:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:19:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:19:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:19:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:19:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:19:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:19:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:20:12 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.79 | ppl 27.66 | bleu 29.02 | wps 2437 | wpb 2024.1 | bsz 97.5 | num_updates 107000 | best_bleu 29.19
2021-05-01 21:20:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 107000 updates
2021-05-01 21:20:12 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_107000.pt
2021-05-01 21:20:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_107000.pt
2021-05-01 21:20:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_107000.pt (epoch 2 @ 107000 updates, score 29.02) (writing took 7.722912876000919 seconds)
2021-05-01 21:20:43 | INFO | train_inner | epoch 002:  46689 / 60421 loss=1.036, ppl=2.05, wps=3938, ups=1.04, wpb=3779.1, bsz=140, num_updates=107100, lr=9.66285e-05, gnorm=1.307, loss_scale=4, train_wall=23, gb_free=10.8, wall=32724
2021-05-01 21:21:06 | INFO | train_inner | epoch 002:  46789 / 60421 loss=1.03, ppl=2.04, wps=16035.7, ups=4.36, wpb=3675.2, bsz=130.6, num_updates=107200, lr=9.65834e-05, gnorm=1.529, loss_scale=4, train_wall=23, gb_free=11.1, wall=32747
2021-05-01 21:21:29 | INFO | train_inner | epoch 002:  46889 / 60421 loss=0.989, ppl=1.98, wps=15700.1, ups=4.29, wpb=3655.4, bsz=144.8, num_updates=107300, lr=9.65384e-05, gnorm=1.478, loss_scale=4, train_wall=23, gb_free=11.1, wall=32770
2021-05-01 21:21:53 | INFO | train_inner | epoch 002:  46989 / 60421 loss=1.07, ppl=2.1, wps=15932.2, ups=4.21, wpb=3783.6, bsz=130, num_updates=107400, lr=9.64935e-05, gnorm=1.386, loss_scale=4, train_wall=24, gb_free=10.8, wall=32794
2021-05-01 21:22:16 | INFO | train_inner | epoch 002:  47089 / 60421 loss=1.079, ppl=2.11, wps=16230, ups=4.36, wpb=3726.4, bsz=120.2, num_updates=107500, lr=9.64486e-05, gnorm=1.55, loss_scale=4, train_wall=23, gb_free=11, wall=32817
2021-05-01 21:22:38 | INFO | train_inner | epoch 002:  47189 / 60421 loss=1.006, ppl=2.01, wps=16558.9, ups=4.4, wpb=3766.8, bsz=136.3, num_updates=107600, lr=9.64037e-05, gnorm=1.241, loss_scale=4, train_wall=23, gb_free=10.8, wall=32839
2021-05-01 21:23:01 | INFO | train_inner | epoch 002:  47289 / 60421 loss=1.031, ppl=2.04, wps=16513.1, ups=4.42, wpb=3735.2, bsz=124.7, num_updates=107700, lr=9.6359e-05, gnorm=1.349, loss_scale=4, train_wall=22, gb_free=11.3, wall=32862
2021-05-01 21:23:24 | INFO | train_inner | epoch 002:  47389 / 60421 loss=1.079, ppl=2.11, wps=16696.8, ups=4.43, wpb=3772.9, bsz=119.8, num_updates=107800, lr=9.63143e-05, gnorm=1.251, loss_scale=4, train_wall=22, gb_free=11.5, wall=32885
2021-05-01 21:23:46 | INFO | train_inner | epoch 002:  47489 / 60421 loss=1.013, ppl=2.02, wps=16553.5, ups=4.43, wpb=3733.6, bsz=155.3, num_updates=107900, lr=9.62696e-05, gnorm=1.412, loss_scale=4, train_wall=22, gb_free=10.8, wall=32907
2021-05-01 21:24:09 | INFO | train_inner | epoch 002:  47589 / 60421 loss=0.992, ppl=1.99, wps=16512.4, ups=4.41, wpb=3748, bsz=128.1, num_updates=108000, lr=9.6225e-05, gnorm=1.159, loss_scale=4, train_wall=23, gb_free=10.9, wall=32930
2021-05-01 21:24:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:24:09 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:24:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:24:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:24:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:24:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:24:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:24:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:24:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:24:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:24:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:24:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:24:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:24:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:24:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:24:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:24:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:25:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.768 | ppl 27.26 | bleu 29.02 | wps 2424.5 | wpb 2024.1 | bsz 97.5 | num_updates 108000 | best_bleu 29.19
2021-05-01 21:25:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 108000 updates
2021-05-01 21:25:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_108000.pt
2021-05-01 21:25:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_108000.pt
2021-05-01 21:25:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_108000.pt (epoch 2 @ 108000 updates, score 29.02) (writing took 7.829613308997068 seconds)
2021-05-01 21:25:45 | INFO | train_inner | epoch 002:  47689 / 60421 loss=1.069, ppl=2.1, wps=3924.1, ups=1.04, wpb=3764.4, bsz=131.6, num_updates=108100, lr=9.61805e-05, gnorm=1.294, loss_scale=4, train_wall=22, gb_free=10.8, wall=33026
2021-05-01 21:26:07 | INFO | train_inner | epoch 002:  47789 / 60421 loss=0.999, ppl=2, wps=16561.6, ups=4.48, wpb=3693.8, bsz=152.8, num_updates=108200, lr=9.61361e-05, gnorm=1.462, loss_scale=4, train_wall=22, gb_free=10.8, wall=33048
2021-05-01 21:26:30 | INFO | train_inner | epoch 002:  47889 / 60421 loss=1.047, ppl=2.07, wps=16470.7, ups=4.46, wpb=3690.7, bsz=137.9, num_updates=108300, lr=9.60917e-05, gnorm=1.493, loss_scale=4, train_wall=22, gb_free=10.8, wall=33070
2021-05-01 21:26:52 | INFO | train_inner | epoch 002:  47989 / 60421 loss=1.079, ppl=2.11, wps=16628.9, ups=4.46, wpb=3730.8, bsz=132.1, num_updates=108400, lr=9.60473e-05, gnorm=1.516, loss_scale=8, train_wall=22, gb_free=11.3, wall=33093
2021-05-01 21:27:15 | INFO | train_inner | epoch 002:  48089 / 60421 loss=1.001, ppl=2, wps=16677.5, ups=4.42, wpb=3776, bsz=146.1, num_updates=108500, lr=9.60031e-05, gnorm=1.207, loss_scale=8, train_wall=22, gb_free=11, wall=33116
2021-05-01 21:27:37 | INFO | train_inner | epoch 002:  48189 / 60421 loss=1.014, ppl=2.02, wps=16583.8, ups=4.44, wpb=3731.1, bsz=129, num_updates=108600, lr=9.59589e-05, gnorm=1.089, loss_scale=8, train_wall=22, gb_free=10.9, wall=33138
2021-05-01 21:28:00 | INFO | train_inner | epoch 002:  48289 / 60421 loss=1.047, ppl=2.07, wps=16468.2, ups=4.39, wpb=3752.9, bsz=131, num_updates=108700, lr=9.59147e-05, gnorm=1.321, loss_scale=8, train_wall=23, gb_free=10.8, wall=33161
2021-05-01 21:28:22 | INFO | train_inner | epoch 002:  48389 / 60421 loss=0.978, ppl=1.97, wps=16457, ups=4.44, wpb=3708.4, bsz=134.7, num_updates=108800, lr=9.58706e-05, gnorm=1.327, loss_scale=8, train_wall=22, gb_free=10.9, wall=33183
2021-05-01 21:28:45 | INFO | train_inner | epoch 002:  48489 / 60421 loss=1.059, ppl=2.08, wps=16424.8, ups=4.41, wpb=3723.3, bsz=123.4, num_updates=108900, lr=9.58266e-05, gnorm=1.308, loss_scale=8, train_wall=22, gb_free=11.1, wall=33206
2021-05-01 21:29:08 | INFO | train_inner | epoch 002:  48589 / 60421 loss=0.929, ppl=1.9, wps=16399.5, ups=4.4, wpb=3729.1, bsz=147.3, num_updates=109000, lr=9.57826e-05, gnorm=1.232, loss_scale=8, train_wall=23, gb_free=10.4, wall=33229
2021-05-01 21:29:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:29:08 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:29:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:29:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:29:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:29:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:29:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:29:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:29:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:29:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:29:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:29:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:29:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:29:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:30:13 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.76 | ppl 27.09 | bleu 28.7 | wps 2418.9 | wpb 2024.1 | bsz 97.5 | num_updates 109000 | best_bleu 29.19
2021-05-01 21:30:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 109000 updates
2021-05-01 21:30:13 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_109000.pt
2021-05-01 21:30:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_109000.pt
2021-05-01 21:30:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_109000.pt (epoch 2 @ 109000 updates, score 28.7) (writing took 7.8260525340010645 seconds)
2021-05-01 21:30:44 | INFO | train_inner | epoch 002:  48689 / 60421 loss=1.03, ppl=2.04, wps=3881.1, ups=1.04, wpb=3748.6, bsz=140.2, num_updates=109100, lr=9.57387e-05, gnorm=1.305, loss_scale=8, train_wall=23, gb_free=10.8, wall=33325
2021-05-01 21:31:08 | INFO | train_inner | epoch 002:  48789 / 60421 loss=0.997, ppl=2, wps=16091.7, ups=4.33, wpb=3715.3, bsz=145, num_updates=109200, lr=9.56949e-05, gnorm=1.308, loss_scale=8, train_wall=23, gb_free=11.3, wall=33348
2021-05-01 21:31:31 | INFO | train_inner | epoch 002:  48889 / 60421 loss=1.061, ppl=2.09, wps=16154.3, ups=4.27, wpb=3783.7, bsz=114.9, num_updates=109300, lr=9.56511e-05, gnorm=1.182, loss_scale=8, train_wall=23, gb_free=10.9, wall=33372
2021-05-01 21:31:55 | INFO | train_inner | epoch 002:  48989 / 60421 loss=1.066, ppl=2.09, wps=15981.6, ups=4.21, wpb=3792.9, bsz=117.4, num_updates=109400, lr=9.56074e-05, gnorm=1.348, loss_scale=8, train_wall=24, gb_free=11.1, wall=33396
2021-05-01 21:32:18 | INFO | train_inner | epoch 002:  49089 / 60421 loss=0.956, ppl=1.94, wps=16138.4, ups=4.33, wpb=3729.7, bsz=149.5, num_updates=109500, lr=9.55637e-05, gnorm=1.09, loss_scale=8, train_wall=23, gb_free=10.8, wall=33419
2021-05-01 21:32:41 | INFO | train_inner | epoch 002:  49189 / 60421 loss=0.994, ppl=1.99, wps=16400.8, ups=4.38, wpb=3746.8, bsz=132.7, num_updates=109600, lr=9.55201e-05, gnorm=1.473, loss_scale=8, train_wall=23, gb_free=10.7, wall=33442
2021-05-01 21:33:03 | INFO | train_inner | epoch 002:  49289 / 60421 loss=1.061, ppl=2.09, wps=16475.8, ups=4.37, wpb=3768.9, bsz=130.5, num_updates=109700, lr=9.54765e-05, gnorm=1.219, loss_scale=8, train_wall=23, gb_free=10.6, wall=33464
2021-05-01 21:33:26 | INFO | train_inner | epoch 002:  49389 / 60421 loss=1.052, ppl=2.07, wps=16421.5, ups=4.43, wpb=3710.3, bsz=124.6, num_updates=109800, lr=9.54331e-05, gnorm=1.419, loss_scale=8, train_wall=22, gb_free=10.7, wall=33487
2021-05-01 21:33:49 | INFO | train_inner | epoch 002:  49489 / 60421 loss=1.034, ppl=2.05, wps=16323.3, ups=4.41, wpb=3699.8, bsz=134.4, num_updates=109900, lr=9.53896e-05, gnorm=1.511, loss_scale=8, train_wall=22, gb_free=10.9, wall=33510
2021-05-01 21:34:12 | INFO | train_inner | epoch 002:  49589 / 60421 loss=0.987, ppl=1.98, wps=16332.5, ups=4.39, wpb=3720.5, bsz=143.7, num_updates=110000, lr=9.53463e-05, gnorm=1.276, loss_scale=8, train_wall=23, gb_free=10.9, wall=33533
2021-05-01 21:34:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:34:12 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:34:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:34:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:34:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:34:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:34:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:34:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:34:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:34:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:34:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:34:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:34:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:34:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:35:18 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.751 | ppl 26.92 | bleu 28.9 | wps 2384.1 | wpb 2024.1 | bsz 97.5 | num_updates 110000 | best_bleu 29.19
2021-05-01 21:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 110000 updates
2021-05-01 21:35:18 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_110000.pt
2021-05-01 21:35:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_110000.pt
2021-05-01 21:35:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_110000.pt (epoch 2 @ 110000 updates, score 28.9) (writing took 7.72191964200465 seconds)
2021-05-01 21:35:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-01 21:35:49 | INFO | train_inner | epoch 002:  49690 / 60421 loss=1.013, ppl=2.02, wps=3881.8, ups=1.02, wpb=3793.6, bsz=139.3, num_updates=110100, lr=9.53029e-05, gnorm=1.165, loss_scale=4, train_wall=23, gb_free=10.9, wall=33630
2021-05-01 21:36:13 | INFO | train_inner | epoch 002:  49790 / 60421 loss=1.03, ppl=2.04, wps=16243.3, ups=4.29, wpb=3786.5, bsz=124, num_updates=110200, lr=9.52597e-05, gnorm=1.16, loss_scale=4, train_wall=23, gb_free=10.6, wall=33654
2021-05-01 21:36:36 | INFO | train_inner | epoch 002:  49890 / 60421 loss=1.1, ppl=2.14, wps=16012.2, ups=4.24, wpb=3774.5, bsz=131, num_updates=110300, lr=9.52165e-05, gnorm=1.414, loss_scale=4, train_wall=23, gb_free=10.8, wall=33677
2021-05-01 21:37:00 | INFO | train_inner | epoch 002:  49990 / 60421 loss=1.052, ppl=2.07, wps=15963.2, ups=4.25, wpb=3752.6, bsz=129.4, num_updates=110400, lr=9.51734e-05, gnorm=1.428, loss_scale=4, train_wall=23, gb_free=10.7, wall=33701
2021-05-01 21:37:23 | INFO | train_inner | epoch 002:  50090 / 60421 loss=1.012, ppl=2.02, wps=16309.7, ups=4.37, wpb=3730.4, bsz=133.9, num_updates=110500, lr=9.51303e-05, gnorm=1.249, loss_scale=4, train_wall=23, gb_free=10.9, wall=33723
2021-05-01 21:37:45 | INFO | train_inner | epoch 002:  50190 / 60421 loss=1.038, ppl=2.05, wps=16492.8, ups=4.41, wpb=3738.7, bsz=131.1, num_updates=110600, lr=9.50873e-05, gnorm=1.086, loss_scale=4, train_wall=22, gb_free=11.1, wall=33746
2021-05-01 21:38:08 | INFO | train_inner | epoch 002:  50290 / 60421 loss=1.012, ppl=2.02, wps=16556.5, ups=4.36, wpb=3799, bsz=133.1, num_updates=110700, lr=9.50443e-05, gnorm=1.084, loss_scale=4, train_wall=23, gb_free=10.7, wall=33769
2021-05-01 21:38:31 | INFO | train_inner | epoch 002:  50390 / 60421 loss=1.098, ppl=2.14, wps=16494, ups=4.41, wpb=3737.1, bsz=114.9, num_updates=110800, lr=9.50014e-05, gnorm=1.488, loss_scale=4, train_wall=22, gb_free=10.7, wall=33792
2021-05-01 21:38:53 | INFO | train_inner | epoch 002:  50490 / 60421 loss=1.041, ppl=2.06, wps=16338.8, ups=4.41, wpb=3701.8, bsz=117.5, num_updates=110900, lr=9.49586e-05, gnorm=1.247, loss_scale=4, train_wall=22, gb_free=10.7, wall=33814
2021-05-01 21:39:16 | INFO | train_inner | epoch 002:  50590 / 60421 loss=1.083, ppl=2.12, wps=16484.3, ups=4.4, wpb=3750.4, bsz=124.2, num_updates=111000, lr=9.49158e-05, gnorm=1.273, loss_scale=4, train_wall=23, gb_free=11, wall=33837
2021-05-01 21:39:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:39:16 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:39:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:39:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:39:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:39:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:39:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:39:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:39:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:39:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:39:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:39:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:39:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:39:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:40:23 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.775 | ppl 27.38 | bleu 29.06 | wps 2381.5 | wpb 2024.1 | bsz 97.5 | num_updates 111000 | best_bleu 29.19
2021-05-01 21:40:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 111000 updates
2021-05-01 21:40:23 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_111000.pt
2021-05-01 21:40:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_111000.pt
2021-05-01 21:40:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_111000.pt (epoch 2 @ 111000 updates, score 29.06) (writing took 7.804787004999525 seconds)
2021-05-01 21:40:54 | INFO | train_inner | epoch 002:  50690 / 60421 loss=1.058, ppl=2.08, wps=3781.9, ups=1.03, wpb=3682.5, bsz=126.2, num_updates=111100, lr=9.48731e-05, gnorm=1.634, loss_scale=4, train_wall=23, gb_free=10.7, wall=33935
2021-05-01 21:41:17 | INFO | train_inner | epoch 002:  50790 / 60421 loss=1.076, ppl=2.11, wps=16129.1, ups=4.3, wpb=3748.1, bsz=124.2, num_updates=111200, lr=9.48304e-05, gnorm=1.463, loss_scale=4, train_wall=23, gb_free=10.9, wall=33958
2021-05-01 21:41:41 | INFO | train_inner | epoch 002:  50890 / 60421 loss=1.016, ppl=2.02, wps=15788.4, ups=4.2, wpb=3762.5, bsz=144.5, num_updates=111300, lr=9.47878e-05, gnorm=1.351, loss_scale=4, train_wall=24, gb_free=10.8, wall=33982
2021-05-01 21:42:04 | INFO | train_inner | epoch 002:  50990 / 60421 loss=0.994, ppl=1.99, wps=16085.2, ups=4.27, wpb=3762.7, bsz=141, num_updates=111400, lr=9.47452e-05, gnorm=1.261, loss_scale=4, train_wall=23, gb_free=11, wall=34005
2021-05-01 21:42:27 | INFO | train_inner | epoch 002:  51090 / 60421 loss=0.975, ppl=1.97, wps=16201.5, ups=4.35, wpb=3721.3, bsz=138.2, num_updates=111500, lr=9.47027e-05, gnorm=1.366, loss_scale=4, train_wall=23, gb_free=10.7, wall=34028
2021-05-01 21:42:50 | INFO | train_inner | epoch 002:  51190 / 60421 loss=0.99, ppl=1.99, wps=16414.7, ups=4.39, wpb=3737.2, bsz=154.8, num_updates=111600, lr=9.46603e-05, gnorm=1.234, loss_scale=4, train_wall=23, gb_free=10.7, wall=34051
2021-05-01 21:43:12 | INFO | train_inner | epoch 002:  51290 / 60421 loss=1.04, ppl=2.06, wps=16311, ups=4.42, wpb=3687, bsz=128.6, num_updates=111700, lr=9.46179e-05, gnorm=1.503, loss_scale=4, train_wall=22, gb_free=10.7, wall=34073
2021-05-01 21:43:35 | INFO | train_inner | epoch 002:  51390 / 60421 loss=0.967, ppl=1.96, wps=16529.7, ups=4.39, wpb=3765.2, bsz=140.9, num_updates=111800, lr=9.45756e-05, gnorm=1.182, loss_scale=4, train_wall=23, gb_free=10.9, wall=34096
2021-05-01 21:43:58 | INFO | train_inner | epoch 002:  51490 / 60421 loss=1.027, ppl=2.04, wps=16525.1, ups=4.4, wpb=3755.4, bsz=139.8, num_updates=111900, lr=9.45333e-05, gnorm=1.157, loss_scale=4, train_wall=23, gb_free=10.9, wall=34119
2021-05-01 21:44:21 | INFO | train_inner | epoch 002:  51590 / 60421 loss=1, ppl=2, wps=16494.4, ups=4.4, wpb=3749.2, bsz=129.8, num_updates=112000, lr=9.44911e-05, gnorm=1.156, loss_scale=4, train_wall=23, gb_free=10.8, wall=34142
2021-05-01 21:44:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:44:21 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:44:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:44:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:44:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:44:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:44:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:44:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:44:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:44:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:44:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:44:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:44:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:44:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:45:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.845 | ppl 28.74 | bleu 28.71 | wps 2455.8 | wpb 2024.1 | bsz 97.5 | num_updates 112000 | best_bleu 29.19
2021-05-01 21:45:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 112000 updates
2021-05-01 21:45:25 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_112000.pt
2021-05-01 21:45:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_112000.pt
2021-05-01 21:45:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_112000.pt (epoch 2 @ 112000 updates, score 28.71) (writing took 7.833477159998438 seconds)
2021-05-01 21:45:56 | INFO | train_inner | epoch 002:  51690 / 60421 loss=1.008, ppl=2.01, wps=3926.5, ups=1.04, wpb=3757.9, bsz=143.3, num_updates=112100, lr=9.4449e-05, gnorm=1.24, loss_scale=4, train_wall=23, gb_free=10.9, wall=34237
2021-05-01 21:46:20 | INFO | train_inner | epoch 002:  51790 / 60421 loss=1.068, ppl=2.1, wps=16372.7, ups=4.28, wpb=3827.5, bsz=129.3, num_updates=112200, lr=9.44069e-05, gnorm=1.113, loss_scale=4, train_wall=23, gb_free=10.9, wall=34261
2021-05-01 21:46:43 | INFO | train_inner | epoch 002:  51890 / 60421 loss=0.979, ppl=1.97, wps=15756, ups=4.25, wpb=3708.2, bsz=152.3, num_updates=112300, lr=9.43648e-05, gnorm=1.368, loss_scale=4, train_wall=23, gb_free=11.1, wall=34284
2021-05-01 21:47:06 | INFO | train_inner | epoch 002:  51990 / 60421 loss=1.057, ppl=2.08, wps=15994.5, ups=4.34, wpb=3682.5, bsz=117.9, num_updates=112400, lr=9.43228e-05, gnorm=1.745, loss_scale=4, train_wall=23, gb_free=10.8, wall=34307
2021-05-01 21:47:29 | INFO | train_inner | epoch 002:  52090 / 60421 loss=1.008, ppl=2.01, wps=16402.9, ups=4.36, wpb=3763.5, bsz=140.6, num_updates=112500, lr=9.42809e-05, gnorm=1.239, loss_scale=4, train_wall=23, gb_free=10.7, wall=34330
2021-05-01 21:47:52 | INFO | train_inner | epoch 002:  52190 / 60421 loss=1.08, ppl=2.11, wps=16431.3, ups=4.41, wpb=3723.3, bsz=124.3, num_updates=112600, lr=9.4239e-05, gnorm=1.563, loss_scale=4, train_wall=22, gb_free=11, wall=34353
2021-05-01 21:48:15 | INFO | train_inner | epoch 002:  52290 / 60421 loss=0.991, ppl=1.99, wps=16346, ups=4.41, wpb=3709.7, bsz=134.3, num_updates=112700, lr=9.41972e-05, gnorm=1.365, loss_scale=4, train_wall=23, gb_free=10.8, wall=34376
2021-05-01 21:48:37 | INFO | train_inner | epoch 002:  52390 / 60421 loss=1.041, ppl=2.06, wps=16485.3, ups=4.4, wpb=3745.6, bsz=156, num_updates=112800, lr=9.41554e-05, gnorm=1.343, loss_scale=4, train_wall=23, gb_free=10.8, wall=34398
2021-05-01 21:49:00 | INFO | train_inner | epoch 002:  52490 / 60421 loss=1.125, ppl=2.18, wps=16648.9, ups=4.42, wpb=3768.1, bsz=117.3, num_updates=112900, lr=9.41137e-05, gnorm=1.572, loss_scale=4, train_wall=22, gb_free=10.8, wall=34421
2021-05-01 21:49:23 | INFO | train_inner | epoch 002:  52590 / 60421 loss=1.005, ppl=2.01, wps=16467.9, ups=4.4, wpb=3742.4, bsz=136.8, num_updates=113000, lr=9.40721e-05, gnorm=1.279, loss_scale=4, train_wall=23, gb_free=10.8, wall=34444
2021-05-01 21:49:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:49:23 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:49:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:49:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:49:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:49:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:49:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:49:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:49:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:49:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:49:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:49:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:49:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:49:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:49:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:49:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:49:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:50:28 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.798 | ppl 27.82 | bleu 29.19 | wps 2414.4 | wpb 2024.1 | bsz 97.5 | num_updates 113000 | best_bleu 29.19
2021-05-01 21:50:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 113000 updates
2021-05-01 21:50:28 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_113000.pt
2021-05-01 21:50:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_113000.pt
2021-05-01 21:50:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_113000.pt (epoch 2 @ 113000 updates, score 29.19) (writing took 14.009021985999425 seconds)
2021-05-01 21:51:06 | INFO | train_inner | epoch 002:  52690 / 60421 loss=1.007, ppl=2.01, wps=3667.8, ups=0.97, wpb=3779.5, bsz=132.6, num_updates=113100, lr=9.40305e-05, gnorm=1.339, loss_scale=4, train_wall=23, gb_free=10.8, wall=34547
2021-05-01 21:51:29 | INFO | train_inner | epoch 002:  52790 / 60421 loss=1.044, ppl=2.06, wps=15984.5, ups=4.31, wpb=3707.9, bsz=128.3, num_updates=113200, lr=9.39889e-05, gnorm=1.442, loss_scale=4, train_wall=23, gb_free=10.8, wall=34570
2021-05-01 21:51:52 | INFO | train_inner | epoch 002:  52890 / 60421 loss=1.043, ppl=2.06, wps=15811.9, ups=4.27, wpb=3705.6, bsz=126.9, num_updates=113300, lr=9.39475e-05, gnorm=1.662, loss_scale=4, train_wall=23, gb_free=10.8, wall=34593
2021-05-01 21:52:15 | INFO | train_inner | epoch 002:  52990 / 60421 loss=1.086, ppl=2.12, wps=16297.2, ups=4.39, wpb=3713.3, bsz=117, num_updates=113400, lr=9.3906e-05, gnorm=1.513, loss_scale=4, train_wall=23, gb_free=10.7, wall=34616
2021-05-01 21:52:38 | INFO | train_inner | epoch 002:  53090 / 60421 loss=1.037, ppl=2.05, wps=16428.6, ups=4.45, wpb=3690, bsz=131.1, num_updates=113500, lr=9.38647e-05, gnorm=1.359, loss_scale=4, train_wall=22, gb_free=11.1, wall=34639
2021-05-01 21:53:00 | INFO | train_inner | epoch 002:  53190 / 60421 loss=1.058, ppl=2.08, wps=16320.2, ups=4.48, wpb=3646.7, bsz=127.1, num_updates=113600, lr=9.38233e-05, gnorm=1.967, loss_scale=4, train_wall=22, gb_free=11.1, wall=34661
2021-05-01 21:53:23 | INFO | train_inner | epoch 002:  53290 / 60421 loss=1.041, ppl=2.06, wps=16530, ups=4.4, wpb=3756.4, bsz=145.5, num_updates=113700, lr=9.37821e-05, gnorm=1.438, loss_scale=4, train_wall=23, gb_free=11.1, wall=34684
2021-05-01 21:53:45 | INFO | train_inner | epoch 002:  53390 / 60421 loss=1.086, ppl=2.12, wps=16720.2, ups=4.37, wpb=3822, bsz=116.3, num_updates=113800, lr=9.37408e-05, gnorm=1.268, loss_scale=4, train_wall=23, gb_free=10.8, wall=34706
2021-05-01 21:54:08 | INFO | train_inner | epoch 002:  53490 / 60421 loss=0.957, ppl=1.94, wps=16527.9, ups=4.41, wpb=3748.3, bsz=147.5, num_updates=113900, lr=9.36997e-05, gnorm=1.078, loss_scale=4, train_wall=22, gb_free=11.1, wall=34729
2021-05-01 21:54:31 | INFO | train_inner | epoch 002:  53590 / 60421 loss=0.99, ppl=1.99, wps=16477.2, ups=4.37, wpb=3773.6, bsz=142.3, num_updates=114000, lr=9.36586e-05, gnorm=1.195, loss_scale=4, train_wall=23, gb_free=10.8, wall=34752
2021-05-01 21:54:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:54:31 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:54:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:54:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:54:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:54:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:54:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:54:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:54:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:54:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:54:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:55:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.814 | ppl 28.14 | bleu 29.08 | wps 2425.1 | wpb 2024.1 | bsz 97.5 | num_updates 114000 | best_bleu 29.19
2021-05-01 21:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 114000 updates
2021-05-01 21:55:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_114000.pt
2021-05-01 21:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_114000.pt
2021-05-01 21:55:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_114000.pt (epoch 2 @ 114000 updates, score 29.08) (writing took 8.051196753003751 seconds)
2021-05-01 21:56:08 | INFO | train_inner | epoch 002:  53690 / 60421 loss=1.067, ppl=2.09, wps=3856.6, ups=1.03, wpb=3729.6, bsz=116.6, num_updates=114100, lr=9.36175e-05, gnorm=1.267, loss_scale=4, train_wall=23, gb_free=10.7, wall=34849
2021-05-01 21:56:31 | INFO | train_inner | epoch 002:  53790 / 60421 loss=1.003, ppl=2, wps=15805, ups=4.25, wpb=3719.2, bsz=138.4, num_updates=114200, lr=9.35765e-05, gnorm=1.376, loss_scale=4, train_wall=23, gb_free=11, wall=34872
2021-05-01 21:56:55 | INFO | train_inner | epoch 002:  53890 / 60421 loss=1.011, ppl=2.02, wps=15768, ups=4.27, wpb=3688.7, bsz=128.1, num_updates=114300, lr=9.35356e-05, gnorm=1.342, loss_scale=4, train_wall=23, gb_free=10.7, wall=34896
2021-05-01 21:57:18 | INFO | train_inner | epoch 002:  53990 / 60421 loss=1.035, ppl=2.05, wps=15988.6, ups=4.33, wpb=3690.1, bsz=114.5, num_updates=114400, lr=9.34947e-05, gnorm=1.512, loss_scale=4, train_wall=23, gb_free=10.8, wall=34919
2021-05-01 21:57:40 | INFO | train_inner | epoch 002:  54090 / 60421 loss=1.111, ppl=2.16, wps=16464.5, ups=4.41, wpb=3736.8, bsz=127.1, num_updates=114500, lr=9.34539e-05, gnorm=1.466, loss_scale=4, train_wall=23, gb_free=11.6, wall=34941
2021-05-01 21:58:03 | INFO | train_inner | epoch 002:  54190 / 60421 loss=1.133, ppl=2.19, wps=16684.7, ups=4.35, wpb=3831.6, bsz=135.6, num_updates=114600, lr=9.34131e-05, gnorm=1.783, loss_scale=4, train_wall=23, gb_free=10.9, wall=34964
2021-05-01 21:58:26 | INFO | train_inner | epoch 002:  54290 / 60421 loss=1.071, ppl=2.1, wps=16701, ups=4.39, wpb=3802.2, bsz=114, num_updates=114700, lr=9.33724e-05, gnorm=1.332, loss_scale=4, train_wall=23, gb_free=11.1, wall=34987
2021-05-01 21:58:49 | INFO | train_inner | epoch 002:  54390 / 60421 loss=1.063, ppl=2.09, wps=16428.9, ups=4.4, wpb=3736, bsz=127.9, num_updates=114800, lr=9.33317e-05, gnorm=1.391, loss_scale=4, train_wall=23, gb_free=10.9, wall=35010
2021-05-01 21:59:11 | INFO | train_inner | epoch 002:  54490 / 60421 loss=1.026, ppl=2.04, wps=16374.3, ups=4.45, wpb=3676.5, bsz=134.1, num_updates=114900, lr=9.32911e-05, gnorm=1.587, loss_scale=4, train_wall=22, gb_free=10.8, wall=35032
2021-05-01 21:59:34 | INFO | train_inner | epoch 002:  54590 / 60421 loss=0.971, ppl=1.96, wps=16296.3, ups=4.4, wpb=3706, bsz=151.7, num_updates=115000, lr=9.32505e-05, gnorm=1.22, loss_scale=4, train_wall=23, gb_free=10.8, wall=35055
2021-05-01 21:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 21:59:34 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 21:59:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:59:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:59:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:59:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:59:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:59:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:59:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:59:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:59:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:59:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:59:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:59:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 21:59:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 21:59:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 21:59:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:00:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.8 | ppl 27.86 | bleu 28.95 | wps 2421.6 | wpb 2024.1 | bsz 97.5 | num_updates 115000 | best_bleu 29.19
2021-05-01 22:00:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 115000 updates
2021-05-01 22:00:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_115000.pt
2021-05-01 22:00:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_115000.pt
2021-05-01 22:00:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_115000.pt (epoch 2 @ 115000 updates, score 28.95) (writing took 7.733469227008754 seconds)
2021-05-01 22:01:11 | INFO | train_inner | epoch 002:  54690 / 60421 loss=1.01, ppl=2.01, wps=3850.9, ups=1.04, wpb=3717.6, bsz=129.2, num_updates=115100, lr=9.321e-05, gnorm=1.331, loss_scale=4, train_wall=23, gb_free=10.7, wall=35152
2021-05-01 22:01:34 | INFO | train_inner | epoch 002:  54790 / 60421 loss=1.041, ppl=2.06, wps=16097.2, ups=4.34, wpb=3710.2, bsz=119.2, num_updates=115200, lr=9.31695e-05, gnorm=1.438, loss_scale=4, train_wall=23, gb_free=10.6, wall=35175
2021-05-01 22:01:57 | INFO | train_inner | epoch 002:  54890 / 60421 loss=0.998, ppl=2, wps=15880.7, ups=4.28, wpb=3710.6, bsz=139.6, num_updates=115300, lr=9.31291e-05, gnorm=1.39, loss_scale=4, train_wall=23, gb_free=10.8, wall=35198
2021-05-01 22:02:20 | INFO | train_inner | epoch 002:  54990 / 60421 loss=1.047, ppl=2.07, wps=16429.2, ups=4.38, wpb=3748.9, bsz=124.6, num_updates=115400, lr=9.30887e-05, gnorm=1.504, loss_scale=4, train_wall=23, gb_free=10.8, wall=35221
2021-05-01 22:02:43 | INFO | train_inner | epoch 002:  55090 / 60421 loss=0.959, ppl=1.94, wps=16413.3, ups=4.38, wpb=3743.9, bsz=139.8, num_updates=115500, lr=9.30484e-05, gnorm=1.236, loss_scale=4, train_wall=23, gb_free=10.9, wall=35244
2021-05-01 22:03:05 | INFO | train_inner | epoch 002:  55190 / 60421 loss=1.028, ppl=2.04, wps=16301.2, ups=4.43, wpb=3675.8, bsz=133, num_updates=115600, lr=9.30082e-05, gnorm=1.536, loss_scale=4, train_wall=22, gb_free=10.8, wall=35266
2021-05-01 22:03:28 | INFO | train_inner | epoch 002:  55290 / 60421 loss=1.106, ppl=2.15, wps=16474, ups=4.44, wpb=3708.9, bsz=111.1, num_updates=115700, lr=9.2968e-05, gnorm=1.558, loss_scale=4, train_wall=22, gb_free=10.7, wall=35289
2021-05-01 22:03:51 | INFO | train_inner | epoch 002:  55390 / 60421 loss=0.98, ppl=1.97, wps=16498.8, ups=4.4, wpb=3746.3, bsz=156.6, num_updates=115800, lr=9.29278e-05, gnorm=1.285, loss_scale=4, train_wall=23, gb_free=10.9, wall=35311
2021-05-01 22:04:13 | INFO | train_inner | epoch 002:  55490 / 60421 loss=1.055, ppl=2.08, wps=16571.8, ups=4.4, wpb=3763.9, bsz=145, num_updates=115900, lr=9.28877e-05, gnorm=1.625, loss_scale=4, train_wall=23, gb_free=10.9, wall=35334
2021-05-01 22:04:36 | INFO | train_inner | epoch 002:  55590 / 60421 loss=1.025, ppl=2.04, wps=16559.8, ups=4.4, wpb=3762.2, bsz=126, num_updates=116000, lr=9.28477e-05, gnorm=1.253, loss_scale=4, train_wall=23, gb_free=10.8, wall=35357
2021-05-01 22:04:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:04:36 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:04:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:04:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:04:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:04:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:04:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:04:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:04:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:04:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:04:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:04:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:04:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:04:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:04:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:04:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:04:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:05:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.794 | ppl 27.74 | bleu 28.92 | wps 2434.2 | wpb 2024.1 | bsz 97.5 | num_updates 116000 | best_bleu 29.19
2021-05-01 22:05:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 116000 updates
2021-05-01 22:05:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_116000.pt
2021-05-01 22:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_116000.pt
2021-05-01 22:05:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_116000.pt (epoch 2 @ 116000 updates, score 28.92) (writing took 7.754246090000379 seconds)
2021-05-01 22:06:12 | INFO | train_inner | epoch 002:  55690 / 60421 loss=1.042, ppl=2.06, wps=3903.6, ups=1.04, wpb=3757.2, bsz=134.3, num_updates=116100, lr=9.28077e-05, gnorm=1.371, loss_scale=4, train_wall=23, gb_free=10.9, wall=35453
2021-05-01 22:06:35 | INFO | train_inner | epoch 002:  55790 / 60421 loss=0.985, ppl=1.98, wps=15904.3, ups=4.35, wpb=3657.2, bsz=121.8, num_updates=116200, lr=9.27677e-05, gnorm=1.491, loss_scale=4, train_wall=23, gb_free=10.8, wall=35476
2021-05-01 22:06:58 | INFO | train_inner | epoch 002:  55890 / 60421 loss=1.032, ppl=2.05, wps=15898.1, ups=4.33, wpb=3675.6, bsz=130.6, num_updates=116300, lr=9.27278e-05, gnorm=1.542, loss_scale=4, train_wall=23, gb_free=11.4, wall=35499
2021-05-01 22:07:21 | INFO | train_inner | epoch 002:  55990 / 60421 loss=1.012, ppl=2.02, wps=16422.4, ups=4.37, wpb=3753.8, bsz=146.6, num_updates=116400, lr=9.2688e-05, gnorm=1.337, loss_scale=4, train_wall=23, gb_free=10.6, wall=35522
2021-05-01 22:07:44 | INFO | train_inner | epoch 002:  56090 / 60421 loss=1.059, ppl=2.08, wps=16610.4, ups=4.46, wpb=3720.4, bsz=129.8, num_updates=116500, lr=9.26482e-05, gnorm=1.385, loss_scale=4, train_wall=22, gb_free=10.8, wall=35545
2021-05-01 22:08:06 | INFO | train_inner | epoch 002:  56190 / 60421 loss=1.019, ppl=2.03, wps=16368, ups=4.45, wpb=3676.7, bsz=131.9, num_updates=116600, lr=9.26085e-05, gnorm=1.541, loss_scale=4, train_wall=22, gb_free=10.9, wall=35567
2021-05-01 22:08:29 | INFO | train_inner | epoch 002:  56290 / 60421 loss=0.979, ppl=1.97, wps=16476.8, ups=4.44, wpb=3714.8, bsz=128.6, num_updates=116700, lr=9.25688e-05, gnorm=1.209, loss_scale=4, train_wall=22, gb_free=11, wall=35590
2021-05-01 22:08:51 | INFO | train_inner | epoch 002:  56390 / 60421 loss=1.014, ppl=2.02, wps=16589.1, ups=4.45, wpb=3730.6, bsz=130.5, num_updates=116800, lr=9.25292e-05, gnorm=1.314, loss_scale=4, train_wall=22, gb_free=10.9, wall=35612
2021-05-01 22:09:14 | INFO | train_inner | epoch 002:  56490 / 60421 loss=1.012, ppl=2.02, wps=16727.8, ups=4.42, wpb=3783.7, bsz=142.3, num_updates=116900, lr=9.24896e-05, gnorm=1.142, loss_scale=4, train_wall=22, gb_free=11.2, wall=35635
2021-05-01 22:09:37 | INFO | train_inner | epoch 002:  56590 / 60421 loss=1.07, ppl=2.1, wps=16628.9, ups=4.38, wpb=3798.2, bsz=125.5, num_updates=117000, lr=9.245e-05, gnorm=1.28, loss_scale=4, train_wall=23, gb_free=10.7, wall=35657
2021-05-01 22:09:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:09:37 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:09:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:09:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:09:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:09:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:09:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:09:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:09:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:09:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:09:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:09:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:09:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:09:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:09:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:09:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:09:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:10:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.832 | ppl 28.48 | bleu 29 | wps 2444 | wpb 2024.1 | bsz 97.5 | num_updates 117000 | best_bleu 29.19
2021-05-01 22:10:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 117000 updates
2021-05-01 22:10:41 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_117000.pt
2021-05-01 22:10:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_117000.pt
2021-05-01 22:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_117000.pt (epoch 2 @ 117000 updates, score 29.0) (writing took 7.733684685998014 seconds)
2021-05-01 22:11:12 | INFO | train_inner | epoch 002:  56690 / 60421 loss=1.001, ppl=2, wps=3903.5, ups=1.04, wpb=3737.7, bsz=118.5, num_updates=117100, lr=9.24105e-05, gnorm=1.047, loss_scale=4, train_wall=23, gb_free=10.8, wall=35753
2021-05-01 22:11:36 | INFO | train_inner | epoch 002:  56790 / 60421 loss=0.982, ppl=1.98, wps=16116.2, ups=4.29, wpb=3756.8, bsz=118.2, num_updates=117200, lr=9.23711e-05, gnorm=1.235, loss_scale=4, train_wall=23, gb_free=10.8, wall=35777
2021-05-01 22:11:59 | INFO | train_inner | epoch 002:  56890 / 60421 loss=1.056, ppl=2.08, wps=15960.3, ups=4.3, wpb=3711.8, bsz=113.3, num_updates=117300, lr=9.23317e-05, gnorm=1.53, loss_scale=4, train_wall=23, gb_free=10.7, wall=35800
2021-05-01 22:12:22 | INFO | train_inner | epoch 002:  56990 / 60421 loss=1.041, ppl=2.06, wps=16430.2, ups=4.35, wpb=3773.1, bsz=122.6, num_updates=117400, lr=9.22924e-05, gnorm=1.215, loss_scale=4, train_wall=23, gb_free=10.7, wall=35823
2021-05-01 22:12:44 | INFO | train_inner | epoch 002:  57090 / 60421 loss=1.044, ppl=2.06, wps=16553.5, ups=4.45, wpb=3721.6, bsz=127.1, num_updates=117500, lr=9.22531e-05, gnorm=1.443, loss_scale=4, train_wall=22, gb_free=10.7, wall=35845
2021-05-01 22:13:07 | INFO | train_inner | epoch 002:  57190 / 60421 loss=0.989, ppl=1.98, wps=16526.3, ups=4.41, wpb=3748.9, bsz=138.1, num_updates=117600, lr=9.22139e-05, gnorm=1.137, loss_scale=4, train_wall=22, gb_free=10.7, wall=35868
2021-05-01 22:13:30 | INFO | train_inner | epoch 002:  57290 / 60421 loss=1.036, ppl=2.05, wps=16796.3, ups=4.41, wpb=3806.4, bsz=127.8, num_updates=117700, lr=9.21747e-05, gnorm=1.224, loss_scale=4, train_wall=22, gb_free=11.1, wall=35891
2021-05-01 22:13:53 | INFO | train_inner | epoch 002:  57390 / 60421 loss=1, ppl=2, wps=16778.6, ups=4.36, wpb=3847.6, bsz=150.9, num_updates=117800, lr=9.21356e-05, gnorm=1.112, loss_scale=4, train_wall=23, gb_free=10.8, wall=35914
2021-05-01 22:14:15 | INFO | train_inner | epoch 002:  57490 / 60421 loss=1.028, ppl=2.04, wps=16667.2, ups=4.37, wpb=3816.6, bsz=122.1, num_updates=117900, lr=9.20965e-05, gnorm=1.106, loss_scale=4, train_wall=23, gb_free=10.7, wall=35936
2021-05-01 22:14:38 | INFO | train_inner | epoch 002:  57590 / 60421 loss=1.045, ppl=2.06, wps=16619.5, ups=4.41, wpb=3766.4, bsz=128.4, num_updates=118000, lr=9.20575e-05, gnorm=1.292, loss_scale=4, train_wall=22, gb_free=10.7, wall=35959
2021-05-01 22:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:14:38 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:14:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:14:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:14:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:14:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:14:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:14:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:14:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:14:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:14:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:14:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:14:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:14:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:14:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:14:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:14:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:15:45 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.84 | ppl 28.63 | bleu 29.31 | wps 2374.3 | wpb 2024.1 | bsz 97.5 | num_updates 118000 | best_bleu 29.31
2021-05-01 22:15:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 118000 updates
2021-05-01 22:15:45 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_118000.pt
2021-05-01 22:15:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_118000.pt
2021-05-01 22:15:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_118000.pt (epoch 2 @ 118000 updates, score 29.31) (writing took 14.325634305991116 seconds)
2021-05-01 22:16:22 | INFO | train_inner | epoch 002:  57690 / 60421 loss=1.008, ppl=2.01, wps=3581.1, ups=0.96, wpb=3734.3, bsz=135.7, num_updates=118100, lr=9.20185e-05, gnorm=1.268, loss_scale=4, train_wall=23, gb_free=11, wall=36063
2021-05-01 22:16:46 | INFO | train_inner | epoch 002:  57790 / 60421 loss=1.025, ppl=2.04, wps=16059.1, ups=4.33, wpb=3712.6, bsz=114.9, num_updates=118200, lr=9.19795e-05, gnorm=1.255, loss_scale=4, train_wall=23, gb_free=11, wall=36086
2021-05-01 22:17:09 | INFO | train_inner | epoch 002:  57890 / 60421 loss=0.992, ppl=1.99, wps=16161.5, ups=4.35, wpb=3717.9, bsz=145.9, num_updates=118300, lr=9.19407e-05, gnorm=1.512, loss_scale=4, train_wall=23, gb_free=11, wall=36109
2021-05-01 22:17:31 | INFO | train_inner | epoch 002:  57990 / 60421 loss=1.031, ppl=2.04, wps=16552.6, ups=4.4, wpb=3758.8, bsz=134.8, num_updates=118400, lr=9.19018e-05, gnorm=1.372, loss_scale=4, train_wall=23, gb_free=10.8, wall=36132
2021-05-01 22:17:54 | INFO | train_inner | epoch 002:  58090 / 60421 loss=1.04, ppl=2.06, wps=16500.6, ups=4.43, wpb=3724.4, bsz=128.2, num_updates=118500, lr=9.1863e-05, gnorm=1.326, loss_scale=4, train_wall=22, gb_free=11, wall=36155
2021-05-01 22:18:16 | INFO | train_inner | epoch 002:  58190 / 60421 loss=1.009, ppl=2.01, wps=16319.6, ups=4.5, wpb=3627.9, bsz=128.9, num_updates=118600, lr=9.18243e-05, gnorm=1.455, loss_scale=4, train_wall=22, gb_free=10.9, wall=36177
2021-05-01 22:18:38 | INFO | train_inner | epoch 002:  58290 / 60421 loss=0.99, ppl=1.99, wps=16410.2, ups=4.46, wpb=3679.5, bsz=136.4, num_updates=118700, lr=9.17856e-05, gnorm=1.283, loss_scale=4, train_wall=22, gb_free=10.7, wall=36199
2021-05-01 22:19:01 | INFO | train_inner | epoch 002:  58390 / 60421 loss=1.043, ppl=2.06, wps=16386.5, ups=4.45, wpb=3679.9, bsz=122.2, num_updates=118800, lr=9.1747e-05, gnorm=1.852, loss_scale=4, train_wall=22, gb_free=11.7, wall=36222
2021-05-01 22:19:24 | INFO | train_inner | epoch 002:  58490 / 60421 loss=1.031, ppl=2.04, wps=16533.4, ups=4.42, wpb=3744, bsz=134, num_updates=118900, lr=9.17084e-05, gnorm=1.423, loss_scale=4, train_wall=22, gb_free=10.8, wall=36245
2021-05-01 22:19:46 | INFO | train_inner | epoch 002:  58590 / 60421 loss=1, ppl=2, wps=16417.3, ups=4.43, wpb=3702.1, bsz=134.7, num_updates=119000, lr=9.16698e-05, gnorm=1.301, loss_scale=4, train_wall=22, gb_free=10.7, wall=36267
2021-05-01 22:19:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:19:46 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:19:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:19:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:19:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:20:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:20:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:20:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:20:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:20:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:20:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:20:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:20:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:20:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:20:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:20:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:20:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:20:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.84 | ppl 28.65 | bleu 29.24 | wps 2404 | wpb 2024.1 | bsz 97.5 | num_updates 119000 | best_bleu 29.31
2021-05-01 22:20:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 119000 updates
2021-05-01 22:20:52 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_119000.pt
2021-05-01 22:20:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_119000.pt
2021-05-01 22:21:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_119000.pt (epoch 2 @ 119000 updates, score 29.24) (writing took 7.715195862998371 seconds)
2021-05-01 22:21:23 | INFO | train_inner | epoch 002:  58690 / 60421 loss=0.938, ppl=1.92, wps=3882.2, ups=1.03, wpb=3767.5, bsz=143.9, num_updates=119100, lr=9.16314e-05, gnorm=1.127, loss_scale=4, train_wall=23, gb_free=10.7, wall=36364
2021-05-01 22:21:46 | INFO | train_inner | epoch 002:  58790 / 60421 loss=0.99, ppl=1.99, wps=15729, ups=4.32, wpb=3638, bsz=124.2, num_updates=119200, lr=9.15929e-05, gnorm=1.347, loss_scale=4, train_wall=23, gb_free=10.8, wall=36387
2021-05-01 22:22:09 | INFO | train_inner | epoch 002:  58890 / 60421 loss=1.057, ppl=2.08, wps=16243.4, ups=4.39, wpb=3699.8, bsz=114.1, num_updates=119300, lr=9.15545e-05, gnorm=1.286, loss_scale=4, train_wall=23, gb_free=11, wall=36410
2021-05-01 22:22:32 | INFO | train_inner | epoch 002:  58990 / 60421 loss=1.006, ppl=2.01, wps=16603.4, ups=4.39, wpb=3783, bsz=135.7, num_updates=119400, lr=9.15162e-05, gnorm=1.104, loss_scale=4, train_wall=23, gb_free=10.8, wall=36433
2021-05-01 22:22:55 | INFO | train_inner | epoch 002:  59090 / 60421 loss=1.004, ppl=2.01, wps=16666.7, ups=4.41, wpb=3783, bsz=133.1, num_updates=119500, lr=9.14779e-05, gnorm=1.337, loss_scale=4, train_wall=23, gb_free=10.8, wall=36456
2021-05-01 22:23:17 | INFO | train_inner | epoch 002:  59190 / 60421 loss=1.022, ppl=2.03, wps=16582.8, ups=4.4, wpb=3771.7, bsz=136.5, num_updates=119600, lr=9.14396e-05, gnorm=1.28, loss_scale=4, train_wall=23, gb_free=11.1, wall=36478
2021-05-01 22:23:40 | INFO | train_inner | epoch 002:  59290 / 60421 loss=0.995, ppl=1.99, wps=16419.9, ups=4.45, wpb=3693.5, bsz=143.2, num_updates=119700, lr=9.14014e-05, gnorm=1.267, loss_scale=4, train_wall=22, gb_free=10.8, wall=36501
2021-05-01 22:24:02 | INFO | train_inner | epoch 002:  59390 / 60421 loss=0.987, ppl=1.98, wps=16470.3, ups=4.41, wpb=3730.9, bsz=126.2, num_updates=119800, lr=9.13633e-05, gnorm=1.13, loss_scale=4, train_wall=22, gb_free=10.9, wall=36523
2021-05-01 22:24:25 | INFO | train_inner | epoch 002:  59490 / 60421 loss=1.044, ppl=2.06, wps=16451.3, ups=4.43, wpb=3715.9, bsz=122.6, num_updates=119900, lr=9.13252e-05, gnorm=1.238, loss_scale=4, train_wall=22, gb_free=11.2, wall=36546
2021-05-01 22:24:48 | INFO | train_inner | epoch 002:  59590 / 60421 loss=1.032, ppl=2.04, wps=16320.3, ups=4.42, wpb=3695.2, bsz=123.1, num_updates=120000, lr=9.12871e-05, gnorm=1.429, loss_scale=4, train_wall=22, gb_free=10.7, wall=36569
2021-05-01 22:24:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:24:48 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:24:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:24:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:24:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:25:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:25:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:25:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:25:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:25:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:25:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:25:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:25:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:25:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:25:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:25:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:25:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:25:53 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.858 | ppl 29 | bleu 28.98 | wps 2422.6 | wpb 2024.1 | bsz 97.5 | num_updates 120000 | best_bleu 29.31
2021-05-01 22:25:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 120000 updates
2021-05-01 22:25:53 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_120000.pt
2021-05-01 22:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_120000.pt
2021-05-01 22:26:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_2_120000.pt (epoch 2 @ 120000 updates, score 28.98) (writing took 8.04267831800098 seconds)
2021-05-01 22:26:24 | INFO | train_inner | epoch 002:  59690 / 60421 loss=1.037, ppl=2.05, wps=3846, ups=1.03, wpb=3716.2, bsz=135.8, num_updates=120100, lr=9.12491e-05, gnorm=1.508, loss_scale=4, train_wall=23, gb_free=10.9, wall=36665
2021-05-01 22:26:47 | INFO | train_inner | epoch 002:  59790 / 60421 loss=1.045, ppl=2.06, wps=15929.3, ups=4.31, wpb=3694.4, bsz=150.6, num_updates=120200, lr=9.12111e-05, gnorm=1.541, loss_scale=4, train_wall=23, gb_free=10.8, wall=36688
2021-05-01 22:27:11 | INFO | train_inner | epoch 002:  59890 / 60421 loss=0.989, ppl=1.98, wps=15983.4, ups=4.34, wpb=3682.3, bsz=130.8, num_updates=120300, lr=9.11732e-05, gnorm=1.42, loss_scale=4, train_wall=23, gb_free=10.8, wall=36711
2021-05-01 22:27:33 | INFO | train_inner | epoch 002:  59990 / 60421 loss=0.914, ppl=1.88, wps=16478.2, ups=4.43, wpb=3720.1, bsz=162.2, num_updates=120400, lr=9.11353e-05, gnorm=1.229, loss_scale=4, train_wall=22, gb_free=11.9, wall=36734
2021-05-01 22:27:55 | INFO | train_inner | epoch 002:  60090 / 60421 loss=1.089, ppl=2.13, wps=16517.1, ups=4.47, wpb=3692.3, bsz=125, num_updates=120500, lr=9.10975e-05, gnorm=1.726, loss_scale=4, train_wall=22, gb_free=11.1, wall=36756
2021-05-01 22:28:18 | INFO | train_inner | epoch 002:  60190 / 60421 loss=0.995, ppl=1.99, wps=16410.2, ups=4.44, wpb=3699.1, bsz=137, num_updates=120600, lr=9.10597e-05, gnorm=1.2, loss_scale=4, train_wall=22, gb_free=11.2, wall=36779
2021-05-01 22:28:41 | INFO | train_inner | epoch 002:  60290 / 60421 loss=0.997, ppl=2, wps=16463.1, ups=4.4, wpb=3745, bsz=148.6, num_updates=120700, lr=9.1022e-05, gnorm=1.343, loss_scale=4, train_wall=23, gb_free=10.8, wall=36802
2021-05-01 22:29:03 | INFO | train_inner | epoch 002:  60390 / 60421 loss=1.001, ppl=2, wps=16631.7, ups=4.44, wpb=3747.4, bsz=133, num_updates=120800, lr=9.09843e-05, gnorm=1.118, loss_scale=4, train_wall=22, gb_free=10.7, wall=36824
2021-05-01 22:29:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:29:10 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:29:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:29:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:29:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:29:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:29:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:29:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:29:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:29:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:29:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:29:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:29:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:29:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:29:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:29:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:29:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:30:15 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.865 | ppl 29.14 | bleu 29.09 | wps 2441.7 | wpb 2024.1 | bsz 97.5 | num_updates 120831 | best_bleu 29.31
2021-05-01 22:30:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 120831 updates
2021-05-01 22:30:15 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint2.pt
2021-05-01 22:30:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint2.pt
2021-05-01 22:30:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint2.pt (epoch 2 @ 120831 updates, score 29.09) (writing took 7.502812556005665 seconds)
2021-05-01 22:30:23 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-05-01 22:30:23 | INFO | train | epoch 002 | loss 1.087 | ppl 2.12 | wps 12264.8 | ups 3.28 | wpb 3737.5 | bsz 132.4 | num_updates 120831 | lr 9.09726e-05 | gnorm 1.423 | loss_scale 4 | train_wall 13718 | gb_free 10.7 | wall 36904
2021-05-01 22:30:23 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:30:23 | INFO | fairseq.trainer | begin training epoch 3
2021-05-01 22:30:23 | INFO | fairseq_cli.train | Start iterating over samples
2021-05-01 22:30:39 | INFO | train_inner | epoch 003:     69 / 60421 loss=0.948, ppl=1.93, wps=3899.9, ups=1.05, wpb=3729, bsz=154.4, num_updates=120900, lr=9.09467e-05, gnorm=1.133, loss_scale=4, train_wall=23, gb_free=10.8, wall=36920
2021-05-01 22:31:02 | INFO | train_inner | epoch 003:    169 / 60421 loss=0.908, ppl=1.88, wps=15978.5, ups=4.35, wpb=3675.5, bsz=124.5, num_updates=121000, lr=9.09091e-05, gnorm=1.223, loss_scale=4, train_wall=23, gb_free=10.9, wall=36943
2021-05-01 22:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:31:02 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:31:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:31:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:31:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:31:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:31:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:31:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:31:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:31:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:31:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:32:07 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.838 | ppl 28.59 | bleu 29.16 | wps 2425.1 | wpb 2024.1 | bsz 97.5 | num_updates 121000 | best_bleu 29.31
2021-05-01 22:32:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 121000 updates
2021-05-01 22:32:07 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_121000.pt
2021-05-01 22:32:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_121000.pt
2021-05-01 22:32:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_121000.pt (epoch 3 @ 121000 updates, score 29.16) (writing took 9.160726257992792 seconds)
2021-05-01 22:32:39 | INFO | train_inner | epoch 003:    269 / 60421 loss=0.924, ppl=1.9, wps=3841, ups=1.03, wpb=3729.3, bsz=141, num_updates=121100, lr=9.08715e-05, gnorm=1.191, loss_scale=4, train_wall=22, gb_free=10.9, wall=37040
2021-05-01 22:33:01 | INFO | train_inner | epoch 003:    369 / 60421 loss=0.964, ppl=1.95, wps=16542.6, ups=4.48, wpb=3692.6, bsz=125, num_updates=121200, lr=9.08341e-05, gnorm=1.298, loss_scale=4, train_wall=22, gb_free=10.9, wall=37062
2021-05-01 22:33:24 | INFO | train_inner | epoch 003:    469 / 60421 loss=0.97, ppl=1.96, wps=16382.5, ups=4.49, wpb=3646.1, bsz=125.2, num_updates=121300, lr=9.07966e-05, gnorm=1.37, loss_scale=4, train_wall=22, gb_free=10.8, wall=37085
2021-05-01 22:33:46 | INFO | train_inner | epoch 003:    569 / 60421 loss=0.992, ppl=1.99, wps=16576.3, ups=4.45, wpb=3728, bsz=132.5, num_updates=121400, lr=9.07592e-05, gnorm=1.272, loss_scale=4, train_wall=22, gb_free=10.6, wall=37107
2021-05-01 22:34:09 | INFO | train_inner | epoch 003:    669 / 60421 loss=0.996, ppl=1.99, wps=16476.3, ups=4.43, wpb=3715.9, bsz=116, num_updates=121500, lr=9.07218e-05, gnorm=1.135, loss_scale=4, train_wall=22, gb_free=10.9, wall=37130
2021-05-01 22:34:31 | INFO | train_inner | epoch 003:    769 / 60421 loss=0.917, ppl=1.89, wps=16460.3, ups=4.41, wpb=3735.4, bsz=132.1, num_updates=121600, lr=9.06845e-05, gnorm=1.087, loss_scale=4, train_wall=23, gb_free=10.8, wall=37152
2021-05-01 22:34:54 | INFO | train_inner | epoch 003:    869 / 60421 loss=0.982, ppl=1.98, wps=16488.3, ups=4.4, wpb=3743.6, bsz=125.4, num_updates=121700, lr=9.06473e-05, gnorm=1.08, loss_scale=4, train_wall=23, gb_free=10.7, wall=37175
2021-05-01 22:35:17 | INFO | train_inner | epoch 003:    969 / 60421 loss=0.904, ppl=1.87, wps=16299.8, ups=4.37, wpb=3733.6, bsz=145, num_updates=121800, lr=9.061e-05, gnorm=1.353, loss_scale=4, train_wall=23, gb_free=11, wall=37198
2021-05-01 22:35:40 | INFO | train_inner | epoch 003:   1069 / 60421 loss=0.961, ppl=1.95, wps=16233.7, ups=4.34, wpb=3739.6, bsz=113.9, num_updates=121900, lr=9.05729e-05, gnorm=1.147, loss_scale=4, train_wall=23, gb_free=10.7, wall=37221
2021-05-01 22:36:03 | INFO | train_inner | epoch 003:   1169 / 60421 loss=0.98, ppl=1.97, wps=16073.4, ups=4.29, wpb=3750.3, bsz=126.7, num_updates=122000, lr=9.05357e-05, gnorm=1.14, loss_scale=4, train_wall=23, gb_free=10.9, wall=37244
2021-05-01 22:36:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:36:03 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:36:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:36:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:36:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:36:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:36:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:36:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:36:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:36:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:36:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:37:09 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.9 | ppl 29.86 | bleu 29.13 | wps 2398 | wpb 2024.1 | bsz 97.5 | num_updates 122000 | best_bleu 29.31
2021-05-01 22:37:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 122000 updates
2021-05-01 22:37:09 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_122000.pt
2021-05-01 22:37:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_122000.pt
2021-05-01 22:37:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_122000.pt (epoch 3 @ 122000 updates, score 29.13) (writing took 7.93167232000269 seconds)
2021-05-01 22:37:40 | INFO | train_inner | epoch 003:   1269 / 60421 loss=0.915, ppl=1.89, wps=3834, ups=1.03, wpb=3708.4, bsz=135, num_updates=122100, lr=9.04987e-05, gnorm=1.241, loss_scale=4, train_wall=22, gb_free=11.1, wall=37341
2021-05-01 22:38:03 | INFO | train_inner | epoch 003:   1369 / 60421 loss=0.952, ppl=1.93, wps=16545.1, ups=4.41, wpb=3753.8, bsz=148.4, num_updates=122200, lr=9.04616e-05, gnorm=1.087, loss_scale=4, train_wall=22, gb_free=10.8, wall=37364
2021-05-01 22:38:25 | INFO | train_inner | epoch 003:   1469 / 60421 loss=0.966, ppl=1.95, wps=16507.7, ups=4.45, wpb=3708.4, bsz=125.4, num_updates=122300, lr=9.04246e-05, gnorm=1.333, loss_scale=4, train_wall=22, gb_free=10.8, wall=37386
2021-05-01 22:38:48 | INFO | train_inner | epoch 003:   1569 / 60421 loss=0.917, ppl=1.89, wps=16606.2, ups=4.44, wpb=3741.6, bsz=147.3, num_updates=122400, lr=9.03877e-05, gnorm=1.169, loss_scale=4, train_wall=22, gb_free=10.9, wall=37409
2021-05-01 22:39:10 | INFO | train_inner | epoch 003:   1669 / 60421 loss=0.938, ppl=1.92, wps=16551.4, ups=4.39, wpb=3769.1, bsz=138.8, num_updates=122500, lr=9.03508e-05, gnorm=1.088, loss_scale=4, train_wall=23, gb_free=10.7, wall=37431
2021-05-01 22:39:33 | INFO | train_inner | epoch 003:   1769 / 60421 loss=0.959, ppl=1.94, wps=16659.5, ups=4.39, wpb=3795.7, bsz=136.9, num_updates=122600, lr=9.03139e-05, gnorm=1.184, loss_scale=4, train_wall=23, gb_free=10.8, wall=37454
2021-05-01 22:39:56 | INFO | train_inner | epoch 003:   1869 / 60421 loss=0.927, ppl=1.9, wps=16525.1, ups=4.39, wpb=3763.2, bsz=145, num_updates=122700, lr=9.02771e-05, gnorm=1.078, loss_scale=4, train_wall=23, gb_free=11.1, wall=37477
2021-05-01 22:40:19 | INFO | train_inner | epoch 003:   1969 / 60421 loss=0.961, ppl=1.95, wps=16425, ups=4.38, wpb=3747.2, bsz=132, num_updates=122800, lr=9.02404e-05, gnorm=1.231, loss_scale=4, train_wall=23, gb_free=10.7, wall=37500
2021-05-01 22:40:42 | INFO | train_inner | epoch 003:   2069 / 60421 loss=1.002, ppl=2, wps=16221.1, ups=4.38, wpb=3707.3, bsz=114.9, num_updates=122900, lr=9.02036e-05, gnorm=1.518, loss_scale=4, train_wall=23, gb_free=10.6, wall=37523
2021-05-01 22:41:05 | INFO | train_inner | epoch 003:   2169 / 60421 loss=0.904, ppl=1.87, wps=16000.6, ups=4.34, wpb=3682.8, bsz=147.6, num_updates=123000, lr=9.0167e-05, gnorm=1.193, loss_scale=4, train_wall=23, gb_free=10.8, wall=37546
2021-05-01 22:41:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:41:05 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:41:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:41:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:41:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:41:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:41:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:41:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:41:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:41:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:41:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:41:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:41:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:41:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:41:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:41:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:41:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:42:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.883 | ppl 29.51 | bleu 29.2 | wps 2407.1 | wpb 2024.1 | bsz 97.5 | num_updates 123000 | best_bleu 29.31
2021-05-01 22:42:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 123000 updates
2021-05-01 22:42:11 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_123000.pt
2021-05-01 22:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_123000.pt
2021-05-01 22:42:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_123000.pt (epoch 3 @ 123000 updates, score 29.2) (writing took 7.859938645007787 seconds)
2021-05-01 22:42:41 | INFO | train_inner | epoch 003:   2269 / 60421 loss=1.015, ppl=2.02, wps=3852.4, ups=1.04, wpb=3711.3, bsz=119.6, num_updates=123100, lr=9.01303e-05, gnorm=1.166, loss_scale=4, train_wall=22, gb_free=10.8, wall=37642
2021-05-01 22:43:04 | INFO | train_inner | epoch 003:   2369 / 60421 loss=0.968, ppl=1.96, wps=16325.8, ups=4.43, wpb=3684.4, bsz=127.4, num_updates=123200, lr=9.00937e-05, gnorm=1.06, loss_scale=4, train_wall=22, gb_free=10.9, wall=37665
2021-05-01 22:43:26 | INFO | train_inner | epoch 003:   2469 / 60421 loss=0.952, ppl=1.93, wps=16413, ups=4.42, wpb=3715.2, bsz=129, num_updates=123300, lr=9.00572e-05, gnorm=1.139, loss_scale=4, train_wall=22, gb_free=10.8, wall=37687
2021-05-01 22:43:49 | INFO | train_inner | epoch 003:   2569 / 60421 loss=0.908, ppl=1.88, wps=16260.4, ups=4.48, wpb=3627, bsz=138.6, num_updates=123400, lr=9.00207e-05, gnorm=1.238, loss_scale=4, train_wall=22, gb_free=10.8, wall=37710
2021-05-01 22:44:12 | INFO | train_inner | epoch 003:   2669 / 60421 loss=1.016, ppl=2.02, wps=16559.9, ups=4.36, wpb=3800.3, bsz=117.4, num_updates=123500, lr=8.99843e-05, gnorm=1.13, loss_scale=4, train_wall=23, gb_free=11, wall=37732
2021-05-01 22:44:34 | INFO | train_inner | epoch 003:   2769 / 60421 loss=0.934, ppl=1.91, wps=16448.4, ups=4.37, wpb=3767.8, bsz=147, num_updates=123600, lr=8.99478e-05, gnorm=1.057, loss_scale=4, train_wall=23, gb_free=10.9, wall=37755
2021-05-01 22:44:57 | INFO | train_inner | epoch 003:   2869 / 60421 loss=0.947, ppl=1.93, wps=16312.4, ups=4.41, wpb=3698.8, bsz=137.8, num_updates=123700, lr=8.99115e-05, gnorm=1.164, loss_scale=4, train_wall=22, gb_free=10.8, wall=37778
2021-05-01 22:45:20 | INFO | train_inner | epoch 003:   2969 / 60421 loss=0.961, ppl=1.95, wps=16066.6, ups=4.35, wpb=3690.7, bsz=136.6, num_updates=123800, lr=8.98752e-05, gnorm=1.291, loss_scale=4, train_wall=23, gb_free=10.8, wall=37801
2021-05-01 22:45:43 | INFO | train_inner | epoch 003:   3069 / 60421 loss=0.969, ppl=1.96, wps=16389.1, ups=4.28, wpb=3832.4, bsz=143.4, num_updates=123900, lr=8.98389e-05, gnorm=1.077, loss_scale=4, train_wall=23, gb_free=10.8, wall=37824
2021-05-01 22:46:07 | INFO | train_inner | epoch 003:   3169 / 60421 loss=1.018, ppl=2.02, wps=16415.8, ups=4.28, wpb=3833.9, bsz=117.6, num_updates=124000, lr=8.98027e-05, gnorm=1.158, loss_scale=4, train_wall=23, gb_free=10.7, wall=37848
2021-05-01 22:46:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:46:07 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:46:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:46:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:46:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:46:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:46:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:46:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:46:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:46:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:46:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:46:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:46:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:46:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:47:14 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.888 | ppl 29.62 | bleu 29.07 | wps 2366.4 | wpb 2024.1 | bsz 97.5 | num_updates 124000 | best_bleu 29.31
2021-05-01 22:47:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 124000 updates
2021-05-01 22:47:14 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_124000.pt
2021-05-01 22:47:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_124000.pt
2021-05-01 22:47:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_124000.pt (epoch 3 @ 124000 updates, score 29.07) (writing took 7.715807397995377 seconds)
2021-05-01 22:47:44 | INFO | train_inner | epoch 003:   3269 / 60421 loss=0.967, ppl=1.95, wps=3838.5, ups=1.03, wpb=3740.5, bsz=135.8, num_updates=124100, lr=8.97665e-05, gnorm=1.235, loss_scale=4, train_wall=22, gb_free=11.8, wall=37945
2021-05-01 22:48:07 | INFO | train_inner | epoch 003:   3369 / 60421 loss=1.032, ppl=2.04, wps=16611.5, ups=4.46, wpb=3725.8, bsz=125.9, num_updates=124200, lr=8.97303e-05, gnorm=1.272, loss_scale=4, train_wall=22, gb_free=10.9, wall=37968
2021-05-01 22:48:29 | INFO | train_inner | epoch 003:   3469 / 60421 loss=0.928, ppl=1.9, wps=16453.3, ups=4.43, wpb=3711, bsz=146.1, num_updates=124300, lr=8.96942e-05, gnorm=1.212, loss_scale=4, train_wall=22, gb_free=10.8, wall=37990
2021-05-01 22:48:52 | INFO | train_inner | epoch 003:   3569 / 60421 loss=1.013, ppl=2.02, wps=16565, ups=4.38, wpb=3779.1, bsz=123.8, num_updates=124400, lr=8.96582e-05, gnorm=1.286, loss_scale=4, train_wall=23, gb_free=10.8, wall=38013
2021-05-01 22:49:15 | INFO | train_inner | epoch 003:   3669 / 60421 loss=0.914, ppl=1.88, wps=16472.6, ups=4.36, wpb=3779, bsz=153.3, num_updates=124500, lr=8.96221e-05, gnorm=1.206, loss_scale=4, train_wall=23, gb_free=10.7, wall=38036
2021-05-01 22:49:37 | INFO | train_inner | epoch 003:   3769 / 60421 loss=0.939, ppl=1.92, wps=16415.4, ups=4.44, wpb=3693.4, bsz=129.1, num_updates=124600, lr=8.95862e-05, gnorm=1.04, loss_scale=4, train_wall=22, gb_free=11, wall=38058
2021-05-01 22:50:00 | INFO | train_inner | epoch 003:   3869 / 60421 loss=0.983, ppl=1.98, wps=16448.8, ups=4.43, wpb=3713, bsz=124.9, num_updates=124700, lr=8.95502e-05, gnorm=1.202, loss_scale=4, train_wall=22, gb_free=10.5, wall=38081
2021-05-01 22:50:23 | INFO | train_inner | epoch 003:   3969 / 60421 loss=0.907, ppl=1.87, wps=16155.4, ups=4.38, wpb=3690.7, bsz=136, num_updates=124800, lr=8.95144e-05, gnorm=1.229, loss_scale=4, train_wall=23, gb_free=10.9, wall=38104
2021-05-01 22:50:46 | INFO | train_inner | epoch 003:   4069 / 60421 loss=0.999, ppl=2, wps=16280.1, ups=4.31, wpb=3778.1, bsz=112, num_updates=124900, lr=8.94785e-05, gnorm=1.103, loss_scale=4, train_wall=23, gb_free=10.9, wall=38127
2021-05-01 22:51:10 | INFO | train_inner | epoch 003:   4169 / 60421 loss=0.938, ppl=1.92, wps=16130.7, ups=4.26, wpb=3787.9, bsz=136.5, num_updates=125000, lr=8.94427e-05, gnorm=1.247, loss_scale=4, train_wall=23, gb_free=10.7, wall=38151
2021-05-01 22:51:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:51:10 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:51:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:51:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:51:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:51:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:51:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:51:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:51:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:51:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:51:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:51:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:51:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:51:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:51:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:51:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:51:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:52:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.946 | ppl 30.83 | bleu 28.88 | wps 2360.8 | wpb 2024.1 | bsz 97.5 | num_updates 125000 | best_bleu 29.31
2021-05-01 22:52:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 125000 updates
2021-05-01 22:52:17 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_125000.pt
2021-05-01 22:52:19 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_125000.pt
2021-05-01 22:52:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_125000.pt (epoch 3 @ 125000 updates, score 28.88) (writing took 7.881715369003359 seconds)
2021-05-01 22:52:47 | INFO | train_inner | epoch 003:   4269 / 60421 loss=0.982, ppl=1.98, wps=3810, ups=1.02, wpb=3725.5, bsz=122.2, num_updates=125100, lr=8.9407e-05, gnorm=1.163, loss_scale=4, train_wall=22, gb_free=11, wall=38248
2021-05-01 22:53:10 | INFO | train_inner | epoch 003:   4369 / 60421 loss=0.979, ppl=1.97, wps=16479.3, ups=4.42, wpb=3731.4, bsz=121.4, num_updates=125200, lr=8.93713e-05, gnorm=1.123, loss_scale=4, train_wall=22, gb_free=10.8, wall=38271
2021-05-01 22:53:32 | INFO | train_inner | epoch 003:   4469 / 60421 loss=1.008, ppl=2.01, wps=16298.4, ups=4.52, wpb=3609.4, bsz=115.7, num_updates=125300, lr=8.93356e-05, gnorm=1.365, loss_scale=4, train_wall=22, gb_free=10.6, wall=38293
2021-05-01 22:53:55 | INFO | train_inner | epoch 003:   4569 / 60421 loss=0.985, ppl=1.98, wps=16632.5, ups=4.37, wpb=3801.9, bsz=125, num_updates=125400, lr=8.93e-05, gnorm=1.084, loss_scale=4, train_wall=23, gb_free=10.7, wall=38316
2021-05-01 22:54:18 | INFO | train_inner | epoch 003:   4669 / 60421 loss=0.977, ppl=1.97, wps=16526.6, ups=4.37, wpb=3782.4, bsz=137.8, num_updates=125500, lr=8.92644e-05, gnorm=1.101, loss_scale=4, train_wall=23, gb_free=10.8, wall=38339
2021-05-01 22:54:41 | INFO | train_inner | epoch 003:   4769 / 60421 loss=0.993, ppl=1.99, wps=16523.9, ups=4.42, wpb=3738.6, bsz=123.9, num_updates=125600, lr=8.92288e-05, gnorm=1.168, loss_scale=4, train_wall=22, gb_free=10.7, wall=38362
2021-05-01 22:55:03 | INFO | train_inner | epoch 003:   4869 / 60421 loss=0.915, ppl=1.89, wps=16314.3, ups=4.42, wpb=3694.2, bsz=136.7, num_updates=125700, lr=8.91933e-05, gnorm=1.084, loss_scale=4, train_wall=22, gb_free=10.9, wall=38384
2021-05-01 22:55:26 | INFO | train_inner | epoch 003:   4969 / 60421 loss=0.939, ppl=1.92, wps=16221.7, ups=4.36, wpb=3723.4, bsz=141.8, num_updates=125800, lr=8.91579e-05, gnorm=1.037, loss_scale=4, train_wall=23, gb_free=10.8, wall=38407
2021-05-01 22:55:49 | INFO | train_inner | epoch 003:   5069 / 60421 loss=1.013, ppl=2.02, wps=16233.4, ups=4.36, wpb=3724.5, bsz=112.9, num_updates=125900, lr=8.91225e-05, gnorm=1.131, loss_scale=4, train_wall=23, gb_free=10.8, wall=38430
2021-05-01 22:56:12 | INFO | train_inner | epoch 003:   5169 / 60421 loss=0.959, ppl=1.94, wps=16058.9, ups=4.29, wpb=3740.9, bsz=130, num_updates=126000, lr=8.90871e-05, gnorm=1.226, loss_scale=4, train_wall=23, gb_free=10.8, wall=38453
2021-05-01 22:56:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 22:56:12 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 22:56:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:56:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:56:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:56:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:56:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:56:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:56:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:56:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:56:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:56:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:56:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:56:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:56:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:56:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:56:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:56:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 22:56:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 22:56:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 22:57:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.946 | ppl 30.82 | bleu 28.43 | wps 2396.4 | wpb 2024.1 | bsz 97.5 | num_updates 126000 | best_bleu 29.31
2021-05-01 22:57:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 126000 updates
2021-05-01 22:57:19 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_126000.pt
2021-05-01 22:57:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_126000.pt
2021-05-01 22:57:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_126000.pt (epoch 3 @ 126000 updates, score 28.43) (writing took 7.888662887999089 seconds)
2021-05-01 22:57:49 | INFO | train_inner | epoch 003:   5269 / 60421 loss=1.009, ppl=2.01, wps=3900.4, ups=1.03, wpb=3788, bsz=118.6, num_updates=126100, lr=8.90517e-05, gnorm=1.091, loss_scale=4, train_wall=23, gb_free=10.8, wall=38550
2021-05-01 22:58:12 | INFO | train_inner | epoch 003:   5369 / 60421 loss=0.958, ppl=1.94, wps=16586, ups=4.44, wpb=3735, bsz=126.6, num_updates=126200, lr=8.90165e-05, gnorm=1.286, loss_scale=4, train_wall=22, gb_free=10.7, wall=38573
2021-05-01 22:58:35 | INFO | train_inner | epoch 003:   5469 / 60421 loss=0.957, ppl=1.94, wps=16603, ups=4.43, wpb=3751.8, bsz=132.5, num_updates=126300, lr=8.89812e-05, gnorm=1.169, loss_scale=4, train_wall=22, gb_free=10.7, wall=38596
2021-05-01 22:58:57 | INFO | train_inner | epoch 003:   5569 / 60421 loss=0.995, ppl=1.99, wps=16558.2, ups=4.42, wpb=3749.5, bsz=137.4, num_updates=126400, lr=8.8946e-05, gnorm=1.239, loss_scale=4, train_wall=22, gb_free=10.7, wall=38618
2021-05-01 22:59:20 | INFO | train_inner | epoch 003:   5669 / 60421 loss=0.922, ppl=1.89, wps=16395.6, ups=4.45, wpb=3681.2, bsz=128.4, num_updates=126500, lr=8.89108e-05, gnorm=1.248, loss_scale=8, train_wall=22, gb_free=10.8, wall=38641
2021-05-01 22:59:42 | INFO | train_inner | epoch 003:   5769 / 60421 loss=0.944, ppl=1.92, wps=16450, ups=4.4, wpb=3736.8, bsz=152.5, num_updates=126600, lr=8.88757e-05, gnorm=1.24, loss_scale=8, train_wall=23, gb_free=10.7, wall=38663
2021-05-01 23:00:06 | INFO | train_inner | epoch 003:   5869 / 60421 loss=0.95, ppl=1.93, wps=16446.9, ups=4.32, wpb=3810.2, bsz=147, num_updates=126700, lr=8.88406e-05, gnorm=1.049, loss_scale=8, train_wall=23, gb_free=10.9, wall=38687
2021-05-01 23:00:29 | INFO | train_inner | epoch 003:   5969 / 60421 loss=0.988, ppl=1.98, wps=16306.3, ups=4.3, wpb=3791.9, bsz=130.5, num_updates=126800, lr=8.88056e-05, gnorm=1.204, loss_scale=8, train_wall=23, gb_free=10.8, wall=38710
2021-05-01 23:00:52 | INFO | train_inner | epoch 003:   6069 / 60421 loss=0.899, ppl=1.87, wps=16059.9, ups=4.32, wpb=3720.6, bsz=139.1, num_updates=126900, lr=8.87706e-05, gnorm=1.096, loss_scale=8, train_wall=23, gb_free=10.9, wall=38733
2021-05-01 23:01:15 | INFO | train_inner | epoch 003:   6169 / 60421 loss=0.945, ppl=1.92, wps=16015, ups=4.28, wpb=3742.7, bsz=151.3, num_updates=127000, lr=8.87357e-05, gnorm=1.277, loss_scale=8, train_wall=23, gb_free=10.8, wall=38756
2021-05-01 23:01:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:01:15 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:01:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:01:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:01:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:01:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:01:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:01:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:01:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:01:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:01:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:01:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:01:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:01:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:01:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:01:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:01:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:02:22 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.946 | ppl 30.83 | bleu 29.06 | wps 2373.9 | wpb 2024.1 | bsz 97.5 | num_updates 127000 | best_bleu 29.31
2021-05-01 23:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 127000 updates
2021-05-01 23:02:22 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_127000.pt
2021-05-01 23:02:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_127000.pt
2021-05-01 23:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_127000.pt (epoch 3 @ 127000 updates, score 29.06) (writing took 7.8439864269894315 seconds)
2021-05-01 23:02:53 | INFO | train_inner | epoch 003:   6269 / 60421 loss=0.939, ppl=1.92, wps=3873.3, ups=1.02, wpb=3783, bsz=148.6, num_updates=127100, lr=8.87007e-05, gnorm=1.207, loss_scale=8, train_wall=23, gb_free=10.7, wall=38854
2021-05-01 23:03:16 | INFO | train_inner | epoch 003:   6369 / 60421 loss=0.939, ppl=1.92, wps=16544.2, ups=4.41, wpb=3751.1, bsz=141.9, num_updates=127200, lr=8.86659e-05, gnorm=1.069, loss_scale=8, train_wall=22, gb_free=10.8, wall=38877
2021-05-01 23:03:39 | INFO | train_inner | epoch 003:   6469 / 60421 loss=0.991, ppl=1.99, wps=16543.7, ups=4.37, wpb=3784.4, bsz=137.2, num_updates=127300, lr=8.8631e-05, gnorm=1.082, loss_scale=8, train_wall=23, gb_free=10.7, wall=38900
2021-05-01 23:04:01 | INFO | train_inner | epoch 003:   6569 / 60421 loss=0.963, ppl=1.95, wps=16624.7, ups=4.4, wpb=3776.1, bsz=133.3, num_updates=127400, lr=8.85962e-05, gnorm=1.135, loss_scale=8, train_wall=23, gb_free=10.8, wall=38922
2021-05-01 23:04:24 | INFO | train_inner | epoch 003:   6669 / 60421 loss=0.902, ppl=1.87, wps=16595.3, ups=4.34, wpb=3827.1, bsz=151.1, num_updates=127500, lr=8.85615e-05, gnorm=1.134, loss_scale=8, train_wall=23, gb_free=10.9, wall=38945
2021-05-01 23:04:47 | INFO | train_inner | epoch 003:   6769 / 60421 loss=0.959, ppl=1.94, wps=16487.5, ups=4.4, wpb=3748.8, bsz=137.8, num_updates=127600, lr=8.85268e-05, gnorm=1.22, loss_scale=8, train_wall=23, gb_free=10.8, wall=38968
2021-05-01 23:05:10 | INFO | train_inner | epoch 003:   6869 / 60421 loss=0.927, ppl=1.9, wps=16289.9, ups=4.44, wpb=3673, bsz=132.8, num_updates=127700, lr=8.84921e-05, gnorm=1.126, loss_scale=8, train_wall=22, gb_free=10.7, wall=38991
2021-05-01 23:05:33 | INFO | train_inner | epoch 003:   6969 / 60421 loss=0.954, ppl=1.94, wps=16159.2, ups=4.38, wpb=3689.8, bsz=133, num_updates=127800, lr=8.84575e-05, gnorm=1.12, loss_scale=8, train_wall=23, gb_free=10.7, wall=39013
2021-05-01 23:05:56 | INFO | train_inner | epoch 003:   7069 / 60421 loss=0.935, ppl=1.91, wps=16035.5, ups=4.31, wpb=3717.8, bsz=133.6, num_updates=127900, lr=8.84229e-05, gnorm=1.144, loss_scale=8, train_wall=23, gb_free=10.8, wall=39037
2021-05-01 23:06:19 | INFO | train_inner | epoch 003:   7169 / 60421 loss=0.914, ppl=1.88, wps=15927, ups=4.27, wpb=3728.8, bsz=126.8, num_updates=128000, lr=8.83883e-05, gnorm=1.155, loss_scale=8, train_wall=23, gb_free=10.8, wall=39060
2021-05-01 23:06:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:06:19 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:06:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:06:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:06:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:06:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:06:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:06:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:06:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:06:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:06:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:06:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:06:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:06:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:06:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:06:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:06:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:07:26 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.964 | ppl 31.21 | bleu 29.28 | wps 2384.1 | wpb 2024.1 | bsz 97.5 | num_updates 128000 | best_bleu 29.31
2021-05-01 23:07:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 128000 updates
2021-05-01 23:07:26 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_128000.pt
2021-05-01 23:07:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_128000.pt
2021-05-01 23:07:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_128000.pt (epoch 3 @ 128000 updates, score 29.28) (writing took 7.809362041996792 seconds)
2021-05-01 23:07:56 | INFO | train_inner | epoch 003:   7269 / 60421 loss=0.954, ppl=1.94, wps=3833.6, ups=1.03, wpb=3724.3, bsz=134.1, num_updates=128100, lr=8.83538e-05, gnorm=1.177, loss_scale=8, train_wall=22, gb_free=10.8, wall=39157
2021-05-01 23:08:19 | INFO | train_inner | epoch 003:   7369 / 60421 loss=0.95, ppl=1.93, wps=16423.1, ups=4.4, wpb=3732.3, bsz=135.7, num_updates=128200, lr=8.83194e-05, gnorm=1.221, loss_scale=8, train_wall=23, gb_free=10.8, wall=39180
2021-05-01 23:08:42 | INFO | train_inner | epoch 003:   7469 / 60421 loss=0.991, ppl=1.99, wps=16543.9, ups=4.41, wpb=3755.4, bsz=139.7, num_updates=128300, lr=8.82849e-05, gnorm=1.135, loss_scale=8, train_wall=23, gb_free=10.8, wall=39203
2021-05-01 23:09:04 | INFO | train_inner | epoch 003:   7569 / 60421 loss=0.932, ppl=1.91, wps=16385.3, ups=4.46, wpb=3676, bsz=132.2, num_updates=128400, lr=8.82506e-05, gnorm=1.156, loss_scale=8, train_wall=22, gb_free=10.7, wall=39225
2021-05-01 23:09:27 | INFO | train_inner | epoch 003:   7669 / 60421 loss=0.913, ppl=1.88, wps=16416.2, ups=4.43, wpb=3705.9, bsz=138.2, num_updates=128500, lr=8.82162e-05, gnorm=1.053, loss_scale=8, train_wall=22, gb_free=10.8, wall=39248
2021-05-01 23:09:50 | INFO | train_inner | epoch 003:   7769 / 60421 loss=0.946, ppl=1.93, wps=16501.3, ups=4.35, wpb=3789.3, bsz=142.2, num_updates=128600, lr=8.81819e-05, gnorm=1.119, loss_scale=8, train_wall=23, gb_free=10.9, wall=39271
2021-05-01 23:10:12 | INFO | train_inner | epoch 003:   7869 / 60421 loss=0.928, ppl=1.9, wps=16276.9, ups=4.4, wpb=3695.4, bsz=126.1, num_updates=128700, lr=8.81476e-05, gnorm=1.285, loss_scale=8, train_wall=23, gb_free=11, wall=39293
2021-05-01 23:10:35 | INFO | train_inner | epoch 003:   7969 / 60421 loss=0.968, ppl=1.96, wps=16119.2, ups=4.38, wpb=3684.1, bsz=123.6, num_updates=128800, lr=8.81134e-05, gnorm=1.247, loss_scale=8, train_wall=23, gb_free=10.8, wall=39316
2021-05-01 23:10:59 | INFO | train_inner | epoch 003:   8069 / 60421 loss=0.926, ppl=1.9, wps=16166.2, ups=4.24, wpb=3809.1, bsz=162.2, num_updates=128900, lr=8.80792e-05, gnorm=1.042, loss_scale=8, train_wall=23, gb_free=10.8, wall=39340
2021-05-01 23:11:22 | INFO | train_inner | epoch 003:   8169 / 60421 loss=0.927, ppl=1.9, wps=15674.7, ups=4.31, wpb=3633.4, bsz=131.7, num_updates=129000, lr=8.80451e-05, gnorm=1.329, loss_scale=8, train_wall=23, gb_free=10.9, wall=39363
2021-05-01 23:11:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:11:22 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:11:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:11:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:11:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:11:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:11:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:11:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:11:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:11:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:11:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:11:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:11:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:11:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:12:29 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.982 | ppl 31.59 | bleu 29.08 | wps 2380.8 | wpb 2024.1 | bsz 97.5 | num_updates 129000 | best_bleu 29.31
2021-05-01 23:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 129000 updates
2021-05-01 23:12:29 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_129000.pt
2021-05-01 23:12:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_129000.pt
2021-05-01 23:12:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_129000.pt (epoch 3 @ 129000 updates, score 29.08) (writing took 7.728397512997617 seconds)
2021-05-01 23:12:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2021-05-01 23:12:59 | INFO | train_inner | epoch 003:   8270 / 60421 loss=1.006, ppl=2.01, wps=3847.8, ups=1.03, wpb=3742.3, bsz=118.7, num_updates=129100, lr=8.8011e-05, gnorm=1.176, loss_scale=4, train_wall=23, gb_free=10.9, wall=39460
2021-05-01 23:13:22 | INFO | train_inner | epoch 003:   8370 / 60421 loss=0.963, ppl=1.95, wps=16628.6, ups=4.37, wpb=3805.2, bsz=122.2, num_updates=129200, lr=8.79769e-05, gnorm=1.144, loss_scale=4, train_wall=23, gb_free=11, wall=39483
2021-05-01 23:13:45 | INFO | train_inner | epoch 003:   8470 / 60421 loss=0.993, ppl=1.99, wps=16751.3, ups=4.4, wpb=3807.1, bsz=128.1, num_updates=129300, lr=8.79429e-05, gnorm=1.068, loss_scale=4, train_wall=23, gb_free=10.7, wall=39506
2021-05-01 23:14:08 | INFO | train_inner | epoch 003:   8570 / 60421 loss=0.984, ppl=1.98, wps=16637.9, ups=4.39, wpb=3789.7, bsz=119.3, num_updates=129400, lr=8.79089e-05, gnorm=1.087, loss_scale=4, train_wall=23, gb_free=10.9, wall=39529
2021-05-01 23:14:30 | INFO | train_inner | epoch 003:   8670 / 60421 loss=0.981, ppl=1.97, wps=16577.7, ups=4.39, wpb=3775.4, bsz=127.1, num_updates=129500, lr=8.7875e-05, gnorm=1.059, loss_scale=4, train_wall=23, gb_free=10.7, wall=39551
2021-05-01 23:14:53 | INFO | train_inner | epoch 003:   8770 / 60421 loss=0.944, ppl=1.92, wps=16304.4, ups=4.4, wpb=3706.4, bsz=131.9, num_updates=129600, lr=8.7841e-05, gnorm=1.105, loss_scale=4, train_wall=23, gb_free=11.1, wall=39574
2021-05-01 23:15:16 | INFO | train_inner | epoch 003:   8870 / 60421 loss=1.024, ppl=2.03, wps=16458.7, ups=4.36, wpb=3773.8, bsz=127.7, num_updates=129700, lr=8.78072e-05, gnorm=1.16, loss_scale=4, train_wall=23, gb_free=11, wall=39597
2021-05-01 23:15:39 | INFO | train_inner | epoch 003:   8970 / 60421 loss=0.929, ppl=1.9, wps=16221.1, ups=4.3, wpb=3769.9, bsz=147.8, num_updates=129800, lr=8.77733e-05, gnorm=1.04, loss_scale=4, train_wall=23, gb_free=10.6, wall=39620
2021-05-01 23:16:02 | INFO | train_inner | epoch 003:   9070 / 60421 loss=0.99, ppl=1.99, wps=15891.8, ups=4.34, wpb=3661.4, bsz=122.4, num_updates=129900, lr=8.77396e-05, gnorm=1.523, loss_scale=4, train_wall=23, gb_free=10.9, wall=39643
2021-05-01 23:16:25 | INFO | train_inner | epoch 003:   9170 / 60421 loss=0.942, ppl=1.92, wps=15849.7, ups=4.33, wpb=3658.2, bsz=125.4, num_updates=130000, lr=8.77058e-05, gnorm=1.555, loss_scale=4, train_wall=23, gb_free=11, wall=39666
2021-05-01 23:16:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:16:25 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:16:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:16:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:16:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:16:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:16:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:16:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:16:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:16:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:16:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:16:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:16:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:16:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:17:32 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.983 | ppl 31.62 | bleu 28.89 | wps 2382.2 | wpb 2024.1 | bsz 97.5 | num_updates 130000 | best_bleu 29.31
2021-05-01 23:17:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 130000 updates
2021-05-01 23:17:32 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_130000.pt
2021-05-01 23:17:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_130000.pt
2021-05-01 23:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_130000.pt (epoch 3 @ 130000 updates, score 28.89) (writing took 7.789429043987184 seconds)
2021-05-01 23:18:03 | INFO | train_inner | epoch 003:   9270 / 60421 loss=0.944, ppl=1.92, wps=3874.7, ups=1.03, wpb=3779.1, bsz=145.2, num_updates=130100, lr=8.76721e-05, gnorm=1.048, loss_scale=4, train_wall=23, gb_free=10.8, wall=39764
2021-05-01 23:18:26 | INFO | train_inner | epoch 003:   9370 / 60421 loss=0.956, ppl=1.94, wps=16425, ups=4.41, wpb=3727.6, bsz=139.4, num_updates=130200, lr=8.76384e-05, gnorm=1.129, loss_scale=4, train_wall=23, gb_free=10.8, wall=39787
2021-05-01 23:18:48 | INFO | train_inner | epoch 003:   9470 / 60421 loss=1.003, ppl=2, wps=16503.8, ups=4.44, wpb=3715.6, bsz=127.6, num_updates=130300, lr=8.76048e-05, gnorm=1.253, loss_scale=4, train_wall=22, gb_free=10.8, wall=39809
2021-05-01 23:19:11 | INFO | train_inner | epoch 003:   9570 / 60421 loss=0.932, ppl=1.91, wps=16506.6, ups=4.36, wpb=3781.7, bsz=141.1, num_updates=130400, lr=8.75712e-05, gnorm=1.109, loss_scale=4, train_wall=23, gb_free=10.7, wall=39832
2021-05-01 23:19:34 | INFO | train_inner | epoch 003:   9670 / 60421 loss=0.984, ppl=1.98, wps=16532.1, ups=4.37, wpb=3785.2, bsz=121.2, num_updates=130500, lr=8.75376e-05, gnorm=1.117, loss_scale=4, train_wall=23, gb_free=10.7, wall=39855
2021-05-01 23:19:57 | INFO | train_inner | epoch 003:   9770 / 60421 loss=0.945, ppl=1.93, wps=16421.9, ups=4.35, wpb=3774.7, bsz=135.5, num_updates=130600, lr=8.75041e-05, gnorm=0.993, loss_scale=4, train_wall=23, gb_free=10.6, wall=39878
2021-05-01 23:20:20 | INFO | train_inner | epoch 003:   9870 / 60421 loss=0.916, ppl=1.89, wps=16122, ups=4.4, wpb=3668.1, bsz=139.6, num_updates=130700, lr=8.74706e-05, gnorm=1.126, loss_scale=4, train_wall=23, gb_free=10.9, wall=39901
2021-05-01 23:20:43 | INFO | train_inner | epoch 003:   9970 / 60421 loss=0.953, ppl=1.94, wps=16381.4, ups=4.29, wpb=3822.5, bsz=141.8, num_updates=130800, lr=8.74372e-05, gnorm=0.986, loss_scale=4, train_wall=23, gb_free=10.9, wall=39924
2021-05-01 23:21:06 | INFO | train_inner | epoch 003:  10070 / 60421 loss=0.963, ppl=1.95, wps=16060, ups=4.29, wpb=3740.8, bsz=126.4, num_updates=130900, lr=8.74038e-05, gnorm=1.173, loss_scale=4, train_wall=23, gb_free=10.7, wall=39947
2021-05-01 23:21:30 | INFO | train_inner | epoch 003:  10170 / 60421 loss=0.969, ppl=1.96, wps=15861.2, ups=4.26, wpb=3727.1, bsz=123.8, num_updates=131000, lr=8.73704e-05, gnorm=1.382, loss_scale=4, train_wall=23, gb_free=10.8, wall=39971
2021-05-01 23:21:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:21:30 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:21:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:21:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:21:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:21:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:21:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:21:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:21:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:21:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:21:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:22:36 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.972 | ppl 31.38 | bleu 28.94 | wps 2403 | wpb 2024.1 | bsz 97.5 | num_updates 131000 | best_bleu 29.31
2021-05-01 23:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 131000 updates
2021-05-01 23:22:36 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_131000.pt
2021-05-01 23:22:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_131000.pt
2021-05-01 23:22:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_131000.pt (epoch 3 @ 131000 updates, score 28.94) (writing took 7.8059152659989195 seconds)
2021-05-01 23:23:06 | INFO | train_inner | epoch 003:  10270 / 60421 loss=0.973, ppl=1.96, wps=3817.8, ups=1.04, wpb=3687.6, bsz=137.3, num_updates=131100, lr=8.73371e-05, gnorm=1.441, loss_scale=4, train_wall=22, gb_free=10.8, wall=40067
2021-05-01 23:23:29 | INFO | train_inner | epoch 003:  10370 / 60421 loss=0.935, ppl=1.91, wps=16568, ups=4.4, wpb=3761.8, bsz=165.8, num_updates=131200, lr=8.73038e-05, gnorm=1.114, loss_scale=4, train_wall=23, gb_free=10.8, wall=40090
2021-05-01 23:23:52 | INFO | train_inner | epoch 003:  10470 / 60421 loss=0.952, ppl=1.93, wps=16714.8, ups=4.43, wpb=3774.8, bsz=144.2, num_updates=131300, lr=8.72705e-05, gnorm=1.126, loss_scale=4, train_wall=22, gb_free=10.7, wall=40113
2021-05-01 23:24:14 | INFO | train_inner | epoch 003:  10570 / 60421 loss=0.958, ppl=1.94, wps=16485.2, ups=4.48, wpb=3681.3, bsz=131.5, num_updates=131400, lr=8.72373e-05, gnorm=1.156, loss_scale=4, train_wall=22, gb_free=10.7, wall=40135
2021-05-01 23:24:36 | INFO | train_inner | epoch 003:  10670 / 60421 loss=1.037, ppl=2.05, wps=16570.6, ups=4.47, wpb=3703, bsz=114.2, num_updates=131500, lr=8.72041e-05, gnorm=1.353, loss_scale=4, train_wall=22, gb_free=10.8, wall=40157
2021-05-01 23:24:58 | INFO | train_inner | epoch 003:  10770 / 60421 loss=1.004, ppl=2.01, wps=16563, ups=4.54, wpb=3652.2, bsz=105.4, num_updates=131600, lr=8.7171e-05, gnorm=1.2, loss_scale=4, train_wall=22, gb_free=10.8, wall=40179
2021-05-01 23:25:21 | INFO | train_inner | epoch 003:  10870 / 60421 loss=0.942, ppl=1.92, wps=16709, ups=4.44, wpb=3764.8, bsz=129.4, num_updates=131700, lr=8.71379e-05, gnorm=1.015, loss_scale=4, train_wall=22, gb_free=11, wall=40202
2021-05-01 23:25:43 | INFO | train_inner | epoch 003:  10970 / 60421 loss=0.964, ppl=1.95, wps=16617.7, ups=4.47, wpb=3720.1, bsz=124, num_updates=131800, lr=8.71048e-05, gnorm=1.154, loss_scale=4, train_wall=22, gb_free=11.1, wall=40224
2021-05-01 23:26:06 | INFO | train_inner | epoch 003:  11070 / 60421 loss=0.899, ppl=1.86, wps=16498.4, ups=4.47, wpb=3691.8, bsz=148.8, num_updates=131900, lr=8.70718e-05, gnorm=1.162, loss_scale=4, train_wall=22, gb_free=10.8, wall=40247
2021-05-01 23:26:28 | INFO | train_inner | epoch 003:  11170 / 60421 loss=0.98, ppl=1.97, wps=16550.9, ups=4.49, wpb=3685.8, bsz=128.4, num_updates=132000, lr=8.70388e-05, gnorm=1.468, loss_scale=4, train_wall=22, gb_free=10.9, wall=40269
2021-05-01 23:26:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:26:28 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:26:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:26:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:26:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:26:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:26:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:26:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:26:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:26:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:26:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:26:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:26:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:26:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:26:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:26:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:26:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:27:33 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.926 | ppl 30.4 | bleu 29.13 | wps 2424.3 | wpb 2024.1 | bsz 97.5 | num_updates 132000 | best_bleu 29.31
2021-05-01 23:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 132000 updates
2021-05-01 23:27:33 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_132000.pt
2021-05-01 23:27:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_132000.pt
2021-05-01 23:27:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_132000.pt (epoch 3 @ 132000 updates, score 29.13) (writing took 7.803745671000797 seconds)
2021-05-01 23:28:04 | INFO | train_inner | epoch 003:  11270 / 60421 loss=0.973, ppl=1.96, wps=3956.1, ups=1.04, wpb=3798.1, bsz=124.6, num_updates=132100, lr=8.70059e-05, gnorm=1.184, loss_scale=4, train_wall=22, gb_free=10.9, wall=40365
2021-05-01 23:28:27 | INFO | train_inner | epoch 003:  11370 / 60421 loss=0.982, ppl=1.97, wps=16663.9, ups=4.42, wpb=3774.1, bsz=141.1, num_updates=132200, lr=8.6973e-05, gnorm=1.149, loss_scale=4, train_wall=22, gb_free=10.8, wall=40388
2021-05-01 23:28:49 | INFO | train_inner | epoch 003:  11470 / 60421 loss=0.972, ppl=1.96, wps=16575.8, ups=4.39, wpb=3772.8, bsz=130.6, num_updates=132300, lr=8.69401e-05, gnorm=1.154, loss_scale=4, train_wall=23, gb_free=10.9, wall=40410
2021-05-01 23:29:12 | INFO | train_inner | epoch 003:  11570 / 60421 loss=0.974, ppl=1.96, wps=16418.9, ups=4.46, wpb=3679.4, bsz=121.2, num_updates=132400, lr=8.69072e-05, gnorm=1.306, loss_scale=4, train_wall=22, gb_free=10.9, wall=40433
2021-05-01 23:29:35 | INFO | train_inner | epoch 003:  11670 / 60421 loss=0.969, ppl=1.96, wps=16460.1, ups=4.4, wpb=3737.4, bsz=125.3, num_updates=132500, lr=8.68744e-05, gnorm=1.255, loss_scale=4, train_wall=23, gb_free=10.8, wall=40455
2021-05-01 23:29:57 | INFO | train_inner | epoch 003:  11770 / 60421 loss=0.917, ppl=1.89, wps=16477.8, ups=4.36, wpb=3782.4, bsz=161, num_updates=132600, lr=8.68417e-05, gnorm=1.189, loss_scale=4, train_wall=23, gb_free=10.8, wall=40478
2021-05-01 23:30:20 | INFO | train_inner | epoch 003:  11870 / 60421 loss=0.941, ppl=1.92, wps=16207.9, ups=4.37, wpb=3712.3, bsz=125, num_updates=132700, lr=8.6809e-05, gnorm=1.16, loss_scale=4, train_wall=23, gb_free=10.8, wall=40501
2021-05-01 23:30:44 | INFO | train_inner | epoch 003:  11970 / 60421 loss=0.998, ppl=2, wps=16081.9, ups=4.31, wpb=3734.7, bsz=112.1, num_updates=132800, lr=8.67763e-05, gnorm=1.17, loss_scale=4, train_wall=23, gb_free=11.1, wall=40525
2021-05-01 23:31:07 | INFO | train_inner | epoch 003:  12070 / 60421 loss=0.96, ppl=1.95, wps=16158, ups=4.27, wpb=3787, bsz=159.4, num_updates=132900, lr=8.67436e-05, gnorm=1.198, loss_scale=4, train_wall=23, gb_free=10.7, wall=40548
2021-05-01 23:31:31 | INFO | train_inner | epoch 003:  12170 / 60421 loss=0.997, ppl=2, wps=15893.8, ups=4.2, wpb=3785.3, bsz=125.8, num_updates=133000, lr=8.6711e-05, gnorm=1.133, loss_scale=4, train_wall=24, gb_free=11, wall=40572
2021-05-01 23:31:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:31:31 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:31:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:31:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:31:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:31:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:31:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:31:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:31:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:31:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:31:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:31:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:31:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:31:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:32:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.966 | ppl 31.26 | bleu 29.09 | wps 2398.5 | wpb 2024.1 | bsz 97.5 | num_updates 133000 | best_bleu 29.31
2021-05-01 23:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 133000 updates
2021-05-01 23:32:37 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_133000.pt
2021-05-01 23:32:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_133000.pt
2021-05-01 23:32:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_133000.pt (epoch 3 @ 133000 updates, score 29.09) (writing took 7.849500991011155 seconds)
2021-05-01 23:33:08 | INFO | train_inner | epoch 003:  12270 / 60421 loss=0.951, ppl=1.93, wps=3853.1, ups=1.03, wpb=3734.4, bsz=137.9, num_updates=133100, lr=8.66784e-05, gnorm=1.231, loss_scale=4, train_wall=23, gb_free=10.7, wall=40669
2021-05-01 23:33:30 | INFO | train_inner | epoch 003:  12370 / 60421 loss=0.967, ppl=1.95, wps=16697.4, ups=4.4, wpb=3793.4, bsz=123.7, num_updates=133200, lr=8.66459e-05, gnorm=1.094, loss_scale=4, train_wall=23, gb_free=10.8, wall=40691
2021-05-01 23:33:53 | INFO | train_inner | epoch 003:  12470 / 60421 loss=1.016, ppl=2.02, wps=16792.1, ups=4.38, wpb=3833.7, bsz=133, num_updates=133300, lr=8.66134e-05, gnorm=1.281, loss_scale=4, train_wall=23, gb_free=10.8, wall=40714
2021-05-01 23:34:16 | INFO | train_inner | epoch 003:  12570 / 60421 loss=0.973, ppl=1.96, wps=16531.3, ups=4.45, wpb=3714.8, bsz=113.8, num_updates=133400, lr=8.65809e-05, gnorm=1.133, loss_scale=4, train_wall=22, gb_free=10.9, wall=40737
2021-05-01 23:34:39 | INFO | train_inner | epoch 003:  12670 / 60421 loss=0.88, ppl=1.84, wps=16412.7, ups=4.37, wpb=3751.6, bsz=148.8, num_updates=133500, lr=8.65485e-05, gnorm=1.05, loss_scale=4, train_wall=23, gb_free=10.6, wall=40760
2021-05-01 23:35:01 | INFO | train_inner | epoch 003:  12770 / 60421 loss=0.934, ppl=1.91, wps=16228.1, ups=4.47, wpb=3632.9, bsz=141.3, num_updates=133600, lr=8.65161e-05, gnorm=1.497, loss_scale=4, train_wall=22, gb_free=11, wall=40782
2021-05-01 23:35:24 | INFO | train_inner | epoch 003:  12870 / 60421 loss=0.939, ppl=1.92, wps=16403.8, ups=4.33, wpb=3790, bsz=138.8, num_updates=133700, lr=8.64837e-05, gnorm=1.146, loss_scale=4, train_wall=23, gb_free=10.6, wall=40805
2021-05-01 23:35:47 | INFO | train_inner | epoch 003:  12970 / 60421 loss=1.024, ppl=2.03, wps=16242.2, ups=4.31, wpb=3768.1, bsz=117.8, num_updates=133800, lr=8.64514e-05, gnorm=1.214, loss_scale=4, train_wall=23, gb_free=10.9, wall=40828
2021-05-01 23:36:11 | INFO | train_inner | epoch 003:  13070 / 60421 loss=0.962, ppl=1.95, wps=16196.1, ups=4.26, wpb=3800.6, bsz=137, num_updates=133900, lr=8.64191e-05, gnorm=1.065, loss_scale=4, train_wall=23, gb_free=10.9, wall=40852
2021-05-01 23:36:34 | INFO | train_inner | epoch 003:  13170 / 60421 loss=0.955, ppl=1.94, wps=15704.4, ups=4.26, wpb=3690.4, bsz=124.5, num_updates=134000, lr=8.63868e-05, gnorm=1.155, loss_scale=4, train_wall=23, gb_free=10.8, wall=40875
2021-05-01 23:36:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:36:34 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:36:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:36:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:36:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:36:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:36:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:36:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:36:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:36:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:36:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:36:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:36:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:36:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:36:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:36:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:36:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:37:40 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.011 | ppl 32.25 | bleu 29.31 | wps 2417.9 | wpb 2024.1 | bsz 97.5 | num_updates 134000 | best_bleu 29.31
2021-05-01 23:37:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 134000 updates
2021-05-01 23:37:40 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_134000.pt
2021-05-01 23:37:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_134000.pt
2021-05-01 23:37:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_134000.pt (epoch 3 @ 134000 updates, score 29.31) (writing took 14.36374337199959 seconds)
2021-05-01 23:38:17 | INFO | train_inner | epoch 003:  13270 / 60421 loss=0.976, ppl=1.97, wps=3680.8, ups=0.97, wpb=3786.8, bsz=134.3, num_updates=134100, lr=8.63546e-05, gnorm=1.142, loss_scale=4, train_wall=23, gb_free=10.8, wall=40978
2021-05-01 23:38:40 | INFO | train_inner | epoch 003:  13370 / 60421 loss=0.932, ppl=1.91, wps=16508.3, ups=4.42, wpb=3732.2, bsz=139.9, num_updates=134200, lr=8.63224e-05, gnorm=1.36, loss_scale=4, train_wall=22, gb_free=10.8, wall=41001
2021-05-01 23:39:02 | INFO | train_inner | epoch 003:  13470 / 60421 loss=0.996, ppl=1.99, wps=16510, ups=4.44, wpb=3716.6, bsz=120.5, num_updates=134300, lr=8.62903e-05, gnorm=1.283, loss_scale=4, train_wall=22, gb_free=10.7, wall=41023
2021-05-01 23:39:25 | INFO | train_inner | epoch 003:  13570 / 60421 loss=0.958, ppl=1.94, wps=16560.2, ups=4.36, wpb=3795.7, bsz=126, num_updates=134400, lr=8.62582e-05, gnorm=1.042, loss_scale=4, train_wall=23, gb_free=10.8, wall=41046
2021-05-01 23:39:48 | INFO | train_inner | epoch 003:  13670 / 60421 loss=0.899, ppl=1.86, wps=16243.3, ups=4.38, wpb=3705.9, bsz=156.9, num_updates=134500, lr=8.62261e-05, gnorm=1.271, loss_scale=4, train_wall=23, gb_free=10.8, wall=41069
2021-05-01 23:40:11 | INFO | train_inner | epoch 003:  13770 / 60421 loss=0.95, ppl=1.93, wps=16289.5, ups=4.41, wpb=3697.5, bsz=128.5, num_updates=134600, lr=8.61941e-05, gnorm=1.242, loss_scale=4, train_wall=23, gb_free=10.9, wall=41092
2021-05-01 23:40:34 | INFO | train_inner | epoch 003:  13870 / 60421 loss=1.018, ppl=2.03, wps=16272.1, ups=4.36, wpb=3734, bsz=136.6, num_updates=134700, lr=8.61621e-05, gnorm=1.25, loss_scale=4, train_wall=23, gb_free=11.3, wall=41115
2021-05-01 23:40:57 | INFO | train_inner | epoch 003:  13970 / 60421 loss=0.965, ppl=1.95, wps=16214, ups=4.28, wpb=3788.4, bsz=121.5, num_updates=134800, lr=8.61301e-05, gnorm=1.078, loss_scale=4, train_wall=23, gb_free=11.2, wall=41138
2021-05-01 23:41:21 | INFO | train_inner | epoch 003:  14070 / 60421 loss=0.998, ppl=2, wps=16140.4, ups=4.25, wpb=3795.4, bsz=119.4, num_updates=134900, lr=8.60982e-05, gnorm=1.117, loss_scale=4, train_wall=23, gb_free=10.7, wall=41162
2021-05-01 23:41:44 | INFO | train_inner | epoch 003:  14170 / 60421 loss=0.955, ppl=1.94, wps=15697.6, ups=4.24, wpb=3699.2, bsz=119.4, num_updates=135000, lr=8.60663e-05, gnorm=0.991, loss_scale=4, train_wall=23, gb_free=10.7, wall=41185
2021-05-01 23:41:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:41:44 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:41:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:41:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:41:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:41:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:41:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:41:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:42:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:42:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:42:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:42:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:42:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:42:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:42:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:42:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:42:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:42:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5 | ppl 31.99 | bleu 29.18 | wps 2415.9 | wpb 2024.1 | bsz 97.5 | num_updates 135000 | best_bleu 29.31
2021-05-01 23:42:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 135000 updates
2021-05-01 23:42:50 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_135000.pt
2021-05-01 23:42:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_135000.pt
2021-05-01 23:42:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_135000.pt (epoch 3 @ 135000 updates, score 29.18) (writing took 7.732104137001443 seconds)
2021-05-01 23:43:20 | INFO | train_inner | epoch 003:  14270 / 60421 loss=0.94, ppl=1.92, wps=3862, ups=1.04, wpb=3707.1, bsz=120.2, num_updates=135100, lr=8.60344e-05, gnorm=1.094, loss_scale=4, train_wall=22, gb_free=10.8, wall=41281
2021-05-01 23:43:43 | INFO | train_inner | epoch 003:  14370 / 60421 loss=0.939, ppl=1.92, wps=16704.8, ups=4.41, wpb=3786.6, bsz=139, num_updates=135200, lr=8.60026e-05, gnorm=1.033, loss_scale=4, train_wall=22, gb_free=10.8, wall=41304
2021-05-01 23:44:05 | INFO | train_inner | epoch 003:  14470 / 60421 loss=0.984, ppl=1.98, wps=16508.2, ups=4.41, wpb=3741.9, bsz=130.1, num_updates=135300, lr=8.59708e-05, gnorm=1.254, loss_scale=4, train_wall=22, gb_free=10.9, wall=41326
2021-05-01 23:44:28 | INFO | train_inner | epoch 003:  14570 / 60421 loss=0.958, ppl=1.94, wps=16469.7, ups=4.37, wpb=3764.9, bsz=131, num_updates=135400, lr=8.59391e-05, gnorm=1.191, loss_scale=4, train_wall=23, gb_free=10.8, wall=41349
2021-05-01 23:44:51 | INFO | train_inner | epoch 003:  14670 / 60421 loss=0.979, ppl=1.97, wps=16520.5, ups=4.35, wpb=3799.1, bsz=137.3, num_updates=135500, lr=8.59074e-05, gnorm=1.144, loss_scale=4, train_wall=23, gb_free=10.9, wall=41372
2021-05-01 23:45:14 | INFO | train_inner | epoch 003:  14770 / 60421 loss=0.954, ppl=1.94, wps=16179.1, ups=4.37, wpb=3704.6, bsz=146.3, num_updates=135600, lr=8.58757e-05, gnorm=1.073, loss_scale=4, train_wall=23, gb_free=11.1, wall=41395
2021-05-01 23:45:37 | INFO | train_inner | epoch 003:  14870 / 60421 loss=0.961, ppl=1.95, wps=16350.3, ups=4.35, wpb=3755.2, bsz=129.5, num_updates=135700, lr=8.5844e-05, gnorm=1.14, loss_scale=4, train_wall=23, gb_free=10.7, wall=41418
2021-05-01 23:46:01 | INFO | train_inner | epoch 003:  14970 / 60421 loss=0.956, ppl=1.94, wps=16006.4, ups=4.27, wpb=3746.8, bsz=140.9, num_updates=135800, lr=8.58124e-05, gnorm=1.141, loss_scale=4, train_wall=23, gb_free=10.8, wall=41442
2021-05-01 23:46:24 | INFO | train_inner | epoch 003:  15070 / 60421 loss=0.961, ppl=1.95, wps=15784.2, ups=4.24, wpb=3727.1, bsz=119.1, num_updates=135900, lr=8.57808e-05, gnorm=1.246, loss_scale=4, train_wall=23, gb_free=11.2, wall=41465
2021-05-01 23:46:48 | INFO | train_inner | epoch 003:  15170 / 60421 loss=0.904, ppl=1.87, wps=15663.6, ups=4.25, wpb=3685.2, bsz=162.3, num_updates=136000, lr=8.57493e-05, gnorm=1.319, loss_scale=4, train_wall=23, gb_free=10.9, wall=41489
2021-05-01 23:46:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:46:48 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:46:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:46:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:46:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:47:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:47:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:47:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:47:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:47:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:47:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:47:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:47:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:47:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:47:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:47:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:47:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:47:54 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.022 | ppl 32.5 | bleu 29.09 | wps 2405.7 | wpb 2024.1 | bsz 97.5 | num_updates 136000 | best_bleu 29.31
2021-05-01 23:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 136000 updates
2021-05-01 23:47:54 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_136000.pt
2021-05-01 23:47:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_136000.pt
2021-05-01 23:48:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_136000.pt (epoch 3 @ 136000 updates, score 29.09) (writing took 7.7023458469921025 seconds)
2021-05-01 23:48:24 | INFO | train_inner | epoch 003:  15270 / 60421 loss=0.935, ppl=1.91, wps=3848.9, ups=1.04, wpb=3714.5, bsz=146.6, num_updates=136100, lr=8.57178e-05, gnorm=1.27, loss_scale=4, train_wall=23, gb_free=10.8, wall=41585
2021-05-01 23:48:47 | INFO | train_inner | epoch 003:  15370 / 60421 loss=0.92, ppl=1.89, wps=16288.8, ups=4.41, wpb=3696.2, bsz=144.4, num_updates=136200, lr=8.56863e-05, gnorm=1.174, loss_scale=4, train_wall=23, gb_free=10.5, wall=41608
2021-05-01 23:49:10 | INFO | train_inner | epoch 003:  15470 / 60421 loss=0.937, ppl=1.91, wps=16575.7, ups=4.37, wpb=3789.5, bsz=133.8, num_updates=136300, lr=8.56549e-05, gnorm=1.001, loss_scale=4, train_wall=23, gb_free=10.8, wall=41631
2021-05-01 23:49:33 | INFO | train_inner | epoch 003:  15570 / 60421 loss=0.917, ppl=1.89, wps=16354, ups=4.4, wpb=3715.1, bsz=136.6, num_updates=136400, lr=8.56235e-05, gnorm=1.18, loss_scale=4, train_wall=23, gb_free=10.7, wall=41653
2021-05-01 23:49:56 | INFO | train_inner | epoch 003:  15670 / 60421 loss=0.897, ppl=1.86, wps=16355, ups=4.34, wpb=3772.2, bsz=149.8, num_updates=136500, lr=8.55921e-05, gnorm=1.167, loss_scale=4, train_wall=23, gb_free=10.9, wall=41677
2021-05-01 23:50:18 | INFO | train_inner | epoch 003:  15770 / 60421 loss=0.93, ppl=1.91, wps=16245.2, ups=4.38, wpb=3710.5, bsz=132.8, num_updates=136600, lr=8.55608e-05, gnorm=1.166, loss_scale=4, train_wall=23, gb_free=10.9, wall=41699
2021-05-01 23:50:41 | INFO | train_inner | epoch 003:  15870 / 60421 loss=0.921, ppl=1.89, wps=16185.9, ups=4.33, wpb=3735.2, bsz=128.4, num_updates=136700, lr=8.55295e-05, gnorm=1.088, loss_scale=4, train_wall=23, gb_free=10.9, wall=41722
2021-05-01 23:51:04 | INFO | train_inner | epoch 003:  15970 / 60421 loss=0.945, ppl=1.93, wps=16007.7, ups=4.36, wpb=3674, bsz=120.7, num_updates=136800, lr=8.54982e-05, gnorm=1.386, loss_scale=4, train_wall=23, gb_free=11, wall=41745
2021-05-01 23:51:28 | INFO | train_inner | epoch 003:  16070 / 60421 loss=0.872, ppl=1.83, wps=15659.4, ups=4.28, wpb=3656.6, bsz=129.2, num_updates=136900, lr=8.5467e-05, gnorm=1.25, loss_scale=4, train_wall=23, gb_free=10.8, wall=41769
2021-05-01 23:51:52 | INFO | train_inner | epoch 003:  16170 / 60421 loss=0.961, ppl=1.95, wps=15834.8, ups=4.16, wpb=3802.5, bsz=129.9, num_updates=137000, lr=8.54358e-05, gnorm=1.147, loss_scale=4, train_wall=24, gb_free=11.1, wall=41793
2021-05-01 23:51:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-05-01 23:51:52 | WARNING | fairseq.logging.progress_bar | tensorboard not found, please install with: pip install tensorboard
2021-05-01 23:52:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:52:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:52:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:52:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:52:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:52:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:52:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:52:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:52:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:52:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:52:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:52:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:52:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-05-01 23:52:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-05-01 23:52:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-05-01 23:52:59 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.041 | ppl 32.92 | bleu 29.24 | wps 2364.7 | wpb 2024.1 | bsz 97.5 | num_updates 137000 | best_bleu 29.31
2021-05-01 23:52:59 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 20 runs
2021-05-01 23:52:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 137000 updates
2021-05-01 23:52:59 | INFO | fairseq.trainer | Saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_137000.pt
2021-05-01 23:53:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_137000.pt
2021-05-01 23:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/ubuntu/fgda_nmt/checkpoints/translation/transformer/test0/mixed_zhen/checkpoint_3_137000.pt (epoch 3 @ 137000 updates, score 29.24) (writing took 9.486572021007305 seconds)
2021-05-01 23:53:08 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-05-01 23:53:08 | INFO | train | epoch 003 | loss 0.957 | ppl 1.94 | wps 12172.5 | ups 3.26 | wpb 3738.4 | bsz 132.9 | num_updates 137000 | lr 8.54358e-05 | gnorm 1.179 | loss_scale 4 | train_wall 3666 | gb_free 11.1 | wall 41869
2021-05-01 23:53:08 | INFO | fairseq_cli.train | done training in 41865.7 seconds
2021-05-01 23:53:08 | INFO | root | Added key: store_based_barrier_key:2 to store for rank: 0
